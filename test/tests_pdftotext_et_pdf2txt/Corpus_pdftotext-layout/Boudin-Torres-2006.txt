                     A Scalable MMR Approach to Sentence Scoring
                      for Multi-Document Update Summarization

        Florian Boudin \ and Marc El-Bèze \                         Juan-Manuel Torres-Moreno \,[
         \
           Laboratoire Informatique d’Avignon                       [
                                                                      École Polytechnique de Montréal
         339 chemin des Meinajaries, BP1228,                       CP 6079 Succ. Centre Ville H3C 3A7
            84911 Avignon Cedex 9, France.                              Montréal (Québec), Canada.
            florian.boudin@univ-avignon.fr                         juan-manuel.torres@univ-avignon.fr
              marc.elbeze@univ-avignon.fr




                       Abstract                                    redundancy with previously read documents (his-
                                                                   tory) has to be removed from the extract.
    We present S MMR, a scalable sentence
    scoring method for query-oriented up-                             A natural way to go about update summarization
    date summarization. Sentences are scored                       would be extracting temporal tags (dates, elapsed
    thanks to a criterion combining query rele-                    times, temporal expressions...) (Mani and Wilson,
    vance and dissimilarity with already read                      2000) or to automatically construct the timeline
    documents (history). As the amount of                          from documents (Swan and Allan, 2000). These
    data in history increases, non-redundancy                      temporal marks could be used to focus extracts on
    is prioritized over query-relevance. We                        the most recently written facts. However, most re-
    show that S MMR achieves promising re-                         cently written facts are not necessarily new facts.
    sults on the DUC 2007 update corpus.                           Machine Reading (MR) was used by (Hickl et
                                                                   al., 2007) to construct knowledge representations
1   Introduction                                                   from clusters of documents. Sentences contain-
                                                                   ing “new” information (i.e. that could not be in-
Extensive experiments on query-oriented multi-
                                                                   ferred by any previously considered document)
document summarization have been carried out
                                                                   are selected to generate summary. However, this
over the past few years. Most of the strategies
                                                                   highly efficient approach (best system in DUC
to produce summaries are based on an extrac-
                                                                   2007 update) requires large linguistic resources.
tion method, which identifies salient textual seg-
                                                                   (Witte et al., 2007) propose a rule-based system
ments, most often sentences, in documents. Sen-
                                                                   based on fuzzy coreference cluster graphs. Again,
tences containing the most salient concepts are se-
                                                                   this approach requires to manually write the sen-
lected, ordered and assembled according to their
                                                                   tence ranking scheme. Several strategies remain-
relevance to produce summaries (also called ex-
                                                                   ing on post-processing redundancy removal tech-
tracts) (Mani and Maybury, 1999).
                                                                   niques have been suggested. Extracts constructed
   Recently emerged from the Document Under-
                                                                   from history were used by (Boudin and Torres-
standing Conference (DUC) 20071 , update sum-
                                                                   Moreno, 2007) to minimize history’s redundancy.
marization attempts to enhance summarization
                                                                   (Lin et al., 2007) have proposed a modified Max-
when more information about knowledge acquired
                                                                   imal Marginal Relevance (MMR) (Carbonell and
by the user is available. It asks the following ques-
                                                                   Goldstein, 1998) re-ranker during sentence selec-
tion: has the user already read documents on the
                                                                   tion, constructing the summary by incrementally
topic? In the case of a positive answer, producing
                                                                   re-ranking sentences.
an extract focusing on only new facts is of inter-
est. In this way, an important issue is introduced:                   In this paper, we propose a scalable sentence
                                                                   scoring method for update summarization derived
      c 2008.    Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-             from MMR. Motivated by the need for relevant
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).         novelty, candidate sentences are selected accord-
Some rights reserved.                                              ing to a combined criterion of query relevance and
    1
      Document Understanding Conferences are conducted
since 2000 by the National Institute of Standards and Tech-        dissimilarity with previously read sentences. The
nology (NIST), http://www-nlpir.nist.gov                           rest of the paper is organized as follows. Section 2

                                                              23
                       Coling 2008: Companion volume – Posters and Demonstrations, pages 23–26
                                              Manchester, August 2008
introduces our proposed sentence scoring method              sentence s and the query Q:
and Section 3 presents experiments and evaluates                                  1 X
our approach.                                                      J We (s, Q) =     ·   max J W(q, m)        (1)
                                                                                 |Q|     m∈S 0
                                                                                       q∈Q

2     Method                                                 where S 0 is the term set of s in which the terms
                                                             m that already have maximized J W(q, m) are re-
                                                             moved. The use of J We smooths normalization and
The underlying idea of our method is that as the
                                                             misspelling errors. Each sentence s is scored using
number of sentences in the history increases, the
                                                             the linear combination:
likelihood to have redundant information within
candidate sentences also increases. We propose                                              ~
                                                               Sim1 (s, Q) = α · cosine(~s, Q)
a scalable sentence scoring method derived from                                     + (1 − α) · J We (s, Q)   (2)
MMR that, as the size of the history increases,
gives more importance to non-redundancy that to              where α = 0.7, optimally tuned on the past DUCs
query relevance. We define H to represent the pre-           data (2005 and 2006). The system produces a list
viously read documents (history), Q to represent             of ranked sentences from which the summary is
the query and s the candidate sentence. The fol-             constructed by arranging the high scored sentences
lowing subsections formally define the similarity            until the desired size is reached.
measures and the scalable MMR scoring method.
                                                             2.2   A scalable MMR approach
                                                             MMR re-ranking algorithm has been successfully
2.1   A query-oriented multi-document                        used in query-oriented summarization (Ye et al.,
      summarizer                                             2005). It strives to reduce redundancy while main-
                                                             taining query relevance in selected sentences. The
We have first started by implementing a simple
                                                             summary is constructed incrementally from a list
summarizer for which the task is to produce query-
                                                             of ranked sentences, at each iteration the sentence
focused summaries from clusters of documents.
                                                             which maximizes MMR is chosen:
Each document is pre-processed: documents are
segmented into sentences, sentences are filtered                MMR = arg max [ λ · Sim1 (s, Q)
(words which do not carry meaning are removed                                s∈S

such as functional words or common words) and                             − (1 − λ) · max Sim2 (s, sj ) ]     (3)
                                                                                        sj ∈E
normalized using a lemmas database (i.e. inflected
forms “go”, “goes”, “went”, “gone”... are replaced           where S is the set of candidates sentences and E
by “go”). An N -dimensional term-space Γ , where             is the set of selected sentences. λ represents an
N is the number of different terms found in the              interpolation coefficient between sentence’s rele-
cluster, is constructed. Sentences are represented           vance and non-redundancy. Sim2 (s, sj ) is a nor-
in Γ by vectors in which each component is the               malized Longest Common Substring (LCS) mea-
term frequency within the sentence. Sentence scor-           sure between sentences s and sj . Detecting sen-
ing can be seen as a passage retrieval task in Infor-        tence rehearsals, LCS is well adapted for redun-
mation Retrieval (IR). Each sentence s is scored by          dancy removal.
computing a combination of two similarity mea-                  We propose an interpretation of MMR to tackle
sures between the sentence and the query. The first          the update summarization issue. Since Sim1 and
measure is the well known cosine angle (Salton et            Sim2 are ranged in [0, 1], they can be seen as prob-
al., 1975) between the sentence and the query vec-           abilities even though they are not. Just as rewriting
torial representations in Γ (denoted respectively ~s         (3) as (NR stands for Novelty Relevance):
      ~ The second similarity measure is based
and Q).                                                        NR = arg max [ λ · Sim1 (s, Q)
on the Jaro-Winkler distance (Winkler, 1999). The                        s∈S
original Jaro-Winkler measure, denoted J W, uses                    + (1 − λ) · (1 − max Sim2 (s, sh )) ] (4)
                                                                                       sh ∈H
the number of matching characters and transposi-
tions to compute a similarity score between two              We can understand that (4) equates to an OR com-
terms, giving more favourable ratings to terms that          bination. But as we are looking for a more intu-
match from the beginning. We have extended this              itive AND and since the similarities are indepen-
measure to calculate the similarity between the              dent, we have to use the product combination. The

                                                        24
scoring method defined in (2) is modified into a                   3. An update summary of documents in C, un-
double maximization criterion in which the best                       der the assumption that the reader has already
ranked sentence will be the most relevant to the                      read documents in A and B.
query AND the most different to the sentences in
H.                                                               Within a topic, the document clusters must be pro-
                                                                 cessed in chronological order. Our system gener-
    S MMR(s) = Sim1 (s, Q)                                       ates a summary for each cluster by arranging the
                                      f (H)                    high ranked sentences until the limit of 100 words
               · 1 − max Sim2 (s, sh )        (5)                is reached.
                        sh ∈H
                                                                 3.2      Evaluation
Decreasing λ in (3) with the length of the sum-
                                                                 Most existing automated evaluation methods work
mary was suggested by (Murray et al., 2005) and
                                                                 by comparing the generated summaries to one or
successfully used in the DUC 2005 by (Hachey
                                                                 more reference summaries (ideally, produced by
et al., 2005), thereby emphasizing the relevance
                                                                 humans). To evaluate the quality of our generated
at the outset but increasingly prioritizing redun-
                                                                 summaries, we choose to use the ROUGE3 (Lin,
dancy removal as the process continues. Sim-
                                                                 2004) evaluation toolkit, that has been found to be
ilarly, we propose to follow this assumption in
                                                                 highly correlated with human judgments. ROUGE -
S MMR using a function denoted f that as the
                                                                 N is a n-gram recall measure calculated between
amount of data in history increases, prioritize non-
                                                                 a candidate summary and a set of reference sum-
redundancy (f (H) → 0).
                                                                 maries. In our experiments ROUGE -1, ROUGE -2
3        Experiments                                             and ROUGE - SU 4 will be computed.

The method described in the previous section has                 3.3      Results
been implemented and evaluated by using the                      Table 1 reports the results obtained on the DUC
DUC 2007 update corpus2 . The following subsec-                  2007 update data set for different sentence scor-
tions present details of the different experiments               ing methods. cosine + J We stands for the scor-
we have conducted.                                               ing method defined in (2) and NR improves it
                                                                 with sentence re-ranking defined in equation (4).
3.1       The DUC 2007 update corpus                             S MMR is the combined adaptation we have pro-
We used for our experiments the DUC 2007 up-                     posed in (5). The function f (H) used in S MMR is
date competition data set. The corpus is composed                the simple rational function H1 , where H increases
of 10 topics, with 25 documents per topic. The up-               with the number of previous clusters (f (H) = 1
date task goal was to produce short (∼100 words)                 for cluster A, 12 for cluster B and 31 for cluster C).
multi-document update summaries of newswire ar-                  This function allows to simply test the assumption
ticles under the assumption that the user has al-                that non-redundancy have to be favoured as the
ready read a set of earlier articles. The purpose                size of history grows. Baseline results are obtained
of each update summary will be to inform the                     on summaries generated by taking the leading sen-
reader of new information about a particular topic.              tences of the most recent documents of the cluster,
Given a DUC topic and its 3 document clusters: A                 up to 100 words (official baseline of DUC). The
(10 documents), B (8 documents) and C (7 doc-                    table also lists the three top performing systems at
uments), the task is to create from the documents                DUC 2007 and the lowest scored human reference.
three brief, fluent summaries that contribute to sat-               As we can see from these results, S MMR out-
isfying the information need expressed in the topic              performs the other sentence scoring methods. By
statement.                                                       ways of comparison our system would have been
                                                                 ranked second at the DUC 2007 update competi-
    1. A summary of documents in cluster A.                      tion. Moreover, no post-processing was applied to
    2. An update summary of documents in B, un-                  the selected sentences leaving an important margin
       der the assumption that the reader has already            of progress. Another interesting result is the high
       read documents in A.                                      performance of the non-update specific method
     2
                                                                 (cosine + J We ) that could be due to the small size
     More information about the DUC 2007 corpus is avail-
                                                                    3
able at http://duc.nist.gov/.                                           ROUGE is available at http://haydn.isi.edu/ROUGE/.

                                                            25
of the corpus (little redundancy between clusters).          Hachey, B., G. Murray, and D. Reitter. 2005. The
                                                               Embra System at DUC 2005: Query-oriented Multi-
                                                               document Summarization with a Very Large Latent
                    ROUGE -1   ROUGE -2   ROUGE - SU 4         Semantic Space. In Document Understanding Con-
                                                               ference (DUC).
    Baseline        0.26232    0.04543      0.08247
    3rd system      0.35715    0.09622      0.13245          Hickl, A., K. Roberts, and F. Lacatusu. 2007. LCC’s
                                                               GISTexter at DUC 2007: Machine Reading for Up-
    2nd system      0.36965    0.09851      0.13509
                                                               date Summarization. In Document Understanding
    cosine + J We   0.35905    0.10161      0.13701            Conference (DUC).
    NR              0.36207    0.10042      0.13781
     S MMR          0.36323    0.10223      0.13886          Lin, Z., T.S. Chua, M.Y. Kan, W.S. Lee, L. Qiu, and
                                                               S. Ye. 2007. NUS at DUC 2007: Using Evolu-
    1st system      0.37032    0.11189      0.14306            tionary Models of Text. In Document Understanding
    Worst human     0.40497    0.10511      0.14779            Conference (DUC).
                                                             Lin, C.Y. 2004. Rouge: A Package for Automatic
Table 1: ROUGE average recall scores computed                  Evaluation of Summaries. In Workshop on Text Sum-
on the DUC 2007 update corpus.                                 marization Branches Out, pages 25–26.
                                                             Mani, I. and M.T. Maybury. 1999. Advances in Auto-
                                                              matic Text Summarization. MIT Press.
4      Discussion and Future Work
                                                             Mani, I. and G. Wilson. 2000. Robust temporal pro-
In this paper we have described S MMR, a scal-                cessing of news. In 38th Annual Meeting on Asso-
able sentence scoring method based on MMR that                ciation for Computational Linguistics, pages 69–76.
achieves very promising results. An important as-             Association for Computational Linguistics Morris-
                                                              town, NJ, USA.
pect of our sentence scoring method is that it does
not requires re-ranking nor linguistic knowledge,            Murray, G., S. Renals, and J. Carletta. 2005. Extractive
which makes it a simple and fast approach to the              Summarization of Meeting Recordings. In Ninth Eu-
issue of update summarization. It was pointed out             ropean Conference on Speech Communication and
                                                              Technology. ISCA.
at the DUC 2007 workshop that Question Answer-
ing and query-oriented summarization have been               Salton, G., A. Wong, and C. S. Yang. 1975. A vector
converging on a common task. The value added                   space model for automatic indexing. Communica-
                                                               tions of the ACM, 18(11):613–620.
by summarization lies in the linguistic quality. Ap-
proaches mixing IR techniques are well suited for            Swan, R. and J. Allan. 2000. Automatic generation
query-oriented summarization but they require in-              of overview timelines. In 23rd annual international
tensive work for making the summary fluent and                 ACM SIGIR conference on Research and develop-
                                                               ment in information retrieval, pages 49–56.
coherent. Among the others, this is a point that we
think is worthy of further investigation.                    Winkler, W. E. 1999. The state of record linkage and
                                                               current research problems. In Survey Methods Sec-
Acknowledgments                                                tion, pages 73–79.
                                                             Witte, R., R. Krestel, and S. Bergler. 2007. Generat-
This work was supported by the Agence Nationale
                                                               ing Update Summaries for DUC 2007. In Document
de la Recherche, France, project RPM2.                         Understanding Conference (DUC).
                                                             Ye, S., L. Qiu, T.S. Chua, and M.Y. Kan. 2005. NUS
References                                                     at DUC 2005: Understanding documents via con-
                                                               cept links. In Document Understanding Conference
Boudin, F. and J.M. Torres-Moreno. 2007. A Co-                 (DUC).
  sine Maximization-Minimization approach for User-
  Oriented Multi-Document Update Summarization.
  In Recent Advances in Natural Language Processing
  (RANLP), pages 81–87.

Carbonell, J. and J. Goldstein. 1998. The use of MMR,
  diversity-based reranking for reordering documents
  and producing summaries. In 21st annual interna-
  tional ACM SIGIR conference on Research and de-
  velopment in information retrieval, pages 335–336.
  ACM Press New York, NY, USA.

                                                        26
