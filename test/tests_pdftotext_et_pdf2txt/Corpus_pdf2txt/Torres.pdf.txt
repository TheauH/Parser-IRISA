Summary Evaluation

with and without References

Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Vel´azquez-Morales

text

study
of

summarization

Abstract—We
evaluation

a new content-based method for
systems without
the
human models which is used to produce system rankings.
The research is carried out using a new content-based
evaluation framework called FRESA to compute a variety of
divergences among probability distributions. We apply our
comparison framework to various well-established content-based
evaluation measures in text summarization such as COVERAGE,
their
RESPONSIVENESS, PYRAMIDS
associations
text
including
generic multi-document summarization in English and French,
focus-based multi-document
summarization in English and
generic single-document summarization in French and Spanish.
Index Terms—Text summarization evaluation, content-based

summarization tasks

and ROUGE

in various

studying

evaluation measures, divergences.

I. INTRODUCTION

issue

complex and controversial

T EXT summarization evaluation has always been a

in computational
linguistics. In the last decade, signiﬁcant advances have been
made in this ﬁeld as well as various evaluation measures have
been designed. Two evaluation campaigns have been led by
the U.S. agence DARPA. The ﬁrst one, SUMMAC, ran from
1996 to 1998 under the auspices of the Tipster program [1],
and the second one, entitled DUC (Document Understanding
Conference) [2], was the main evaluation forum from 2000
until 2007. Nowadays, the Text Analysis Conference (TAC)
[3] provides a forum for assessment of different information
access technologies including text summarization.

Evaluation in text summarization can be extrinsic or
intrinsic [4]. In an extrinsic evaluation, the summaries are
assessed in the context of an speciﬁc task carried out by a
human or a machine. In an intrinsic evaluation, the summaries
are evaluated in reference to some ideal model. SUMMAC
was mainly extrinsic while DUC and TAC followed an
intrinsic evaluation paradigm. In an intrinsic evaluation, an

Manuscript received June 8, 2010. Manuscript accepted for publication July

25, 2010.

Juan-Manuel
and

France
(juan-manuel.torres@univ-avignon.fr).

Torres-Moreno
´Ecole

is with
Polytechnique

Eric

with
(eric.sanjuan@univ-avignon.fr).

SanJuan

is

LIA/Universit´e
de

Montr´eal,

d’Avignon,
Canada

LIA/Universit´e

d’Avignon,

France

Horacio Saggion is with DTIC/Universitat Pompeu Fabra, Spain

(horacio.saggion@upf.edu).

Iria da Cunha

is with IULA/Universitat Pompeu Fabra, Spain;
LIA/Universit´e d’Avignon, France and Instituto de Ingenier´ıa/UNAM, Mexico
(iria.dacunha@upf.edu).

is

with

VM

Labs,

France

Patricia

Vel´azquez-Morales
(patricia velazquez@yahoo.com).

automatically generated summary (peer) has to be compared
with one or more reference summaries (models). DUC used
an interface called SEE to allow human judges to compare
a peer with a model. Thus, judges give a COVERAGE score
to each peer produced by a system and the ﬁnal system
COVERAGE score is the average of the COVERAGE’s scores
asigned. These system’s COVERAGE scores can then be used
to rank summarization systems. In the case of query-focused
summarization (e.g. when the summary should answer a
question or series of questions) a RESPONSIVENESS score
is also assigned to each summary, which indicates how
responsive the summary is to the question(s).

Because manual comparison of peer summaries with model
summaries is an arduous and costly process, a body of
research has been produced in the last decade on automatic
content-based evaluation procedures. Early studies used text
similarity measures such as cosine similarity (with or without
weighting schema) to compare peer and model summaries
[5]. Various vocabulary overlap measures such as n-grams
overlap or longest common subsequence between peer and
model have also been proposed [6], [7]. The BLEU machine
translation evaluation measure [8] has also been tested in
summarization [9]. The DUC conferences adopted the ROUGE
package for content-based evaluation [10]. ROUGE implements
a series of recall measures based on n-gram co-occurrence
between a peer summary and a set of model summaries. These
measures are used to produce systems’ rank. It has been shown
that system rankings, produced by some ROUGE measures
(e.g., ROUGE-2, which uses 2-grams), have a correlation with
rankings produced using COVERAGE.

In recent years the PYRAMIDS evaluation method [11] has
been introduced. It is based on the distribution of “content”
of a set of model summaries. Summary Content Units (SCUs)
are ﬁrst identiﬁed in the model summaries, then each SCU
receives a weight which is the number of models containing
or expressing the same unit. Peer SCUs are identiﬁed in the
peer, matched against model SCUs, and weighted accordingly.
The PYRAMIDS score given to a peer is the ratio of the sum
of the weights of its units and the sum of the weights of the
best possible ideal summary with the same number of SCUs as
the peer. The PYRAMIDS scores can be also used for ranking
summarization systems. [11] showed that PYRAMIDS scores
produced reliable system rankings when multiple (4 or more)
models were used and that PYRAMIDS rankings correlate with
rankings produced by ROUGE-2 and ROUGE-SU2 (i.e. ROUGE
with skip 2-grams). However, this method requires the creation

13Polibits (42) 2010they did not

summarization

(DUC 2004 Task

of models and the identiﬁcation, matching, and weighting of
SCUs in both: models and peers.
[12] evaluated the effectiveness of the Jensen-Shannon
(J S) [13] theoretic measure in predicting systems ranks
in two summarization tasks: query-focused and update
summarization. They have shown that
ranks produced
and those produced by J S measure
by PYRAMIDS
correlate. However,
investigate the effect
the measure in summarization tasks such as generic
of
multi-document
2),
biographical summarization (DUC 2004 Task 5), opinion
summarization (TAC 2008 OS), and summarization in
languages other than English.
In this paper we present a series of experiments aimed at
a better understanding of the value of the J S divergence
for ranking summarization systems. We have carried out
experimentation with the proposed measure and we have
veriﬁed that
in certain tasks (such as those studied by
there is a strong correlation among PYRAMIDS,
[12])
RESPONSIVENESS and the J S divergence, but as we will
show in this paper, there are datasets in which the correlation
is not so strong. We also present experiments in Spanish
and French showing positive correlation between the J S
and ROUGE which is the de facto evaluation measure used
in evaluation of non-English summarization. To the best of
our knowledge this is the more extensive set of experiments
interpreting the value of evaluation without human models.

The rest of the paper is organized in the following way:
First in Section II we introduce related work in the area of
content-based evaluation identifying the departing point for
our inquiry; then in Section III we explain the methodology
adopted in our work and the tools and resources used for
experimentation. In Section IV we present the experiments
carried out together with the results. Section V discusses the
results and Section VI concludes the paper and identiﬁes future
work.

II. RELATED WORK

One of the ﬁrst works to use content-based measures in
text summarization evaluation is due to [5], who presented an
evaluation framework to compare rankings of summarization
systems produced by recall and cosine-based measures. They
showed that
there was weak correlation among rankings
produced by recall, but that content-based measures produce
rankings which were strongly correlated. This put forward
the idea of using directly the full document for comparison
purposes in text summarization evaluation. [6] presented a
set of evaluation measures based on the notion of vocabulary
overlap including n-gram overlap, cosine similarity, and
longest common subsequence, and they applied them to
summarization in English and Chinese.
multi-document
However,
the
measures in different summarization tasks. [7] also compared
various evaluation measures based on vocabulary overlap.
Although these measures were able to separate random from

they did not evaluate the performance of

(cid:88)

non-random systems, no clear conclusion was reached on the
value of each of the studied measures.

Nowadays,

summarization

a widespread

evaluation
framework is ROUGE [14], which offers a set of statistics
that compare peer
It counts
co-occurrences of n-grams in peer and models to derive a
score. There are several statistics depending on the used
n-grams and the text processing applied to the input texts
(e.g., lemmatization, stop-word removal).

summaries with models.

[15] proposed a method of evaluation based on the
use of “distances” or divergences between two probability
distributions (the distribution of units in the automatic
summary and the distribution of units
in the model
summary). They studied two different Information Theoretic
measures of divergence: the Kullback-Leibler (KL) [16] and
Jensen-Shannon (J S) [13] divergences. KL computes the
divergence between probability distributions P and Q in the
following way:

(1)

2Qw

1
2

DKL(P||Q) =

Pw log2
While J S divergence is deﬁned as follows:
DJ S (P||Q) =

(cid:88)

Pw log2

2Pw

w

Pw
Qw

+ Qw log2

1
2

w

Pw + Qw

Pw + Qw
(2)
These measures can be applied to the distribution of units in
system summaries P and reference summaries Q. The value
obtained may be used as a score for the system summary. The
method has been tested by [15] over the DUC 2002 corpus for
single and multi-document summarization tasks showing good
correlation among divergence measures and both coverage and
ROUGE rankings.

[12] went even further and, as in [5], they proposed to
compare directly the distribution of words in full documents
with the distribution of words in automatic summaries to
derive a content-based evaluation measure. They found a
high correlation between rankings produced using models
and rankings produced without models. This last work is the
departing point for our inquiry into the value of measures that
do not rely on human models.

III. METHODOLOGY

The followed methodology in this paper mirrors the one
adopted in past work (e.g. [5], [7], [12]). Given a particular
summarization task T , p data points to be summarized
with input material {Ii}p−1
i=0 (e.g. document(s), question(s),
topic(s)), s peer summaries {SUMi,k}s−1
k=0 for input i, and
m model summaries {MODELi,j}m−1
for input i, we will
compare rankings of the s peer summaries produced by various
evaluation measures. Some measures that we use compare
summaries with n of the m models:

j=0

MEASUREM (SUMi,k,{MODELi,j}n−1
j=0 )

(3)

Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales14Polibits (42) 2010while other measures compare peers with all or some of the
input material:

MEASUREM (SUMi,k, I(cid:48)
i)

(4)

i

where I(cid:48)
is some subset of input Ii. The values produced
by the measures for each summary SUMi,k are averaged
for each system k = 0, . . . , s − 1 and these averages are
used to produce a ranking. Rankings are then compared
using Spearman Rank correlation [17] which is used to
measure the degree of association between two variables
whose values are used to rank objects. We have chosen
to use this correlation to compare directly results to those
presented in [12]. Computation of correlations is done using
the Statistics-RankCorrelation-0.12 package1, which computes
the rank correlation between two vectors. We also veriﬁed
the good conformity of the results with the correlation test
of Kendall τ calculated with the statistical software R. The
two nonparametric tests of Spearman and Kendall do not
really stand out as the treatment of ex-æquo. The good
correspondence between the two tests shows that they do not
introduce bias in our analysis. Subsequently will mention only
the ρ of Sperman more widely used in this ﬁeld.

A. Tools

based

evaluation measures

We carry out experimentation using a new summarization
evaluation framework: FRESA –FRamework for Evaluating
Summaries Automatically–, which includes document-based
summary
probabilities
distribution2. As in the ROUGE package, FRESA supports
different n-grams and skip n-grams probability distributions.
The FRESA environment can be used in the evaluation of
summaries in English, French, Spanish and Catalan, and it
integrates ﬁltering and lemmatization in the treatment of
summaries and documents. It is developed in Perl and will
be made publicly available. We also use the ROUGE package
[10] to compute various ROUGE statistics in new datasets.

on

B. Summarization Tasks and Data Sets

We have conducted our experimentation with the following

summarization tasks and data sets:

1) Generic multi-document-summarization

in English
(production of a short summary of a cluster of related
documents) using data from DUC’043,
task 2: 50
clusters, 10 documents each – 294,636 words.

2) Focused-based summarization in English (production of
a short focused multi-document summary focused on the
question “who is X?”, where X is a person’s name) using
data from the DUC’04 task 5: 50 clusters, 10 documents
each plus a target person name – 284,440 words.

3) Update-summarization task that consists of creating a
summary out of a cluster of documents and a topic. Two
sub-tasks are considered here: A) an initial summary has
to be produced based on an initial set of documents and
topic; B) an update summary has to be produced from
a different (but related) cluster assuming documents
used in A) are known. The English TAC’08 Update
Summarization dataset is used, which consists of 48
topics with 20 documents each – 36,911 words.

4) Opinion summarization where systems have to analyze
a set of blog articles and summarize the opinions
about a target
in the articles. The TAC’08 Opinion
Summarization in English4 data set (taken from the
Blogs06 Text Collection) is used: 25 clusters and targets
(i.e., target entity and questions) were used – 1,167,735
words.

5) Generic single-document summarization in Spanish
using the Medicina Cl´ınica5 corpus, which is composed
of 50 medical articles in Spanish, each one with its
corresponding author abstract – 124,929 words.

6) Generic single document summarization in French using
the “Canadien French Sociological Articles” corpus
from the journal Perspectives interdisciplinaires sur le
travail et la sant´e (PISTES)6. It contains 50 sociological
articles in French, each one with its corresponding
author abstract – 381,039 words.

7) Generic multi-document-summarization in French using
data from the RPM27 corpus [18], 20 different themes
consisting of 10 articles and 4 abstracts by reference
thematic – 185,223 words.

For experimentation in the TAC and the DUC datasets we use
directly the peer summaries produced by systems participating
in the evaluations. For experimentation in Spanish and French
(single and multi-document summarization) we have created
summaries at a similar ratio to those of reference using the
following systems:

– ENERTEX [19], a summarizer based on a theory of

textual energy;

– CORTEX [20], a single-document sentence extraction
system for Spanish and French that combines various
statistical measures of relevance (angle between sentence
and topic, various Hamming weights for sentences, etc.)
and applies an optimal decision algorithm for sentence
selection;

– SUMMTERM [21], a terminology-based summarizer that
is used for summarization of medical articles and
uses specialized terminology for scoring and ranking
sentences;

– REG [22], summarization system based on an greedy

algorithm;

1http://search.cpan.org/∼gene/Statistics-RankCorrelation-0.12/
2FRESA is available at: http://lia.univavignon.fr/ﬁleadmin/axes/TALNE/

Ressources.html

3http://www-nlpir.nist.gov/projects/duc/guidelines/2004.html

4http://www.nist.gov/tac/data/index.html
5http://www.elsevier.es/revistas/ctl servlet? f=7032&revistaid=2
6http://www.pistes.uqam.ca/
7http://www-labs.sinequa.com/rpm2

Summary Evaluation with and without References15Polibits (42) 2010– J S summarizer, a summarization system that scores
and ranks sentences according to their Jensen-Shannon
divergence to the source document;

– a lead-based summarization system that selects the lead

sentences of the document;

– a random-based summarization system that

selects

sentences at random;

– Open Text Summarizer [23], a multi-lingual summarizer

based on the frequency and

– commercial systems: Word, SSSummarizer8, Pertinence9

and Copernic10.

C. Evaluation Measures

The following measures derived from human assessment of

the content of the summaries are used in our experiments:

– COVERAGE is understood as the degree to which one
peer summary conveys the same information as a model
summary [2]. COVERAGE was used in DUC evaluations.
This measure is used as indicated in equation 3 using
human references or models.

– RESPONSIVENESS ranks summaries in a 5-point scale
indicating how well
the summary satisﬁed a given
information need [2].
is used in focused-based
summarization tasks. This measure is used as indicated
in equation 4 since a human judges the summary
with respect
to a given input “user need” (e.g., a
question). RESPONSIVENESS was used in DUC and TAC
evaluations.

It

– PYRAMIDS [11] is a content assessment measure which
compares content units in a peer summary to weighted
content units in a set of model summaries. This
measure is used as indicated in equation 3 using human
references or models. PYRAMIDS is the adopted metric
for content-based evaluation in the TAC evaluations.

For DUC and TAC datasets the values of these measures are
available and we used them directly. We used the following
automatic evaluation measures in our experiments:

– ROUGE [14], which is a recall metric that takes into
account n-grams as units of content for comparing peer
and model summaries. The ROUGE formula speciﬁed in
[10] is as follows:

m ∈ M(cid:80)
(cid:80)
(cid:80)
m ∈ M(cid:80) count(n-gram)

ROUGE-n(R, M ) =
n−gram∈P countmatch(n − gram)

(5)

where R is the summary to be evaluated, M is the set of
model (human) summaries, countmatch is the number of
common n-grams in m and P , and count is the number
of n-grams in the model summaries. For the experiments

8http://www.kryltech.com/summarizer.htm
9http://www.pertinence.net
10http://www.copernic.com/en/products/summarizer

presented here we used uni-grams, 2-grams, and the skip
2-grams with maximum skip distance of 4 (ROUGE-1,
ROUGE-2 and ROUGE-SU4). ROUGE is used to compare
a peer summary to a set of model summaries in our
framework (as indicated in equation 3).

– Jensen-Shannon divergence formula given in Equation 2
is implemented in our FRESA package with the following
speciﬁcation (Equation 6) for the probability distribution
of words w.

(cid:40)

C T
w
Pw =
N
if w ∈ S
otherwise

CS
w
NS
CT
w +δ
N +δ∗B

(6)

Qw =

Where P is the probability distribution of words w in
text T and Q is the probability distribution of words w
in summary S; N is the number of words in text and
summary N = NT +NS, B = 1.5|V |, C T
w is the number
of words in the text and C S
w is the number of words in
the summary. For smoothing the summary’s probabilities
we have used δ = 0.005. We have also implemented
other smoothing approaches (e.g. Good-Turing [24], that
uses the CPAN Perl’s Statistics-Smoothing-SGT-2.1.2
package11)
in FRESA, but we do not use them in
the experiments reported here. Following the ROUGE
approach, in addition to word uni-grams we use 2-grams
and skip n-grams computing divergences such as J S
(using uni-grams) J S 2 (using 2-grams), J S 4 (using the
skip n-grams of ROUGE-SU4), and J SM which is an
average of the J S i. J Ss measures are used to compare a
peer summary to its source document(s) in our framework
(as indicated in equation 4). In the case of summarization
of multiple documents, these are concatenated (in the
given input order) to form a single input from which
probabilities are computed.

IV. EXPERIMENTS AND RESULTS

We ﬁrst replicated the experiments presented in [12] to
verify that our implementation of J S produced correlation
results compatible with that work. We used the TAC’08
Update Summarization data set and computed J S and
ROUGE measures for each peer summary. We produced
two system rankings (one for each measure), which were
compared to rankings produced using the manual PYRAMIDS
and RESPONSIVENESS scores. Spearman correlations were
computed among the different
rankings. The results are
presented in Table I. These results conﬁrm a high correlation
among PYRAMIDS, RESPONSIVENESS and J S. We also
veriﬁed high correlation between J S and ROUGE-2 (0.83
Spearman correlation, not shown in the table) in this task and
dataset.

Then, we experimented with data from DUC’04, TAC’08
task as well as single and

Opinion Summarization pilot

11http://search.cpan.org/∼bjoernw/Statistics-Smoothing-SGT-2.1.2/

Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales16Polibits (42) 2010SPEARMAN CORRELATION OF CONTENT-BASED MEASURES IN TAC’08

UPDATE SUMMARIZATION TASK

TABLE I

Mesure
ROUGE-2
J S

PYRAMIDS

0.96
0.85

p-value
p < 0.005
p < 0.005

RESPONSIVENESS

0.92
0.74

p-value
p < 0.005
p < 0.005

multi-document summarization in Spanish and French. In spite
of the fact that the experiments for French and Spanish corpora
use less data points (i.e., less summarizers per task) than
for English, results are still quite signiﬁcant. For DUC’04,
we computed the J S measure for each peer summary in
tasks 2 and 5 and we used J S, ROUGE, COVERAGE and
RESPONSIVENESS scores to produce systems’ rankings. The
various Spearman’s rank correlation values for DUC’04 are
presented in Tables II (for task 2) and III (for task 5).
For task 2, we have veriﬁed a strong correlation between
J S and COVERAGE. For task 5,
the correlation between
J S and COVERAGE is weak, and that between J S and
RESPONSIVENESS is weak and negative.

Although the Opinion Summarization (OS) task is a new
type of summarization task and its evaluation is a complicated
issue, we have decided to compare J S rankings with those
obtained using PYRAMIDS and RESPONSIVENESS in TAC’08.
Spearman’s correlation values are listed in Table IV. As it can
be seen, there is weak and negative correlation of J S with
both PYRAMIDS and RESPONSIVENESS. Correlation between
PYRAMIDS and RESPONSIVENESS rankings is high for this
task (0.71 Spearman’s correlation value).

For experimentation in mono-document summarization
in Spanish and French, we have run 11 multi-lingual
summarization systems; for experimentation in French, we
have run 12 systems. In both cases, we have produced
summaries at a compression rate close to the compression rate
of the authors’ provided abstracts. We have then computed J S
and ROUGE measures for each summary and we have averaged
the measure’s values for each system. These averages were
used to produce rankings per each measure. We computed
Spearman’s correlations for all pairs of rankings.
Results are presented in Tables V, VI and VII. All results
show medium to strong correlation between the J S measures
and ROUGE measures. However the J S measure based on
uni-grams has lower correlation than J Ss which use n-grams
table VII presents results for
of higher order. Note that
generic multi-document summarization in French,
in this
case correlation scores are lower than correlation scores for
single-document summarization in French, a result which may
be expected given the diversity of input in multi-document
summarization.

V. DISCUSSION

The departing point for our inquiry into text summarization
evaluation has been recent work on the use of content-based

evaluation metrics that do not rely on human models but that
compare summary content to input content directly [12]. We
have some positive and some negative results regarding the
direct use of the full document in content-based evaluation.

and

We have veriﬁed that
in

in both generic muti-document
summarization
topic-based multi-document
summarization in English correlation among measures
that use human models
(PYRAMIDS, RESPONSIVENESS
and ROUGE) and a measure that does not use models
(J S divergence) is strong. We have found that correlation
among the same measures is weak for summarization of
biographical information and summarization of opinions in
blogs. We believe that in these cases content-based measures
should be considered, in addition to the input document, the
summarization task (i.e. text-based representation, description)
to better assess the content of the peers [25], the task being a
determinant factor in the selection of content for the summary.
Our multi-lingual experiments in generic single-document
summarization conﬁrm a strong correlation among the
J S divergence and ROUGE measures. It
is worth noting
that ROUGE is
the chosen framework for
presenting content-based evaluation results in non-English
summarization.

in general

For the experiments in Spanish, we are conscious that we
only have one model summary to compare with the peers.
Nevertheless, these models are the corresponding abstracts
written by the authors. As the experiments in [26] show, the
professionals of a specialized domain (as, for example, the
medical domain) adopt similar strategies to summarize their
texts and they tend to choose roughly the same content chunks
for their summaries. Previous studies have shown that author
abstracts are able to reformulate content with ﬁdelity [27] and
these abstracts are ideal candidates for comparison purposes.
Because of this, the summary of the author of a medical article
can be taken as reference for summaries evaluation. It is worth
noting that there is still debate on the number of models to be
used in summarization evaluation [28]. In the French corpus
PISTES, we suspect the situation is similar to the Spanish
case.

VI. CONCLUSIONS AND FUTURE WORK

This paper has presented a series of experiments in
content-based measures that do not rely on the use of model
summaries for comparison purposes. We have carried out
extensive experimentation with different summarization tasks
drawing a clearer picture of tasks where the measures could
be applied. This paper makes the following contributions:

– We have shown that if we are only interested in ranking
summarization systems according to the content of their
automatic summaries, there are tasks were models could
be subtituted by the full document in the computation of
the J S measure obtaining reliable rankings. However,
we have also found that
the substitution of models
by full-documents is not always advisable. We have

Summary Evaluation with and without References17Polibits (42) 2010SPEARMAN ρ OF CONTENT-BASED MEASURES WITH COVERAGE IN DUC’04 TASK 2

TABLE II

Mesure
ROUGE-2
J S

COVERAGE

0.79
0.68

p-value

p < 0.0050
p < 0.0025

SPEARMAN ρ OF CONTENT-BASED MEASURES IN DUC’04 TASK 5

TABLE III

Mesure
ROUGE-2
J S

COVERAGE

0.78
0.40

p-value
p < 0.001
p < 0.050

RESPONSIVENESS

0.44
-0.18

p-value
p < 0.05
p < 0.25

SPEARMAN ρ OF CONTENT-BASED MEASURES IN TAC’08 OS TASK

TABLE IV

Mesure
J S

PYRAMIDS

-0.13

p-value
p < 0.25

RESPONSIVENESS

-0.14

p-value
p < 0.25

SPEARMAN ρ OF CONTENT-BASED MEASURES WITH ROUGE IN THE Medicina Cl´ınica CORPUS (SPANISH)

TABLE V

Mesure
J S
J S2
J S4
J SM

ROUGE-1

0.56
0.88
0.88
0.82

p-value
p < 0.100
p < 0.001
p < 0.001
p < 0.005

ROUGE-2

0.46
0.80
0.80
0.71

p-value
p < 0.100
p < 0.002
p < 0.002
p < 0.020

ROUGE-SU4

0.45
0.81
0.81
0.71

p-value
p < 0.200
p < 0.005
p < 0.005
p < 0.010

found weak correlation among different rankings in
complex summarization tasks such as the summarization
of biographical information and the summarization of
opinions.

– We have also carried out

large-scale experiments in
Spanish and French which show positive medium to
strong correlation among system’s ranks produced by
ROUGE and divergence measures that do not use the
model summaries.
– We have also presented a new framework, FRESA, for
the computation of measures based on J S divergence.
Following the ROUGE approach, FRESA package use
word uni-grams, 2-grams and skip n-grams computing
divergences. This framework will be available to the
community for research purposes.

Although we have made a number of contributions, this paper
leaves many open questions than need to be addressed. In
order to verify correlation between ROUGE and J S, in the
short term we intend to extend our investigation to other
languages such as Portuguese and Chinesse for which we
have access to data and summarization technology. We also
plan to apply FRESA to the rest of the DUC and TAC
summarization tasks, by using several smoothing techniques.
As a novel idea, we contemplate the possibility of adapting
the evaluation framework for the phrase compression task
[29], which, to our knowledge, does not have an efﬁcient
evaluation measure. The main idea is to calculate J S from
an automatically-compressed sentence taking the complete
sentence by reference. In the long term, we plan to incorporate

a representation of
the task/topic in the calculation of
measures. To carry out these comparisons, however, we are
dependent on the existence of references.

FRESA will also be used in the new question-answer task
campaign INEX’2010 (http://www.inex.otago.ac.nz/tracks/qa/
qa.asp) for the evaluation of long answers. This task aims
to answer a question by extraction and agglomeration of
sentences in Wikipedia. This kind of
task corresponds
to those for which we have found a high correlation
among the measures J S and evaluation methods with
human intervention. Moreover, the J S calculation will be
among the summaries produced and a representative set of
relevant passages from Wikipedia. FRESA will be used to
compare three types of systems, although different tasks: the
multi-document summarizer guided by a query, the search
systems targeted information (focused IR) and the question
answering systems.

ACKNOWLEDGMENT

We are grateful

to the Programa Ram´on y Cajal from
Ministerio de Ciencia e Innovaci´on, Spain. This work is
partially supported by: a postdoctoral grant from the National
Program for Mobility of Research Human Resources (National
Plan of Scientiﬁc Research, Development and Innovation
2008-2011, Ministerio de Ciencia e Innovaci´on, Spain); the
research project CONACyT, number 82050, and the research
project PAPIIT-DGAPA (Universidad Nacional Aut´onoma de
M´exico), number IN403108.

Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales18Polibits (42) 2010SPEARMAN ρ OF CONTENT-BASED MEASURES WITH ROUGE IN THE PISTES CORPUS (FRENCH)

TABLE VI

Mesure
J S
J S2
J S4
J SM

ROUGE-1

0.70
0.93
0.83
0.88

p-value
p < 0.050
p < 0.002
p < 0.020
p < 0.010

ROUGE-2

0.73
0.86
0.76
0.83

p-value
p < 0.05
p < 0.01
p < 0.05
p < 0.02

ROUGE-SU4

0.73
0.86
0.76
0.83

p-value
p < 0.500
p < 0.005
p < 0.050
p < 0.010

SPEARMAN ρ OF CONTENT-BASED MEASURES WITH ROUGE IN THE RPM2 CORPUS (FRENCH)

TABLE VII

Measure
J S
J S2
J S4
J SM

ROUGE-1

0.830
0.800
0.750
0.850

p-value

p < 0.002
p < 0.005
p < 0.010
p < 0.002

ROUGE-2

0.660
0.590
0.520
0.640

p-value
p < 0.05
p < 0.05
p < 0.10
p < 0.05

ROUGE-SU4

0.741
0.680
0.620
0.740

p-value
p < 0.01
p < 0.02
p < 0.05
p < 0.01

[18] C. de Loupy, M. Gu´egan, C. Ayache, S. Seng, and J.-M. Torres-Moreno,
“A French Human Reference Corpus
for multi-documents
summarization and sentence compression,” in LREC’10, vol. 2,
Malta, 2010, p. In press.

[19] S. Fernandez, E. SanJuan, and J.-M. Torres-Moreno, “Textual Energy
of Associative Memories: performants applications of Enertex algorithm
in text summarization and topic segmentation,” in MICAI’07, 2007, pp.
861–871.

[20] J.-M. Torres-Moreno, P. Vel´azquez-Morales, and J.-G. Meunier,
“Condens´es de textes par des m´ethodes num´eriques,” in JADT’02, vol. 2,
St Malo, France, 2002, pp. 723–734.

[21] J. Vivaldi, I. da Cunha, J.-M. Torres-Moreno, and P. Vel´azquez-Morales,
semantic

“Automatic
resources,” in LREC’10, vol. 2, Malta, 2010, p. In press.

summarization

terminological

using

[22] J.-M. Torres-Moreno and J. Ramirez, “REG : un algorithme glouton
appliqu´e au r´esum´e automatique de texte,” in JADT’10. Rome, 2010,
p. In press.

[23] V. Yatsko and T. Vishnyakov, “A method for evaluating modern
systems of automatic text summarization,” Automatic Documentation
and Mathematical Linguistics, vol. 41, no. 3, pp. 93–103, 2007.

[24] C. D. Manning and H. Sch¨utze, Foundations of Statistical Natural
Cambridge, Massachusetts: The MIT Press,

and

Language Processing.
1999.

[25] K. Sp¨arck Jones, “Automatic summarising: The state of the art,” IPM,

vol. 43, no. 6, pp. 1449–1481, 2007.

[26] I. da Cunha, L. Wanner, and M. T. Cabr´e, “Summarization of specialized
discourse: The case of medical articles in spanish,” Terminology, vol. 13,
no. 2, pp. 249–286, 2007.

[27] C.-K. Chuah, “Types of lexical substitution in abstracting,” in ACL
Toulouse, France: Association for

Student Research Workshop.
Computational Linguistics, 9-11 July 2001 2001, pp. 49–54.

[28] K. Owkzarzak and H. T. Dang, “Evaluation of automatic summaries:
Metrics under varying data conditions,” in UCNLG+Sum’09, Suntec,
Singapore, August 2009, pp. 23–30.

[29] K. Knight and D. Marcu, “Statistics-based summarization-step one:
Sentence compression,” in Proceedings of the National Conference on
Artiﬁcial Intelligence. Menlo Park, CA; Cambridge, MA; London;
AAAI Press; MIT Press; 1999, 2000, pp. 703–710.

REFERENCES

[1] I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, and
B. Sundheim, “Summac: a text summarization evaluation,” Natural
Language Engineering, vol. 8, no. 1, pp. 43–68, 2002.

[2] P. Over, H. Dang, and D. Harman, “DUC in context,” IPM, vol. 43,

no. 6, pp. 1506–1520, 2007.

[3] Proceedings of the Text Analysis Conference. Gaithesburg, Maryland,

USA: NIST, November 17-19 2008.

[4] K. Sp¨arck Jones and J. Galliers, Evaluating Natural Language
Processing Systems, An Analysis and Review, ser. Lecture Notes in
Computer Science. Springer, 1996, vol. 1083.

[5] R. L. Donaway, K. W. Drummey, and L. A. Mather, “A comparison of
rankings produced by summarization evaluation measures,” in NAACL
Workshop on Automatic Summarization, 2000, pp. 69–78.

[6] H. Saggion, D. Radev, S. Teufel, and W. Lam, “Meta-evaluation
of Summaries in a Cross-lingual Environment using Content-based
Metrics,” in COLING 2002, Taipei, Taiwan, August 2002, pp. 849–855.
[7] D. R. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer, H. Qi, A. C¸ elebi,
D. Liu, and E. Dr´abek, “Evaluation challenges in large-scale document
summarization,” in ACL’03, 2003, pp. 375–382.

[8] K. Papineni, S. Roukos, T. Ward, , and W. J. Zhu, “BLEU: a method
for automatic evaluation of machine translation,” in ACL’02, 2002, pp.
311–318.

[9] K. Pastra and H. Saggion, “Colouring summaries BLEU,” in Evaluation
Initiatives in Natural Language Processing. Budapest, Hungary: EACL,
14 April 2003.

[10] C.-Y. Lin, “ROUGE: A Package for Automatic Evaluation of
Summaries,” in Text Summarization Branches Out: ACL-04 Workshop,
M.-F. Moens and S. Szpakowicz, Eds., Barcelona, July 2004, pp. 74–81.
[11] A. Nenkova and R. J. Passonneau, “Evaluating Content Selection in
Summarization: The Pyramid Method,” in HLT-NAACL, 2004, pp.
145–152.

[12] A. Louis and A. Nenkova, “Automatically Evaluating Content Selection
in Summarization without Human Models,” in Empirical Methods in
Natural Language Processing, Singapore, August 2009, pp. 306–314.
[Online]. Available: http://www.aclweb.org/anthology/D/D09/D09-1032
[13] J. Lin, “Divergence Measures based on the Shannon Entropy,” IEEE

Transactions on Information Theory, vol. 37, no. 145-151, 1991.

[14] C.-Y. Lin and E. Hovy, “Automatic Evaluation of Summaries Using
N-gram Co-occurrence Statistics,” in HLT-NAACL. Morristown, NJ,
USA: Association for Computational Linguistics, 2003, pp. 71–78.

[15] C.-Y. Lin, G. Cao, J. Gao, and J.-Y. Nie, “An information-theoretic
approach to automatic evaluation of summaries,” in HLT-NAACL,
Morristown, USA, 2006, pp. 463–470.

[16] S. Kullback and R. Leibler, “On information and sufﬁciency,” Ann. of

Math. Stat., vol. 22, no. 1, pp. 79–86, 1951.

[17] S. Siegel and N. Castellan, Nonparametric Statistics for the Behavioral

Sciences. McGraw-Hill, 1998.

Summary Evaluation with and without References19Polibits (42) 2010