<preamble>
BLESS.pdf
</preamble>
<titre>
How we BLESSed distributional semantic evaluation
</titre>
<auteurs>
<auteur>Marco Baroni (marco.baroni@unitn.it)</auteur>
<affiliation>
University of Trento 
Trento, Italy 
</affiliation>
<auteur>Alessandro Lenci (alessandro.lenci@ling.unipi.it)</auteur>
<affiliation>
University of Pisa
Pisa, Italy
</affiliation>
</auteurs>
<abstract>
                       Abstract

     We introduce BLESS, a data set specifically
     designed for the evaluation of distributional
     semantic models. BLESS contains a set of tu-

     ples instantiating different, explicitly typed se-
     mantic relations, plus a number of controlled
     random tuples. It is thus possible to assess the
     ability of a model to detect truly related word
     pairs, as well as to perform in-depth analy-
     ses of the types of semantic relations that a
     model favors. We discuss the motivations for
     BLESS, describe its construction and struc-

     ture, and present examples of its usage in the
     evaluation of distributional semantic models.



</abstract>
<introduction>
1   Introduction
In NLP, it is customary to distinguish between in-
trinsic evaluations, testing a system in itself, and
extrinsic evaluations, measuring its performance in
some task or application (Sparck Jones and Galliers,
1996). For instance, the intrinsic evaluation of a de-
pendency parser will measure its accuracy in identi-
fying specific syntactic relations, while its extrinsic
evaluation will focus on the impact of the parser on
tasks such as question answering or machine trans-
lation. Current approaches to the evaluation of Dis-
tributional Semantic Models (DSMs, also known
as semantic spaces, vector-space models, etc.; see

Turney and Pantel (2010) for a survey) are task-
oriented. Model performance is evaluated in “se-
mantic tasks”, such as detecting synonyms, recog-
nizing analogies, modeling verb selectional prefer-
ences, ranking paraphrases, etc. Measuring the per-
formance of DSMs on such tasks represents an in-



       direct test of their ability to capture lexical mean-
       ing. The task-oriented benchmarks adopted in dis-
       tributional semantics have not specifically been de-
       signed to evaluate DSMs. For instance, the widely

       used TOEFL synonym detection task was designed

       to test the learners’ proficiency in English as a sec-
       ond language, and not to investigate the structure of
       their semantic representations (cf. Section 2).
          To gain a real insight into the abilities of DSMs to
       address lexical semantics, existing benchmarks must
       be complemented with a more intrinsically oriented

       approach, to perform direct tests on the specific as-


       pects of lexical knowledge captured by the models.
       In order to achieve this goal, three conditions must
       be met: (i) to single out the particular aspects of
       meaning that we want to focus on in the evaluation
       of DSMs; (ii) to design a data set that is able to ex-
       plicitly and reliably encode the target semantic infor-
       mation; (iii) to specify the evaluation criteria of the
       system performance on the data set, in order to get
       an estimate of the intrinsic ability of DSMs to cope
       with the selected semantic aspects. In this paper, we
       address these three conditions by presenting BLESS
       (Baroni and Lenci Evaluation of Semantic Spaces),
       a new data set specifically geared towards the in-
       trinsic evaluation of DSMs, downloadable from:
       http://clic.cimec.unitn.it/distsem.
</introduction>
<Results>
 The concept-by-concept z-normalized distributions
 of cosines of relata instantiating each of our rela-
 tions are presented, for each of the example mod-
 els, in Fig. 1. The RelatumFrequency baseline
 shows a preference for adjectives and verbs in gen-
 eral, independently of whether they are meaningful
 (attributes, events) or not (random adjectives and
 verbs), reflecting the higher frequencies of adjec-
 tives and verbs in BLESS (Table 1). The Relation-
 Cardinality baseline produces even less interesting
 results, with a strong preference for random nouns,
 followed by coordinates, events and random verbs
 (as predicted by the distribution in Table 1). We can
 conclude that the semantically meaningful patterns
 produced by the other models cannot be explained
 by trivial differences in relatum frequency or rela-
 tion cardinality in the BLESS data set.
     Moving then to the real DSMs, ContentWindow2
 essentially partitions the relations into 3 groups: co-
 ordinates are the closest relata, which makes sense
 since they are, taxonomically, the most similar en-
 tities to target concepts. They are followed by (but
 significantly closer to the concept than) events, hy-
 pernyms and meronyms (events and hypernyms sig-
 nificantly above meronyms). Next come the at-
 tributes (significantly lower cosines than all relation
 types above). All the meaningful relata are signif-
 icantly closer to the concepts than the random re-
 lata. Similar patterns can be observed in the Con-
 tentWindow20 distribution, however in this case the
 events, while still significantly below the coordi-
 nates, are significantly above the (statistically in-
 distinguishable) hypernym, meronym and attribute
 set. Again, all meaningful relata are above the ran-
 dom ones. Both content-window-based models pro-
 vide reasonable results, with ContentWindow2 be-
 ing probably closer to our “ontological” intuitions.
 The high ranking of events is probably explained
 by the fact that a nominal concept will often ap-
 pear as subject or object of verbs expressing asso-
 ciated events (dog barks, fishing tuna), and thus the
 corresponding verbs will share even relatively nar-
 row context windows with the concept noun. The
 AllWindow2 distribution probably reflects the fact
 that many contexts picked by this DSM are function
                           RelatumFrequency




                                                                          2





      2
                                                    ●







                                                    ●

                                                    ●
                                                    ●
                                                    ●





                                                                          1
                            ●

                     ●
                                                    ●





      1
                     ●
                     ●                              ●
                                                    ●
                                                    ●
                     ●                              ●
                     ●                              ●
                     ●                              ●
                     ●
                                                    ●
                     ●
                     ●      ●

                     ●




                                                                          0
                     ●      ●
                     ●
                            ●

                     ●      ●

                     ●





                     ●      ●


      0




                     ●      ●
                            ●
                            ●

             ●






                                                                          −1


                                                                                 ●





                                                                                 ●
                                                                                 ●
      −1




                                                                                 ●
                                                                                 ●
                            ●                                                    ●
                                                                                 ●
                                                                                 ●




                                                                          −2
                                                                                 ●






             ●
             ●
             ●
      −2




             ●




           COORD   HYPER   MERO   ATTRI   EVENT   RAN.N   RAN.J   RAN.V        COORD   HYPER



                           ContentWindow20






                                                                          2
                                    ●
                                    ●
      2




                                                                                         ●






                                                                    ●


                                                    ●       ●
                                                                    ●




                                                                          1

                                                            ●
      1




                                                                    ●





                                                                    ●


                                                    ●       ●
                                                            ●
                                                                    ●
                                                            ●       ●
                                                    ●

                                                    ●                            ●
                                                                                 ●


                                                                          0

                                                    ●
                                                    ●
      0




                                                    ●
                                                    ●


















                                                                                         ●
                                                                                         ●
                                                                          −1





      −1




             ●
                                                                                         ●
             ●

             ●                                                                           ●
             ●









             ●


                                                                          −2




                                                                    ●
      −2
















           COORD   HYPER   MERO   ATTRI   EVENT   RAN.N   RAN.J   RAN.V        COORD   HYPER




Figure 1: Distribution of relata cosines across concepts (values on ordinate are cosines after
normalization).


words, and thus they capture syntactic, rather than
semantic distributional properties. As a result, ran-
dom nouns are as high (statistically indistinguish-
able from) hypernyms and meronyms. Interestingly,
attributes also belong to this subset of relations –
probably due to the effect of determiners, quantifiers
and other DP-initial function words, that will often
occur both before nouns and before adjectives. In-
deed, even random adjectives, although significantly
below the other relations we discussed, are signif-
icantly above both random and meaningful verbs
(i.e., events). For the Document model, all mean-
ingful relations are significantly above the random
ones. However, coordinates, while still the nearest
neighbours (significantly closer than all other rela-
tions) are much less distinct than in the window-
based models. Note that we cannot say a priori that
ContentWindow2 is better than Document because
it favors coordinates. However, while they are both
able to sort out true and random relata, the latter
shows a weaker ability to discriminate among differ-
ent types of semantic relations (co-occurring within
a document is indeed a much looser cue to similarity
than specifically co-occurring within a narrow win-
dow). Traditional DSM tests, based on a single qual-



 RelationCardinality                                                   ContentWindow2





                                                                       ●










                                                 2
                                                                                       ●
                                                                                       ●

                                                                                                               ●



                                                                               ●               ●








                                                                               ●                       ●




                                                 1






                                                                                               ●

                                                                                               ●
                                                                                               ●               ●

                                                                                                       ●       ●
                                                                                                               ●
                                                                                                               ●







                                                        ●                                      ●
                                                                                               ●
                                                                                               ●       ●
                                                                                               ●       ●       ●
                   ●                                                                           ●




                                                 0
                                                        ●                                      ●       ●       ●
                   ●                                                                           ●       ●       ●
                   ●                                                                                   ●





                                                        ●

                   ●                                    ●
                                                        ●
                                           ●
                                           ●
                                           ●





            ●
            ●                              ●
            ●                              ●




                                                 −1

            ●






                                                                                               ●               ●













                                                 −2
                                                                                                       ●

                                                                                               ●






                                                                                               ●




 MERO    ATTRI   EVENT   RAN.N   RAN.J   RAN.V        COORD   HYPER   MERO   ATTRI   EVENT   RAN.N   RAN.J   RAN.V



        AllWindow2                                                           Document
                                                                               ●
                                                                               ●






  ●                                                                            ●










                                                 2
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●







                                                                                                               ●






                                                                               ●




                                                 1
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                                                               ●

                                                                                                               ●

                                                                                                       ●       ●
                                                                                                               ●




                                                                                                               ●

                                                                                                               ●






                                                                                                               ●
                                                                                                       ●       ●
                                                                                               ●               ●
                                                                                                               ●




                                                 0
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●       ●
                                                                                               ●       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                               ●
                           ●
  ●                        ●
  ●         ●





  ●





                           ●

  ●
            ●






                                                 −1
  ●
  ●         ●
  ●

  ●
  ●
  ●                                                                                            ●





                                                                                               ●





                                                                                               ●
                                                                                                       ●




                                                 −2
                                                                                               ●




 MERO    ATTRI   EVENT   RAN.N   RAN.J   RAN.V        COORD   HYPER   MERO   ATTRI   EVENT   RAN.N   RAN.J   RAN.V




 concept-by-concept z-



                 ity measure, would not have given us this broad view
                 of how models are behaving.
</Results>
<conclusion>
                 6         Conclusion

                 We introduced BLESS, the first data set specifically
                 designed for the intrinsic evaluation of DSMs. The
                 data set contains tuples instantiating different, ex-
                 plicitly typed semantic relations, plus a number of
                 controlled random tuples. Thus, BLESS can be used
                 to evaluate both the ability of DSMs to discriminate
                 truly related word pairs, and to perform in-depth
                 analyses of the types of semantic relata that different
                 models tend to favor among the nearest neighbors of
                 a target concept. Even a simple comparison of the
                 performance of a few DSMs on BLESS - like the
                 one we have shown here - is able to highlight inter-
                 esting differences in the semantic spaces produced
                 by the various models. The success of BLESS will
                 obviously depend on whether it will become a refer-
                 ence model for the evaluation of DSMs, something
                 that can not be foreseen a priori. Whatever its des-
                 tiny, we believe that the BLESS approach can boost
                 and innovate evaluation in distributional semantics,
                 as a key condition to get at a deeper understanding
                 of its potentialities as a viable model for meaning.
References

Herv Abdi and Lynne Williams. 2010. Newman-Keuls

   and Tukey test. In N.J. Salkind, D.M. Dougherty, and

   B. Frey, editors, Encyclopedia of Research Design.

   Sage, Thousand Oaks, CA.

Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana

   Kravalova, Marius Pasça, and Aitor Soroa. 2009. A

   study on similarity and relatedness using distributional

   and WordNet-based approaches. In Proceedings of

   HLT-NAACL, pages 19–27, Boulder, CO.

Abdulrahman Almuhareb. 2006. Attributes in Lexical
   Acquisition. Phd thesis, University of Essex.

Marco Baroni and Alessandro Lenci. 2010. Dis-

   tributional Memory: A general framework for
   corpus-based semantics. Computational Linguistics,
   36(4):673–721.

Marco Baroni, Stefan Evert, and Alessandro Lenci, edi-
   tors. 2008. Bridging the Gap between Semantic The-
   ory and Computational Simulations: Proceedings of
   the ESSLLI Workshop on Distributional Lexical Se-
   mantic. FOLLI, Hamburg.
Marco Baroni, Eduard Barbu, Brian Murphy, and Mas-
   simo Poesio. 2010. Strudel: A distributional semantic
   model based on properties and types. Cognitive Sci-
   ence, 34(2):222–254.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
   ating wordnet-based measures of lexical semantic re-
   latedness. Computational Linguistics, 32:13–47.
D. A. Cruse. 1986. Lexical Semantics. Cambridge Uni-
   versity Press, Cambridge.
Stefan Evert. 2005. The Statistics of Word Cooccur-
   rences. Dissertation, Stuttgart University.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
   tronic Lexical Database. MIT Press, Cambridge, MA.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
   Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
   Ruppin. 2002. Placing search in context: The concept
   revisited. ACM Transactions on Information Systems,
   20(1):116–131.
Thomas Landauer and Susan Dumais. 1997. A solu-
   tion to Plato’s problem: The latent semantic analysis
   theory of acquisition, induction, and representation of
   knowledge. Psychological Review, 104(2):211–240.
Hugo Liu and Push Singh. 2004. ConceptNet: A prac-
   tical commonsense reasoning toolkit. BT Technology
   Journal, pages 211–226.
Kevin Lund and Curt Burgess. 1996. Producing
   high-dimensional semantic spaces from lexical co-
   occurrence. Behavior Research Methods, 28:203–208.
Ken McRae, George Cree, Mark Seidenberg, and Chris
   McNorgan. 2005. Semantic feature production norms
   for a large set of living and nonliving things. Behavior
   Research Methods, 37(4):547–559.


                                                        10
 George Miller and Walter Charles. 1991. Contextual cor-
   relates of semantic similarity. Language and Cogni-

   tive Processes, 6(1):1–28.

 Gregory Murphy. 2002. The Big Book of Concepts. MIT

   Press, Cambridge, MA.

 Timothy Rogers and James McClelland. 2004. Seman-

   tic Cognition: A Parallel Distributed Processing Ap-

   proach. MIT Press, Cambridge, MA.

 Herbert Rubenstein and John Goodenough. 1965. Con-

   textual correlates of synonymy. Communications of

   the ACM, 8(10):627–633.

 Karen Sparck Jones and Julia R. Galliers. 1996. Evaluat-
   ing Natural Language Processing Systems: An Analy-

   sis and Review. Springer Verlag, Berlin.

 Peter Turney and Patrick Pantel. 2010. From frequency
   to meaning: Vector space models of semantics. Jour-
   nal of Artificial Intelligence Research, 37:141–188.

 Peter Turney. 2006. Similarity of semantic relations.
   Computational Linguistics, 32(3):379–416.
 Morton E. Winston, Roger Chaffin, and Douglas Her-
   rmann. 1987. A taxonomy of part-whole relations.
   Cognitive Science, 11:417–444.

































</conclusion>
<Reference : >
References

Herv Abdi and Lynne Williams. 2010. Newman-Keuls

   and Tukey test. In N.J. Salkind, D.M. Dougherty, and

   B. Frey, editors, Encyclopedia of Research Design.

   Sage, Thousand Oaks, CA.

Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana

   Kravalova, Marius Pasça, and Aitor Soroa. 2009. A

   study on similarity and relatedness using distributional

   and WordNet-based approaches. In Proceedings of

   HLT-NAACL, pages 19–27, Boulder, CO.

Abdulrahman Almuhareb. 2006. Attributes in Lexical
   Acquisition. Phd thesis, University of Essex.

Marco Baroni and Alessandro Lenci. 2010. Dis-

   tributional Memory: A general framework for
   corpus-based semantics. Computational Linguistics,
   36(4):673–721.

Marco Baroni, Stefan Evert, and Alessandro Lenci, edi-
   tors. 2008. Bridging the Gap between Semantic The-
   ory and Computational Simulations: Proceedings of
   the ESSLLI Workshop on Distributional Lexical Se-
   mantic. FOLLI, Hamburg.
Marco Baroni, Eduard Barbu, Brian Murphy, and Mas-
   simo Poesio. 2010. Strudel: A distributional semantic
   model based on properties and types. Cognitive Sci-
   ence, 34(2):222–254.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
   ating wordnet-based measures of lexical semantic re-
   latedness. Computational Linguistics, 32:13–47.
D. A. Cruse. 1986. Lexical Semantics. Cambridge Uni-
   versity Press, Cambridge.
Stefan Evert. 2005. The Statistics of Word Cooccur-
   rences. Dissertation, Stuttgart University.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
   tronic Lexical Database. MIT Press, Cambridge, MA.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
   Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
   Ruppin. 2002. Placing search in context: The concept
   revisited. ACM Transactions on Information Systems,
   20(1):116–131.
Thomas Landauer and Susan Dumais. 1997. A solu-
   tion to Plato’s problem: The latent semantic analysis
   theory of acquisition, induction, and representation of
   knowledge. Psychological Review, 104(2):211–240.
Hugo Liu and Push Singh. 2004. ConceptNet: A prac-
   tical commonsense reasoning toolkit. BT Technology
   Journal, pages 211–226.
Kevin Lund and Curt Burgess. 1996. Producing
   high-dimensional semantic spaces from lexical co-
   occurrence. Behavior Research Methods, 28:203–208.
Ken McRae, George Cree, Mark Seidenberg, and Chris
   McNorgan. 2005. Semantic feature production norms
   for a large set of living and nonliving things. Behavior
   Research Methods, 37(4):547–559.


                                                        10
 George Miller and Walter Charles. 1991. Contextual cor-
   relates of semantic similarity. Language and Cogni-

   tive Processes, 6(1):1–28.

 Gregory Murphy. 2002. The Big Book of Concepts. MIT

   Press, Cambridge, MA.

 Timothy Rogers and James McClelland. 2004. Seman-

   tic Cognition: A Parallel Distributed Processing Ap-

   proach. MIT Press, Cambridge, MA.

 Herbert Rubenstein and John Goodenough. 1965. Con-

   textual correlates of synonymy. Communications of

   the ACM, 8(10):627–633.

 Karen Sparck Jones and Julia R. Galliers. 1996. Evaluat-
   ing Natural Language Processing Systems: An Analy-

   sis and Review. Springer Verlag, Berlin.

 Peter Turney and Patrick Pantel. 2010. From frequency
   to meaning: Vector space models of semantics. Jour-
   nal of Artificial Intelligence Research, 37:141–188.

 Peter Turney. 2006. Similarity of semantic relations.
   Computational Linguistics, 32(3):379–416.
 Morton E. Winston, Roger Chaffin, and Douglas Her-
   rmann. 1987. A taxonomy of part-whole relations.
   Cognitive Science, 11:417–444.

































</Reference : >
