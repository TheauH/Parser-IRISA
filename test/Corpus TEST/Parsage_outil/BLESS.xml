<preamble>
BLESS.pdf
</preamble>
<titre>
How we BLESSed distributional semantic evaluation
</titre>
<auteurs>
<auteur>Marco Baroni (marco.baroni@unitn.it)</auteur>
<affiliation>
University of Trento 
Trento, Italy 
</affiliation>
<auteur>Alessandro Lenci (alessandro.lenci@ling.unipi.it)</auteur>
<affiliation>
University of Pisa
Pisa, Italy
</affiliation>
</auteurs>
<abstract>
                       Abstract

     We introduce BLESS, a data set specifically
     designed for the evaluation of distributional
     semantic models. BLESS contains a set of tu-

     ples instantiating different, explicitly typed se-
     mantic relations, plus a number of controlled
     random tuples. It is thus possible to assess the
     ability of a model to detect truly related word
     pairs, as well as to perform in-depth analy-
     ses of the types of semantic relations that a
     model favors. We discuss the motivations for
     BLESS, describe its construction and struc-

     ture, and present examples of its usage in the
     evaluation of distributional semantic models.



</abstract>
<introduction>
1   Introduction
In NLP, it is customary to distinguish between in-
trinsic evaluations, testing a system in itself, and
extrinsic evaluations, measuring its performance in
some task or application (Sparck Jones and Galliers,
1996). For instance, the intrinsic evaluation of a de-
pendency parser will measure its accuracy in identi-
fying specific syntactic relations, while its extrinsic
evaluation will focus on the impact of the parser on
tasks such as question answering or machine trans-
lation. Current approaches to the evaluation of Dis-
tributional Semantic Models (DSMs, also known
as semantic spaces, vector-space models, etc.; see

Turney and Pantel (2010) for a survey) are task-
oriented. Model performance is evaluated in “se-
mantic tasks”, such as detecting synonyms, recog-
nizing analogies, modeling verb selectional prefer-
ences, ranking paraphrases, etc. Measuring the per-
formance of DSMs on such tasks represents an in-



       direct test of their ability to capture lexical mean-
       ing. The task-oriented benchmarks adopted in dis-
       tributional semantics have not specifically been de-
       signed to evaluate DSMs. For instance, the widely

       used TOEFL synonym detection task was designed

       to test the learners’ proficiency in English as a sec-
       ond language, and not to investigate the structure of
       their semantic representations (cf. Section 2).
          To gain a real insight into the abilities of DSMs to
       address lexical semantics, existing benchmarks must
       be complemented with a more intrinsically oriented

       approach, to perform direct tests on the specific as-


       pects of lexical knowledge captured by the models.
       In order to achieve this goal, three conditions must
       be met: (i) to single out the particular aspects of
       meaning that we want to focus on in the evaluation
       of DSMs; (ii) to design a data set that is able to ex-
       plicitly and reliably encode the target semantic infor-
       mation; (iii) to specify the evaluation criteria of the
       system performance on the data set, in order to get
       an estimate of the intrinsic ability of DSMs to cope
       with the selected semantic aspects. In this paper, we
       address these three conditions by presenting BLESS
       (Baroni and Lenci Evaluation of Semantic Spaces),
       a new data set specifically geared towards the in-
       trinsic evaluation of DSMs, downloadable from:
       http://clic.cimec.unitn.it/distsem.
</introduction>
<corps>
       2   Distributional semantics benchmarks

       There are several benchmarks that have been widely
       adopted for the evaluation of DSMs, all of them cap-
       turing interesting challenges a DSM should meet.
       We briefly review here some commonly used and
       representative benchmarks, and discuss why we felt

  1

Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics, EMNLP 2011, pages 1–10,
the need to add BLESS to the set. We notice at the
outset of this discussion that we want to carve out a
space for BLESS, and not to detract from the impor-
tance and usefulness of other data sets. We further
remark that we focus on data sets that, like BLESS,
are monolingual English and, while task-oriented,
not aimed at a specific application setting (such as
machine translation or ontology population).
    Probably the most commonly used benchmark in
distributional semantics is the TOEFL synonym de-
tection task introduced to computational linguis-
tics by Landauer and Dumais (1997). It consists of
80 multiple-choice questions, each made of a target
word (a noun, verb, adjective or adverb) and 4 re-
sponse words, 1 of them a synonym of the target.
For example, given the target levied, the matched
words are imposed, believed, requested, correlated,
the first one being the correct choice. The task for
a system is then to pick the true synonym among
the responses. The TOEFL task focuses on a single
semantic relation, namely synonymy. Synonymy is
actually not a common semantic relation and one of
the hardest to define, to the point that many lexi-
cal semanticists have concluded that true synonymy
does not exist (Cruse, 1986). Just looking at a few
examples of synonym pairs from the TOEFL set will
illustrate the problem: discrepancy/difference, pro-
lific/productive, percentage/proportion, to market/to
sell, color/hue. Moreover, the criteria adopted to
choose the distractors (probably motivated by the
language proficiency testing purposes of TOEFL)
are not known. By looking at the set, it is hard
to discern a coherent pattern. In certain cases, the
distractors are semantically close to the target word
(volume, sample and profit for percentage), whereas
in other cases they are not (home, trail, and song for
annals). It it thus not clear whether we are asking the
models to distinguish a semantically related word
(the synonym) from random elements, or a more
tightly related word (the synonym, again) from other
related words. The TOEFL task, finally, is based on
a discrete choice (either you get the right word, or
you don’t), with the result that evaluation is “quan-
tized”, leading to large accuracy gains for small ac-
tual differences (one model that guesses one more
synonym right than another gets 1.25% more points
in percentage accuracy).
    The WordSim 353 data set (Finkelstein et al.,

                                                     2
 2002) is a widely used example of semantic simi-
 larity rating set (see also Rubenstein and Goode-
 nough (1965) and Miller and Charles (1991)). Sub-
 jects were asked to rate a set of 353 word pairs on a
 “similarity” scale and average ratings for each pair
 were computed. Models are then evaluated in terms
 of correlation of their similarity scores with aver-
 age ratings across pairs. From the point of view
 of assessing the performance of a DSM, the Word-
 Sim (and related) similarity ratings are a mixed bag,
 in two senses. First, the data set contains a vari-
 ety of different semantic relations. In a recent se-
 mantic annotation of the WordSim performed by
 Agirre et al. (2009) we find that, among the 174
 pairs with above-median score (and thus presum-
 ably related), there is 1 identical pair, 17 synonym
 pairs, 28 hyper-/hyponym pairs, 30 coordinate pairs,
 6 holo-/meronym pairs and 92 (more than half) pairs
 that are “topically related, but none of the above”.
 Second, the scores are a mixture of intuitions about
 which of these relations are more semantically tight
 and intuitions about more or less connected pairs
 within each of the relations. For example, among
 the top-rated scores we find synonyms such as jour-
 ney/voyage and coordinate concepts (king/queen).
 If we look at the relations characterizing pairs
 around the median rating, we find both less “per-
 fect” synonyms (monk/brother, that are synonymous
 only under an unusual sense of brother) and less
 close coordinates (skin/eye), as well as pairs in-
 stantiating other, less taxonomically tight relations,
 such as many syntagmatically connected items (fam-
 ily/planning, disaster/area, bread/butter). Appar-
 ently, a single scale is merging intuitions about se-
 mantic similarity of specific pairs and semantic sim-
 ilarity of different relations.
    A perhaps more principled way to evaluate DSMs
 that has recently gained some popularity is the con-
 cept categorization task, where a DSM has to clus-
 ter a set of nouns expressing basic-level concepts
 into gold standard categories. A particularly care-
 fully constructed example is the Almuhareb-Poesio
 (AP) set of 402 concepts introduced in Almuhareb
 (2006). Concept categorization sets also include the
 Battig (Baroni et al., 2010) and ESSLLI 2008 (Ba-
 roni et al., 2008) lists. The AP concepts must be
 clustered into 21 classes, each represented by be-
 tween 13 and 21 nouns. Examples include the ve-

hicle class (helicopter, motorcycle. . . ), the motiva-
tion class (ethics, incitement, . . . ), and the social
unit class (platoon, branch). The concepts are bal-
anced in terms of frequency and ambiguity, so that,
e.g., the tree class contains a common concept such
as pine but also the casuarina tree, as well as the
samba tree, that is not only an ambiguous term, but
one where the non-arboreal sense dominates.
   Concept categorization data sets, while interest-
ing to simulate one of the basic aspects of human
cognition, are limited to one kind of semantic re-
lation (discovering coordinates). More importantly,
the quality of the results will depend not only on the
underlying DSMs, but also on the clustering algo-
rithm being used (and on how this interacts with the
overall structure of the DSM), thus making it hard
to interpret the performance of DSMs. The forced
“hard” category choice is also problematic, and ex-
aggerates performance differences between models
especially in the presence of ambiguous terms (a
model that puts samba in the occasion class with
dance and ball might be penalized as much as a
model that puts it in the monetary currency class).
   A more general issue with all benchmarks is that
tasks are based on comparing a single quality score
for each considered model (accuracy for TOEFL,
correlation for WordSim, a clustering quality mea-
sure for AP, etc.). This gives little insight into how
and why the models differ. Moreover, there is no
well-established statistical procedure to assess sig-
nificance of differences for most commonly used
measures. Finally, either because the data sets were
not originally intended as standard benchmarks, or
even on purpose, they all are likely to cause coverage
problems even for DSMs trained on very large cor-
pora. Think of the presence of extremely rare nouns
like casuarina in AP, of proper nouns in WordSim (it
is not clear to us that DSMs are adequate semantic
models for referring expressions – at the very least
they should not be mixed up lightly with common
nouns), or multi-word expressions in other data sets.

3   How we intend to BLESS distributional
    semantic evaluation

DSMs measure the distributional similarity between
words, under the assumption that proximity in distri-
butional space models semantic relatedness, includ-

                                                     3
 ing, as a special case, semantic similarity (Budanit-
 sky and Hirst, 2006). However, semantically related
 words in turn differ for the type of relation hold-
 ing between them: e.g., dog is strongly related to
 both animal and tail, but with different types of re-
 lations. Therefore, evaluating the intrinsic ability of
 DSMs to represent the semantic space of a word en-
 tails both (i) determining to what extent words close
 in semantic space are actually semantically related,
 and (ii) analyzing, among related words, which type
 of semantic relation they tend to instantiate. Two
 models can be equally very good in identifying se-
 mantically related words, while greatly differing for
 the type of related pairs they favor.
     The BLESS data set complies with both these
 constraints. The set is populated with tuples ex-
 pressing a relation between a target concept (hence-
 forth referred to as concept) and a relatum concept
 (henceforth referred to as relatum). For instance, in
 the BLESS tuple coyote-hyper-animal, the concept
 coyote is linked to the relatum animal via the hy-
 pernymy relation (the relatum is a hypernym of the
 concept). BLESS focuses on a coherent set of basic-
 level nominal concrete concepts and a small but ex-
 plicit set of semantic relations, each instantiated by
 multiple relata. Depending on the type of relation,
 relata can be nouns, verbs or adjectives. Moreover,
 BLESS also contains, for each concept, a number of
 random “relatum” words that are not semantically
 related to the concept. Thus, it also allows to evalu-
 ate a model in terms of its ability to harvest related
 words given a concept (by comparing true and ran-
 dom relata), and to identify specific types of relata,
 both in terms of semantic relation and part of speech.
     A data set intending to represent a gold standard
 for evaluation should include tests items that are as
 little controversial as possible. The choice of re-
 stricting BLESS to concrete concepts is motivated
 by the fact that they are by far the most studied ones,
 and there is better agreement about the relations that
 characterize them (Murphy, 2002; Rogers and Mc-
 Clelland, 2004).
     As for the types of relation to include, we are
 faced with a dilemma. On the one hand, there is
 wide evidence that taxonomic relations, the best un-
 derstood type, only represent a tiny portion of the
 rich spectrum covered by semantic relatedness. On
 the other hand, most of these wider semantic rela-

tions are also highly controversial, and may easily
lead to questionable classifications. For instance,
concepts are related to events, but often it is not clear
how to distinguish the events expressing a typical
function of nominal concepts (e.g., car and trans-
port), from those events that are also strongly re-
lated to them but without representing their typical
function sensu stricto (e.g., car and fix). As will be
shown in Section 4, the BLESS data set tries to over-
come this dilemma by attempting a difficult com-
promise: Semantic relations are not limited to tax-
onomic types and also include attributes and events
strongly related to a concept, but in these cases we
have resorted to underspecification, rather than com-
mitting ourselves to questionable granular relations.
   BLESS strives to capture those differences and
similarities among DSMs that do not depend on
coverage, processing choices or lexical preferences.
BLESS has been constructed using a publicly avail-
able collection of corpora for reference (see Section
4.4 below), which means that anybody can train a
DSM on the same data and be sure to have perfect
coverage (but this is not strictly necessary). For each
concept and relation, we pick a variety of relata (see
next section) in order to abstract away from inciden-
tal gaps of models or different lexical/topical prefer-

ences. For example, the concept robin has 7 hyper-
nyms including the very general and non-technical
animal and bird and the more specific and techni-
cal passerine. A model more geared toward techni-
cal terminology might assign a high similarity score
to the latter, whereas a commonsense-knowledge-
oriented DSM might pick bird. Both models have
captured similarity with a hypernym, and we have
no reason, in general semantic terms, to penalize one
or the other. To maximize coverage, we also make
sure that, for each concept and relation, a reason-
able number of relata are frequently attested in our
reference corpora (see statistics below), we only in-
clude single-word relata and, where appropriate, we
include multiple forms for the same relatum (both
sock and socks as coordinates of scarf – as discussed
in Section 4.1, we avoided similar ambiguous items
as target concepts).
   Currently, distributional models for attributional
similarity and relational similarity (Turney, 2006)
are tested on different data sets, e.g., TOEFL and
SAT respectively (briefly, attributional similarity

                                                       4
 pertains to similarity between a pair of concepts in
 terms of shared properties, whereas relational sim-
 ilarity measures the similarity of the relations in-
 stantiated by couples of concept pairs). Conversely,
 BLESS is not biased towards any particular type of
 semantic similarity and thus allows both families of
 models to be evaluated on the same data set. Given
 a concept, we can analyze the types of relata that are
 selected by a model as more attributionally similar
 to the target. Alternatively, given a concept-relatum
 pair instantiating a specific semantic relation (e.g.,
 hypernymy) we can evaluate a model ability to iden-
 tify analogically similar pairs, i.e., others concept-
 relatum pairs instantiating the same relation (we do
 not illustrate this possibility here).
    Finally, by collecting distributions of 200 similar-
 ity values for each relation, BLESS allows reliable
 statistical testing of the significance of differences
 in similarity within a DSM (for example, using the
 procedure we present in Section 5 below), as well
 as across DSMs (for example, via a linear/ANOVA
 model with relations and DSMs as factors – not il-
 lustrated here).

 4     Construction

 4.1    Concepts

 BLESS includes 200 distinct English concrete
 nouns as target concepts, equally divided be-
 tween living and non-living entities. Concepts
 have been grouped into 17 broader classes: AM -
 PHIBIAN REPTILE (including amphibians and rep-
 tiles: alligator), APPLIANCE (toaster), BIRD
 (crow), BUILDING (cottage), CLOTHING (sweater),
 CONTAINER (bottle), FRUIT (banana), FURNI -
 TURE (chair), GROUND MAMMAL (beaver), IN -
 SECT (cockroach), MUSICAL INSTRUMENT (vio-
 lin), TOOL (i.e., manipulable tools or devices: ham-
 mer), TREE (birch), VEGETABLE (cabbage), VEHI -
 CLE (bus), WATER ANIMAL (including fish and sea
 mammals: herring), WEAPON (dagger).
    All 200 BLESS concepts are single-word nouns
 in the singular form (we avoided concepts such as
 socks whose surface form might change depending
 on lemmatization choices). The major source we
 used to select the concepts were the McRae Norms
 (McRae et al., 2005), a collection of living and non-
 living basic-level concepts described by 725 sub-

jects with semantic features, each tagged with its
property type. As further constraints guiding our
selection, we wanted concepts with a reasonably
high frequency (cf. Section 4.4), we avoided am-
biguous or highly polysemous concepts and we bal-
anced inter- and intra-class composition. Classes in-
clude both prototypical and atypical instances (e.g.,
robin and penguin for BIRD), and have a wide spec-
trum of internal variation (e.g., the class VEHICLE
contains wheeled, air and sea vehicles). 175 BLESS
concepts are attested in the McRae Norms, while the
remnants were selected by the authors according to
the above constraints. The average number of con-
cepts per class is 11.76 (median 11; min. 5 AMPHIB -
IAN REPTILE; max. 21 GROUND MAMMAL ).

4.2   Relations
For each concept noun, BLESS includes several
relatum words, linked to the concept by one of
the following 5 relations. COORD: the relatum
is a noun that is a co-hyponym (coordinate) of
the concept, i.e., they belong to the same (nar-
rowly or broadly defined) semantic class: alligator-
coord-lizard; HYPER: the relatum is a noun that
is a hypernym of the concept: alligator-hyper-
animal; MERO: the relatum is a noun referring
to a part/component/organ/member of the concept,
or something that the concept contains or is made
of: alligator-mero-mouth; ATTRI: the relatum is
an adjective expressing an attribute of the concept:
alligator-attri-aquatic; EVENT: the relatum is a
verb referring to an action/activity/happening/event
the concept is involved in or is performed by/with
the concept: alligator-event-swim. BLESS also
includes the relations RAN . N, RAN . J and RAN . V,
which relate the target concepts to control tuples
with random noun, adjective and verb relata, respec-
tively.
   The BLESS relations cover a wide spectrum of
information useful to describe a target concept and
to qualify the notion of semantic relatedness: taxo-
nomically related entities (hyper and coord), typical
attributes (attri), components (mero), and associated
events (event). However, except for hyper and co-
ord (corresponding to the standard relations of class
inclusion and co-hyponymy respectively), the other
BLESS relations are highly underspecified. For in-
stance, mero corresponds to a very broad notion of

                                                   5
 meronymy, including not only parts (dog-tail), but
 also the material (table-wood) as well as the mem-
 bers (hospital-patient) of the entity the target con-
 cept refers to (Winston et al., 1987); event is used to
 represent the behaviors of animals (dog-bark), typi-
 cal functions of instruments (violin-play), and events
 that are simply associated with the target concept
 (car-park); attri captures a large range of attributes,
 from physical (elephant-big) to evaluative ones (car-
 expensive). As we said in section 3, we did not at-
 tempt to further specify these relations to avoid any
 commitment to controversial ontologies of property
 types. Note that we exclude synonymy both because
 of the inherent problems in this very notion (Cruse,
 1986), and because it is impossible to find convinc-
 ing synonyms for 200 concrete concepts.
    In BLESS, we have adopted the simplifying as-
 sumption that each relation type has relata belonging
 to the same part of speech: nouns for hyper, coord
 and mero, verbs for event, and adjectives for attri.
 Therefore, we abstract away from the fact that the
 same semantic relation can be realized with different
 parts of speech, e.g., a related event can be expressed
 by a verb (transport) or by a noun (transportation).

 4.3   Relata
 The relata of the non-random relations are English
 nouns, verbs and adjectives selected and validated
 by both authors using two types of sources: se-
 mantic sources (the McRae Norms (McRae et al.,
 2005), WordNet (Fellbaum, 1998) and ConceptNet
 (Liu and Singh, 2004)) and text sources (Wikipedia
 and the Web-derived ukWaC corpus, see Section 4.4
 below). These resources greatly differ in dimension,
 origin and content and therefore provide comple-
 mentary views on relata. Their relative contribution
 to BLESS also depends on the type of relation and
 the target concept. For instance, the rich taxonomic
 structure of WordNet has been the main source of in-
 formation for many technical hypernyms (e.g. gym-
 nosperm, oscine), which instead are missing from
 more commonsense-oriented resources such as the
 McRae Norms and ConceptNet. Meronyms are
 rarer in WordNet, and were collected mainly from
 the latter two resources, with many technical terms
 (e.g., parts of ships, weapons) harvested from the
 Wikipedia entries for the target concepts.
    Attributes and events were collected from McRae

Norms, ConceptNet and ukWaC. In the McRae
Norms, the number of features per concept is fairly
limited, but they correspond to highly distinctive,
prototypical and cognitively salient properties. Con-
ceptNet instead provides a much wider array of as-
sociated events and attributes that are part of our
commonsense knowledge about the target concepts
(e.g., the events park, steal and break, etc. for car).
ConceptNet relations such as Created by, Used for,
Capable of etc. have been analyzed to identify po-
tential event relata, while the Has property relation
has been inspected to look for attributes. The most
salient adjectival and verbal collocates of the tar-
get nouns in the ukWaC corpus were also used to
identify associated attributes and events. For in-
stance, the target concept elephant is not attested in
the McRae Norms and has few properties in Con-
ceptNet. Thus, many of its related events have been
harvested from ukWaC. They include verbs such as
hunt, kill, etc. which are quite salient and frequent
with respect to elephants, although they can hardly
be defined as prototypical properties of this animal.
As a result of the combined use of such different
types of sources, the BLESS relata are representative
of a wide spectrum of semantic information about
the target concepts: they include domain-specific
terms side by side to commonsense ones, very dis-
tinctive features of a concept (e.g., hoot for owl)
together with attributes and events that are instead
shared by a whole class of concepts (e.g., all animals
have relata such as eat, feed, and live), prototypical
features as well as events and attributes that are sta-
tistically salient for the target, etc.
   In many cases, the concept properties contained
in semantic sources are expressed with phrases, e.g.,
lay eggs, eat grass, live in Africa, etc. We decided,
however, to keep only single-word relata in BLESS,
because DSMs are typically populated with single
words, and, when they are not, they differ in the
kinds of multi-word elements they store. There-
fore, phrasal relata have always been reduced to
their head: a verb for properties expressed by a verb
phrase, and a noun for properties expressed by a

noun phrase. For instance, from the property lay

eggs, we derived the event relatum lay.

   To extract the random relata, we adopted the fol-
lowing procedure. For each relatum that instantiates

a true relation with the concept, we also randomly



                                                     6
 picked from our combined corpus (cf. Section 4.4)
 another lemma with the same part of speech, and
 frequency within 1 absolute logarithmic unit from
 the frequency of the corresponding true relatum.
 Since picking a random term does not guarantee
 that it will not be related to the concept, we filtered
 the extracted list by crowdsourcing, using the Ama-
 zon Mechanical Turk via the CrowdFlower interface
 (CF).1 We presented CF workers with the list of
 about 15K concept+random-term pairs selected with
 the procedure we just described, plus a manually
 checked validation set (a “gold set” in CF terminol-
 ogy) comprised of 500 concept+true-relatum pairs
 and 500 concept+random-term pairs (these elements
 are used by CF to determine the reliability of work-
 ers, and discard the ratings of unreliable ones), plus a
 further set of 1.5K manually checked concept+true-
 relatum pairs to make the random-true distribution
 less skewed. The workers’ task was, for each pair,
 to check a YES radio button if they thought there is
 a relation between the words, NO otherwise. The
 words were annotated with their part of speech, and
 workers were instructed to pay attention to this in-
 formation when making their choices. Extensive
 commented examples of both related pairs and un-
 related ones were also provided in the instruction
 page. A minimum of 2 CF workers rated each pair,
 and, conservatively, we preserved only those items
 (about 12K) that were unanimously rated as unre-
 lated to their concept by the judges. See Table 1 for
 summary statistics about the preserved random sets
 (nouns: RAND . N, adjectives: RAN . J, verbs:RAN . V).

 4.4     BLESS statistics
 For frequency information, we rely on the combi-
 nation of the freely available ukWaC and Wackype-
 dia corpora (size: 1.915B and 820M tokens, respec-
 tively).2 The data set contains 200 concepts that
 have a mean corpus frequency of 53K occurrences
 (min. 1416 chisel, max. 793K car). The relata of
 these concepts (26,554 in total) are distributed as re-
 ported in Table 1.

    Note that the distributions reflect certain “natural”

 differences between relations (hypernyms tend to be

 more frequent words than coordinates, but there are

    1
        http://crowdflower.com/
    2
        http://wacky.sslmit.unibo.it/


                     frequency          cardinality
    relation   min       avg   max   min   avg max
    COORD        0      37K 1.7M       6 17.1       35

    HYPER       31     138K 1.9M       2    6.7     15
    MERO         0     133K    2M      2 14.7       53

    ATTRI        0     501K 3.7M       4 13.6       27
    EVENT        0     517K 5.4M       6 19.1       40
    RAN . N      0      92K 2.4M      16 32.9       67
    RAN . J      1     472K 4.5M       3 10.9       24
    RAN . V      1     508K 7.7M       4 16.3       34
Table 1: Distribution (minimum, mean and maximum) of
the relata of all BLESS concepts: the frequency columns
report summary statistics for corpus counts across relata
instantiating a relation; the cardinality columns report
summary statistics for number of relata instantiating a
relation across the 200 concepts, only considering relata
with corpus frequency ≥ 100.

more coordinates than hypernyms, etc.). Instead of
trying to artificially control for these differences, we
assess their impact in Section 5 by looking at the
behavior of baselines that exploit the frequency and
cardinality of relations as proxies to semantic simi-
larity (such factors could also be entered as regres-
sors in a linear model).

5     Evaluation

This section illustrates one possible way to use
BLESS to explore and evaluate DSMs. Given the
similarity scores provided by a model for a concept
with all its relata across all relations, we pick the re-
latum with the highest score (nearest neighbour) for
each relation (see discussion in Section 3 above on
why we allow models to pick their favorite from a
set of relata instantiating the same relation). In this
way, for each of the 200 BLESS concepts, we obtain
8 similarity scores, one per relation. In order to fac-
tor out concept-specific effects that might add to the
overall score variance (for example, a frequent con-

cept might have a denser neighborhood than a rarer

one, and consequently the nearest relatum scores of

the former are trivially higher than those of the lat-

ter), we transform the 8 similarity scores of each

concept onto standardized z scores (mean: 0; s.d: 1)

by subtracting from each their mean, and dividing by

their standard deviation. After this transformation,

we produce a boxplot summarizing the distribution
of scores per relation across the 200 concepts (i.e.,



                                                         7
 each box of the plot summarizes the distribution of
 the 200 standardized scores picked for each rela-

 tion). Our boxplots (see examples in Fig. 1 below)


 display the median of a distribution as a thick hori-
 zontal line within a box extending from the first to
 the third quartile, with whiskers covering 1.5 of the
 interquartile range in each direction from the box,
 and values outside this extended range – extreme
 outliers – plotted as circles (these are the default
 boxplotting option of the R statistical package).3
 While the boxplots are extremely informative about
 the relation types that are best captured by models,
 we expect some degree of overlap among the distri-
 butions of different relations, and in such cases we
 might want to ask whether a certain model assigns
 significantly higher scores to one relation rather than
 another (for example, to coordinates rather than ran-
 dom nouns). It is difficult to decide a priori which
 pairwise statistical comparisons will be interesting.
 We thus take a conservative approach in which we
 perform all pairwise comparisons using the Tukey
 Honestly Significant Difference test, that is simi-
 lar to the standard t test, but accounts for the greater
 likelihood of Type I errors when multiple compar-
 isons are performed (Abdi and Williams, 2010). We
 only report the Tukey test results for those com-
 parisons that are of interest in the analysis of the
 boxplots, using the standard α = 0.05 significance
 threshold.

 5.1     Models
 Occurrence and co-occurrence statistics for all mod-
 els are extracted from the combined ukWaC and
 Wackypedia corpora (see Section 4.4 above). We ex-
 ploit the automated morphosyntactic annotation of
 the corpora by building our DSMs out of lemmas
 (instead of inflected words), and relying on part of
 speech information.

 Baselines. The RelatumFrequency baseline uses

 the frequency of occurrence of a relatum as a sur-

 rogate of its cosine with the concept. With this ap-

 proach, we want to verify that the unequal frequency

 distribution across relations (see Table 1 above) is

 not trivially sufficient to differentiate relation classes

 in a semantically interesting way. For our second

 baseline, we assign a random number as cosine sur-

    3
        http://www.r-project.org/


rogate to each relatum (to smooth these random val-
ues, we generate them by first sampling, for each
relatum, 10K random variates from a uniform distri-
bution, and then averaging them). If the set of relata
instantiating a certain relation is larger, it is more
likely that it will contain the highest random value.
Thus, this RelationCardinality baseline will favor
relations that tend to have large relata set across con-
cepts, controlling for effects due to different cardi-
nalities across semantic relations (again, see Table 1
above).

DSMs. We choose a few ways to construct DSMs
for illustrative purposes only. All the models contain
vector representations for the same words, namely,
approximately, the top 20K most frequent nouns, 5K
most frequent adjectives and 5K most frequent verbs
in the combined corpora. All the models use Local
Mutual Information (Evert, 2005; Baroni and Lenci,
2010) to weight raw co-occurrence counts (this asso-
ciation measure is obtained by multiplying the raw
count by Pointwise Mutual Information, and it is a
close approximation to the Log-Likelihood Ratio).
Three DSMs are based on counting co-occurrences
with collocates within a window of fixed width,
in the tradition of HAL (Lund and Burgess, 1996)
and many later models. The ContentWindow2
model records sentence-internal co-occurrence with
the nearest 2 content words to the left and right
of each target concept (the same 30K target nouns,
verbs and adjectives are also employed as context
content words). ContentWindow20 is like Con-
tentWindow2, but considers a larger window of 20
words to the left and right of the target. AllWin-
dow2 adopts the same window of ContentWindow2,
but considers all co-occurrences, not only those with
content words. The Document model, finally, is
based on a (Local-Mutual-Information transformed)
word-by-document matrix, recording the distribu-
tion of the 30K target words across the documents in
the concatenated corpus. This DSM is thus akin to
traditional Latent Semantic Analysis (Landauer and
Dumais, 1997), without dimensionality reduction.
The content-window-based models have, by con-
struction, about 30K dimensions. The other models
are much larger, and for practical reasons we only
keep 1 million dimensions (those that account, cu-
mulatively, for the largest proportion of the overall

                                                      8
 Local Mutual Information mass).

 5.2   Results
 The concept-by-concept z-normalized distributions
 of cosines of relata instantiating each of our rela-
 tions are presented, for each of the example mod-
 els, in Fig. 1. The RelatumFrequency baseline
 shows a preference for adjectives and verbs in gen-
 eral, independently of whether they are meaningful
 (attributes, events) or not (random adjectives and
 verbs), reflecting the higher frequencies of adjec-
 tives and verbs in BLESS (Table 1). The Relation-
 Cardinality baseline produces even less interesting
 results, with a strong preference for random nouns,
 followed by coordinates, events and random verbs
 (as predicted by the distribution in Table 1). We can
 conclude that the semantically meaningful patterns
 produced by the other models cannot be explained
 by trivial differences in relatum frequency or rela-
 tion cardinality in the BLESS data set.
     Moving then to the real DSMs, ContentWindow2
 essentially partitions the relations into 3 groups: co-
 ordinates are the closest relata, which makes sense
 since they are, taxonomically, the most similar en-
 tities to target concepts. They are followed by (but
 significantly closer to the concept than) events, hy-
 pernyms and meronyms (events and hypernyms sig-
 nificantly above meronyms). Next come the at-
 tributes (significantly lower cosines than all relation
 types above). All the meaningful relata are signif-
 icantly closer to the concepts than the random re-
 lata. Similar patterns can be observed in the Con-
 tentWindow20 distribution, however in this case the
 events, while still significantly below the coordi-
 nates, are significantly above the (statistically in-
 distinguishable) hypernym, meronym and attribute
 set. Again, all meaningful relata are above the ran-
 dom ones. Both content-window-based models pro-
 vide reasonable results, with ContentWindow2 be-
 ing probably closer to our “ontological” intuitions.
 The high ranking of events is probably explained
 by the fact that a nominal concept will often ap-
 pear as subject or object of verbs expressing asso-
 ciated events (dog barks, fishing tuna), and thus the
 corresponding verbs will share even relatively nar-
 row context windows with the concept noun. The
 AllWindow2 distribution probably reflects the fact
 that many contexts picked by this DSM are function

                           RelatumFrequency




                                                                          2





      2
                                                    ●







                                                    ●

                                                    ●
                                                    ●
                                                    ●





                                                                          1
                            ●

                     ●
                                                    ●





      1
                     ●
                     ●                              ●
                                                    ●
                                                    ●
                     ●                              ●
                     ●                              ●
                     ●                              ●
                     ●
                                                    ●
                     ●
                     ●      ●

                     ●




                                                                          0
                     ●      ●
                     ●
                            ●

                     ●      ●

                     ●





                     ●      ●


      0




                     ●      ●
                            ●
                            ●

             ●






                                                                          −1


                                                                                 ●





                                                                                 ●
                                                                                 ●
      −1




                                                                                 ●
                                                                                 ●
                            ●                                                    ●
                                                                                 ●
                                                                                 ●




                                                                          −2
                                                                                 ●






             ●
             ●
             ●
      −2




             ●




           COORD   HYPER   MERO   ATTRI   EVENT   RAN.N   RAN.J   RAN.V        COORD   HYPER



                           ContentWindow20






                                                                          2
                                    ●
                                    ●
      2




                                                                                         ●






                                                                    ●


                                                    ●       ●
                                                                    ●




                                                                          1

                                                            ●
      1




                                                                    ●





                                                                    ●


                                                    ●       ●
                                                            ●
                                                                    ●
                                                            ●       ●
                                                    ●

                                                    ●                            ●
                                                                                 ●


                                                                          0

                                                    ●
                                                    ●
      0




                                                    ●
                                                    ●


















                                                                                         ●
                                                                                         ●
                                                                          −1





      −1




             ●
                                                                                         ●
             ●

             ●                                                                           ●
             ●









             ●


                                                                          −2




                                                                    ●
      −2
















           COORD   HYPER   MERO   ATTRI   EVENT   RAN.N   RAN.J   RAN.V        COORD   HYPER




Figure 1: Distribution of relata cosines across concepts (values on ordinate are cosines after
normalization).


words, and thus they capture syntactic, rather than
semantic distributional properties. As a result, ran-
dom nouns are as high (statistically indistinguish-
able from) hypernyms and meronyms. Interestingly,
attributes also belong to this subset of relations –
probably due to the effect of determiners, quantifiers
and other DP-initial function words, that will often
occur both before nouns and before adjectives. In-
deed, even random adjectives, although significantly
below the other relations we discussed, are signif-
icantly above both random and meaningful verbs
(i.e., events). For the Document model, all mean-
ingful relations are significantly above the random
ones. However, coordinates, while still the nearest
neighbours (significantly closer than all other rela-
tions) are much less distinct than in the window-
based models. Note that we cannot say a priori that
ContentWindow2 is better than Document because
it favors coordinates. However, while they are both
able to sort out true and random relata, the latter
shows a weaker ability to discriminate among differ-
ent types of semantic relations (co-occurring within
a document is indeed a much looser cue to similarity
than specifically co-occurring within a narrow win-
dow). Traditional DSM tests, based on a single qual-



 RelationCardinality                                                   ContentWindow2





                                                                       ●










                                                 2
                                                                                       ●
                                                                                       ●

                                                                                                               ●



                                                                               ●               ●








                                                                               ●                       ●




                                                 1






                                                                                               ●

                                                                                               ●
                                                                                               ●               ●

                                                                                                       ●       ●
                                                                                                               ●
                                                                                                               ●







                                                        ●                                      ●
                                                                                               ●
                                                                                               ●       ●
                                                                                               ●       ●       ●
                   ●                                                                           ●




                                                 0
                                                        ●                                      ●       ●       ●
                   ●                                                                           ●       ●       ●
                   ●                                                                                   ●





                                                        ●

                   ●                                    ●
                                                        ●
                                           ●
                                           ●
                                           ●





            ●
            ●                              ●
            ●                              ●




                                                 −1

            ●






                                                                                               ●               ●













                                                 −2
                                                                                                       ●

                                                                                               ●






                                                                                               ●




 MERO    ATTRI   EVENT   RAN.N   RAN.J   RAN.V        COORD   HYPER   MERO   ATTRI   EVENT   RAN.N   RAN.J   RAN.V



        AllWindow2                                                           Document
                                                                               ●
                                                                               ●






  ●                                                                            ●










                                                 2
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●







                                                                                                               ●






                                                                               ●




                                                 1
                                                                               ●
                                                                               ●
                                                                               ●
                                                                               ●
                                                                                                               ●

                                                                                                               ●

                                                                                                       ●       ●
                                                                                                               ●




                                                                                                               ●

                                                                                                               ●






                                                                                                               ●
                                                                                                       ●       ●
                                                                                               ●               ●
                                                                                                               ●




                                                 0
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●       ●
                                                                                               ●       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                                       ●
                                                                                               ●
                           ●
  ●                        ●
  ●         ●





  ●





                           ●

  ●
            ●






                                                 −1
  ●
  ●         ●
  ●

  ●
  ●
  ●                                                                                            ●





                                                                                               ●





                                                                                               ●
                                                                                                       ●




                                                 −2
                                                                                               ●




 MERO    ATTRI   EVENT   RAN.N   RAN.J   RAN.V        COORD   HYPER   MERO   ATTRI   EVENT   RAN.N   RAN.J   RAN.V




 concept-by-concept z-



                 ity measure, would not have given us this broad view
                 of how models are behaving.

                 6         Conclusion

                 We introduced BLESS, the first data set specifically
                 designed for the intrinsic evaluation of DSMs. The
                 data set contains tuples instantiating different, ex-
                 plicitly typed semantic relations, plus a number of
                 controlled random tuples. Thus, BLESS can be used
                 to evaluate both the ability of DSMs to discriminate
                 truly related word pairs, and to perform in-depth
                 analyses of the types of semantic relata that different
                 models tend to favor among the nearest neighbors of
                 a target concept. Even a simple comparison of the
                 performance of a few DSMs on BLESS - like the
                 one we have shown here - is able to highlight inter-
                 esting differences in the semantic spaces produced
                 by the various models. The success of BLESS will
                 obviously depend on whether it will become a refer-
                 ence model for the evaluation of DSMs, something
                 that can not be foreseen a priori. Whatever its des-
                 tiny, we believe that the BLESS approach can boost
                 and innovate evaluation in distributional semantics,
                 as a key condition to get at a deeper understanding
                 of its potentialities as a viable model for meaning.

</corps>
<conclusion>
                 6         Conclusion

                 We introduced BLESS, the first data set specifically
                 designed for the intrinsic evaluation of DSMs. The
                 data set contains tuples instantiating different, ex-
                 plicitly typed semantic relations, plus a number of
                 controlled random tuples. Thus, BLESS can be used
                 to evaluate both the ability of DSMs to discriminate
                 truly related word pairs, and to perform in-depth
                 analyses of the types of semantic relata that different
                 models tend to favor among the nearest neighbors of
                 a target concept. Even a simple comparison of the
                 performance of a few DSMs on BLESS - like the
                 one we have shown here - is able to highlight inter-
                 esting differences in the semantic spaces produced
                 by the various models. The success of BLESS will
                 obviously depend on whether it will become a refer-
                 ence model for the evaluation of DSMs, something
                 that can not be foreseen a priori. Whatever its des-
                 tiny, we believe that the BLESS approach can boost
                 and innovate evaluation in distributional semantics,
                 as a key condition to get at a deeper understanding
                 of its potentialities as a viable model for meaning.
References

Herv Abdi and Lynne Williams. 2010. Newman-Keuls

   and Tukey test. In N.J. Salkind, D.M. Dougherty, and

   B. Frey, editors, Encyclopedia of Research Design.

   Sage, Thousand Oaks, CA.

Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana

   Kravalova, Marius Pasça, and Aitor Soroa. 2009. A

   study on similarity and relatedness using distributional

   and WordNet-based approaches. In Proceedings of

   HLT-NAACL, pages 19–27, Boulder, CO.

Abdulrahman Almuhareb. 2006. Attributes in Lexical
   Acquisition. Phd thesis, University of Essex.

Marco Baroni and Alessandro Lenci. 2010. Dis-

   tributional Memory: A general framework for
   corpus-based semantics. Computational Linguistics,
   36(4):673–721.

Marco Baroni, Stefan Evert, and Alessandro Lenci, edi-
   tors. 2008. Bridging the Gap between Semantic The-
   ory and Computational Simulations: Proceedings of
   the ESSLLI Workshop on Distributional Lexical Se-
   mantic. FOLLI, Hamburg.
Marco Baroni, Eduard Barbu, Brian Murphy, and Mas-
   simo Poesio. 2010. Strudel: A distributional semantic
   model based on properties and types. Cognitive Sci-
   ence, 34(2):222–254.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
   ating wordnet-based measures of lexical semantic re-
   latedness. Computational Linguistics, 32:13–47.
D. A. Cruse. 1986. Lexical Semantics. Cambridge Uni-
   versity Press, Cambridge.
Stefan Evert. 2005. The Statistics of Word Cooccur-
   rences. Dissertation, Stuttgart University.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
   tronic Lexical Database. MIT Press, Cambridge, MA.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
   Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
   Ruppin. 2002. Placing search in context: The concept
   revisited. ACM Transactions on Information Systems,
   20(1):116–131.
Thomas Landauer and Susan Dumais. 1997. A solu-
   tion to Plato’s problem: The latent semantic analysis
   theory of acquisition, induction, and representation of
   knowledge. Psychological Review, 104(2):211–240.
Hugo Liu and Push Singh. 2004. ConceptNet: A prac-
   tical commonsense reasoning toolkit. BT Technology
   Journal, pages 211–226.
Kevin Lund and Curt Burgess. 1996. Producing
   high-dimensional semantic spaces from lexical co-
   occurrence. Behavior Research Methods, 28:203–208.
Ken McRae, George Cree, Mark Seidenberg, and Chris
   McNorgan. 2005. Semantic feature production norms
   for a large set of living and nonliving things. Behavior
   Research Methods, 37(4):547–559.


                                                        10
 George Miller and Walter Charles. 1991. Contextual cor-
   relates of semantic similarity. Language and Cogni-

   tive Processes, 6(1):1–28.

 Gregory Murphy. 2002. The Big Book of Concepts. MIT

   Press, Cambridge, MA.

 Timothy Rogers and James McClelland. 2004. Seman-

   tic Cognition: A Parallel Distributed Processing Ap-

   proach. MIT Press, Cambridge, MA.

 Herbert Rubenstein and John Goodenough. 1965. Con-

   textual correlates of synonymy. Communications of

   the ACM, 8(10):627–633.

 Karen Sparck Jones and Julia R. Galliers. 1996. Evaluat-
   ing Natural Language Processing Systems: An Analy-

   sis and Review. Springer Verlag, Berlin.

 Peter Turney and Patrick Pantel. 2010. From frequency
   to meaning: Vector space models of semantics. Jour-
   nal of Artificial Intelligence Research, 37:141–188.

 Peter Turney. 2006. Similarity of semantic relations.
   Computational Linguistics, 32(3):379–416.
 Morton E. Winston, Roger Chaffin, and Douglas Her-
   rmann. 1987. A taxonomy of part-whole relations.
   Cognitive Science, 11:417–444.

































</conclusion>
<Reference : >
References

Herv Abdi and Lynne Williams. 2010. Newman-Keuls

   and Tukey test. In N.J. Salkind, D.M. Dougherty, and

   B. Frey, editors, Encyclopedia of Research Design.

   Sage, Thousand Oaks, CA.

Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana

   Kravalova, Marius Pasça, and Aitor Soroa. 2009. A

   study on similarity and relatedness using distributional

   and WordNet-based approaches. In Proceedings of

   HLT-NAACL, pages 19–27, Boulder, CO.

Abdulrahman Almuhareb. 2006. Attributes in Lexical
   Acquisition. Phd thesis, University of Essex.

Marco Baroni and Alessandro Lenci. 2010. Dis-

   tributional Memory: A general framework for
   corpus-based semantics. Computational Linguistics,
   36(4):673–721.

Marco Baroni, Stefan Evert, and Alessandro Lenci, edi-
   tors. 2008. Bridging the Gap between Semantic The-
   ory and Computational Simulations: Proceedings of
   the ESSLLI Workshop on Distributional Lexical Se-
   mantic. FOLLI, Hamburg.
Marco Baroni, Eduard Barbu, Brian Murphy, and Mas-
   simo Poesio. 2010. Strudel: A distributional semantic
   model based on properties and types. Cognitive Sci-
   ence, 34(2):222–254.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
   ating wordnet-based measures of lexical semantic re-
   latedness. Computational Linguistics, 32:13–47.
D. A. Cruse. 1986. Lexical Semantics. Cambridge Uni-
   versity Press, Cambridge.
Stefan Evert. 2005. The Statistics of Word Cooccur-
   rences. Dissertation, Stuttgart University.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
   tronic Lexical Database. MIT Press, Cambridge, MA.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
   Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
   Ruppin. 2002. Placing search in context: The concept
   revisited. ACM Transactions on Information Systems,
   20(1):116–131.
Thomas Landauer and Susan Dumais. 1997. A solu-
   tion to Plato’s problem: The latent semantic analysis
   theory of acquisition, induction, and representation of
   knowledge. Psychological Review, 104(2):211–240.
Hugo Liu and Push Singh. 2004. ConceptNet: A prac-
   tical commonsense reasoning toolkit. BT Technology
   Journal, pages 211–226.
Kevin Lund and Curt Burgess. 1996. Producing
   high-dimensional semantic spaces from lexical co-
   occurrence. Behavior Research Methods, 28:203–208.
Ken McRae, George Cree, Mark Seidenberg, and Chris
   McNorgan. 2005. Semantic feature production norms
   for a large set of living and nonliving things. Behavior
   Research Methods, 37(4):547–559.


                                                        10
 George Miller and Walter Charles. 1991. Contextual cor-
   relates of semantic similarity. Language and Cogni-

   tive Processes, 6(1):1–28.

 Gregory Murphy. 2002. The Big Book of Concepts. MIT

   Press, Cambridge, MA.

 Timothy Rogers and James McClelland. 2004. Seman-

   tic Cognition: A Parallel Distributed Processing Ap-

   proach. MIT Press, Cambridge, MA.

 Herbert Rubenstein and John Goodenough. 1965. Con-

   textual correlates of synonymy. Communications of

   the ACM, 8(10):627–633.

 Karen Sparck Jones and Julia R. Galliers. 1996. Evaluat-
   ing Natural Language Processing Systems: An Analy-

   sis and Review. Springer Verlag, Berlin.

 Peter Turney and Patrick Pantel. 2010. From frequency
   to meaning: Vector space models of semantics. Jour-
   nal of Artificial Intelligence Research, 37:141–188.

 Peter Turney. 2006. Similarity of semantic relations.
   Computational Linguistics, 32(3):379–416.
 Morton E. Winston, Roger Chaffin, and Douglas Her-
   rmann. 1987. A taxonomy of part-whole relations.
   Cognitive Science, 11:417–444.

































</Reference : >
