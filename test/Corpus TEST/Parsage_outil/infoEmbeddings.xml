<preamble>
infoEmbeddings.pdf
</preamble>
<titre>
An Empirical Evaluation of Text Representation Schemes on Multilingual Social Web to Filter the Textual Aggression
</titre>
<auteurs>
<auteur>Sandip Modha</auteur>
<auteur>Prasenjit Majumdera</auteur>
<affiliation>
and Prasenjit Majumdera
</affiliation>
</auteurs>
<abstract>
                                         An Empirical Evaluation of Text Representation Schemes on
                                         Multilingual Social Web to Filter the Textual Aggression

                                         Sandip Modha        a   and Prasenjit Majumdera
                                         a
                                             DA-IICT Gandhinagar,India;
arXiv:1904.08770v1 [cs.IR] 16 Apr 2019








</abstract>
<introduction>
1. Introduction

The Social Web is a great source for studying human interaction and behavior. In
the last few years, there is an exponential growth in Social Media user base. Sensing
content of Social Media like Facebook, Twitter, by the smart autonomous application
empower its user community with real-time information which is unfolded across the
different part of the world. Social media provide the easiest and anonymous platform
for common people to voice their opinion or view on a various entity like celebrity,
politician, product, stock market etc or any social movement. Sometime such opinions
might be aggressive in nature and propagate hate in the social media community.
    With the unprecedented increase in the user base of the social media and its avail-
ability on the Smartphones, incidents like Hate speech, trolling, Cyberbullying, and
Aggressive posts are increasing exponentially. A smart autonomous system is required
which enable surveillance on the social media platform and detect such incidents.
Some of the researchers look posts from the aspect like aggression Kumar Ritesh et
al. (2018) to filter the contents. some of the posts contain words which might be qual-
ified as either highly or overly aggressive or have hidden aggression. Sometimes posts
do not have any aggression. Based on these, posts or comments are categorized into
three classes namely: ‘Overtly Aggressive‘, ‘Covertly Aggressive‘ and ‘Non-aggressive‘
Kumar Ritesh et al. (2018). Henceforth, in the rest of the paper, we will denote these
classes by these abbreviations namely: OAG, CAG, NAG respectively.Table 1 shows
the sample posts belonging to these classes.
    Social Media, specifically Microblog has proved its importance during the disaster-
related incidents like an earthquake, Hurricane and floods 1 . Organizations involved
in relief operation actively track posts related to situational information posted on
Facebook and Twitter during the disaster. However, At the same time, social media
is flooded with lots of prayer and condolence messages. Posts which contain factual
information are extremely important for the organization involved in post-disaster
relief operations for coordination. Filtering and Ranking of the posts containing factual
information will be very useful to them. We believe that this is the special problem
of the Sentiment Analysis task. We consider this problem as a combination of two-
class classification problem: factual posts and nob-factual posts plus Ranking. Table
2 shows the example of the posts of belong to these class.
    The Text representation of social web content plays a pivotal role in any NLP task.
Bag-of-word is the oldest and simple technique to represent the document or post into
a fixed length vector. The BoW techniques generate very sparse and high dimensional
space vector. Text representation using distributed word/sentence representation or
word embedding is gain rapid momentum recently. In this paper, one of the objectives

1 https://phys.org/news/2018-08-social-media-bad-disaster-zones.html



       Table 2. sample post for the each class.

                Post text                                                          Class Label
        1       #Nepal #Earthquake day four. Slowly in the capital valley          Factual
                Internet and electricity beeing restored . A relief for at least
                some ones
        2       PMOIndia Indian Government is doing every possible help to         Non-factual
                the earthquake victims and they need money so plz contribute




is to find the best text representation scheme to model social web content for the
machine learning classifier and deep neural net. Various Text representation scheme
based on BoW, word embedding and are studied empirically. We have reported result
on popular word embedding technique like Word2vec, Glove and fastText on stan-
dard machine learning classifier like Multinomial Naive Bayes (MNB), Logistic Re-
gression(LR), K-Nearest neighbors KNN, Support Vector Classifier (SVC), Decision
Tree (DT),Stochastic Gradient Descent(SGD), Random forest (RF), Ridge, AdaBoost,
Perceptron, Deep neural net based on LSTM, CNN and Bidirectional LSTM. Results
are also reported on Doc2vec embedding, a popular sentence or paragraph embedding
technique for the above classifiers.
   Transfer Learning is well practiced in the area of computer vision. However, in the
NLP, transfer learning has limited application in the form of pre-trained word vector
which is used to initialize the weights of the embedding layer of the deep neural net-
work. With the advent of transfer learning method like ELMO (Peters et al. , 2018),
ULMFiT (Howaard et al., 2018) claimed substantial improvement in the performance
of various NLP tasks like Sentiment Analysis, Question/Answering, Textual Entail-
ment empirically. The main idea behind these methods is to train language model
on the large corpus and fine tune on the task-specific corpus. In this paper, We have
evaluated the performance of these methods in the Aggression classification tasks.


1.1. Research Questions
In this study, experiments are performed on the benchmark dataset with to answer
the following questions
   • Which is the best Text Representation scheme to model text from the Social
     Web?
   • Does pre-trained language model based on transfer learning better than pre-
     trained word embedding based on shallow transfer learning on Social media
     data?
   • Does Making too Deep Neural net make sense?
   To answer all research question listed above, experiments are performed on two
tasks namely: Aggression detection (Trolling Aggression and Cyberbullying (TRAC)
dataset) Kumar et al. (2018) and Fact detection (FIRE iRMDI Dataset)Basu et al.
 (2018). In this paper, we present exhaustive benchmarking of text representation
schemes on these datasets. Our results reveal that fastText with pre-trained vector
along with CNN outperform standard machine learning classifiers based on BoW
Model and marginally perform better than Word2vec and Glove. Paragraph vector
or Doc2vec Le, Quoc et al. (2014) perform very poor on our dataset and turn out to
be the worst text representation scheme among all. We also found that model based
on the deep neural net is more robust than machine learning classifier when tested
on lexically different dataset than training Dataset. i.e. deep neural model substan-

</introduction>
<Results>
In this section, we will present the comprehensive result analysis and try to answer
the research questions which framed before the experiments were performed. As we
look at the table 6 7, and 8, Overall, LSTM and CNN with pre-trained fastText word
embedding marginally outperform (around 2 % to 4%) standard machine learning
   Table 8. F1-score on TRAC Facebook and Twitter English Dataset:Using Pre-trained word vectors.

                          Facebook Test Dataset                      Twitter Test Dataset
     Classifier   p-Word2vec       p-Glove   p-Fasttext     p-Word2vec        p-Glove   p-Fasttext
     NB                0.5342      0.5373      0.5519
     LR                0.5799      0.6050      0.6045
     KNN               0.4981      0.5103      0.4819
     SVC               0.5832      0.5678      0.6120
     DT                0.4700      0.4515      0.4900
     SGD               0.5019      0.5521      0.5360
     RF                0.5402      0.5338      0.5505
     Ridge             0.5829      0.5952      0.6140
     AdaB              0.5713      0.5781      0.5907
     Perce.            0.5114      0.5201      0.5660
     ANN               0.5025      0.5498      0.5722
      Ensemble         0.5300      0.5500      0.5558
     LSTM              0.4979      0.4979      0.6178
      BLSTM            0.5501      0.6062      0.6000
      CNN              0.4749      0.5405      0.6407
      NCNN             0.5169      0.5883      0.5600




Table 9. F1-score on TRAC Facebook Code-mixed Hindi Dataset
 Classifier   Count-      TF/IDF W2Vec       Glove      Fasttext
              vector
 NB           0.5535      0.6031   0.3001    0.372      0.2959
 LR           0.5855      0.6134   0.5779    0.464      0.5457
 KNN          0.3340      0.1721   0.4998    0.425      0.5106
 SVC          0.5556      0.5862   0.4806    0.373      0.5186
 DT           0.5307      0.5025   0.4629    0.388      0.4392
 SGD          0.5533      0.5922   0.3912    0.393      0.3670
 RF           0.5473      0.5473   0.5374    0.440      0.5047
 Ridge        0.5780      0.5850   0.5293    0.381      0.5092
 AdaB         0.5373      0.5233   0.5342    0.479      0.5336
 Perce.       0.5213      0.5598   0.4232    0.364      0.3763
 ANN          0.5703      0.5350   0.5455    0.5037     0.5842
 Ensemble     0.5700      0.6087   0.5558    0.4067     0.534
 LSTM                              0.5649    0.590      0.6021
 BLSTM                             0.5759    0.527      0.5770
 CNN                               0.5515    0.566      0.5950
 NCNN                              0.5919    0.573      0.5912




Table 10. F1-score on TRAC Twitter Code-mixed Hindi Dataset
 Classifier   Count-      TF/IDF W2Vec       Glove      Fasttext
              vector
 NB           0.2970      0.2902   0.3215    0.273      0.3359
 LR           0.3787      0.3724   0.2819    0.279      0.3184
 KNN          0.2527      0.2553   0.3704    0.334      0.3381
 SVC          0.3781      0.3886   0.2821    0.261      0.3087
 DT           0.3685      0.3936   0.3572    0.326      0.3475
 SGD          0.3996      0.3993   0.2822    0.287      0.2739
 RF           0.3585      0.3737   0.3286    0.344      0.3449
 Ridge        0.3616      0.3872   0.2811    0.242      0.3346
 AdaB         0.1886      0.1903   0.3256    0.362      0.3261
 Perce.       0.3931      0.3868   0.2802    0.329      0.2787
 ANN          0.430       0.44     0.3163    0.3552     0.2399
 Ensemble     0.4400      0.4600   0.4500    0.3938     0.3426
 LSTM                              0.3840    0.376      0.3667
 BLSTM                             0.2846    0.318      0.3005
 CNN                               0.3323    0.317      0.2669
 NCNN                              0.3338    0.380      0.3494




                                                  14
 0.4152       0.4527        0.4276
 0.4197       0.4527        0.4441
 0.3405       0.3959        0.3912
 0.4446       0.4581        0.4350
 0.3640       0.3949        0.3632
 0.3692       0.3793        0.3852
 0.3394       0.3716        0.3687
 0.4092       0.4530        0.4461
 0.4241       0.4261        0.4033
 0.4224       0.4118        0.4049
 0.3728       0.3722        0.4842
 0.3728       0.3722        0.4500
 0.5537       0.5518         0.5541
 0.5359       0.5466        0.5423
 0.5226       0.5667        0.5520
 0.5384       0.5067        0.5407





     p-            doc2vec-     doc2vec-
     fastText      dmc          dbow
     0.3176        0.3459       0.4736
     0.5518        0.3894       0.4380
     0.4909        0.3768       0.4038
     0.5442        0.3879       0.4344
     0.4288        0.3485       0.3485
     0.4746        0.3331       0.4134
     0.4788        0.3512       0.3477
     0.5544        0.3866       0.4292
     0.4913        0.3751       0.4214
     0.4873        0.2661       0.3282
     0.5190        0.4091       0.4440
     0.5612        0.4980       0.4401
     0.5916
     0.5900
     0.6081
     0.5965





    p-fastText     doc2vec-     doc2vec-
                   dmc          dbow
    0.2897         0.3270       0.3205
    0.3524         0.2438       0.2833
    0.2917         0.3051       0.3299
    0.3472         0.2580       0.2905
    0.3473         0.2988       0.2988
    0.3163         0.2605       0.2588
    0.3288         0.2981       0.2988
    0.3361         0.2549       0.2875
    0.3441         0.2614       0.2933
    0.3835         0.2616       0.2752
    0.3593         0.2419       0.3132
    0.3555         0.3230       0.3281
    0.4600
    0.4600
    0.4992
    0.4600



              Table 11. F1 score on TRAC Facebook English Test Dataset using
              Transfer Learning methods

                Transfer learning Model                   English
                                           Facebook Dataset    Twitter Dataset
                ELMO                             0.3699
                ULMFiT                           0.4725


Table 12. weighted F1-score TRAC Test Dataset: comparison

 System                               English
                         Facebook           Twitter
                         Dataset            Dataset
 Our system result       0.6407             0.5541
 DA-LD-hildesheim        0.6178             0.552
 saroyehun               0.6425             0.5920
 EBSI-LIA-UNAM           0.6315             0.5715
 TakeLab                 0.5920             0.5651
 taraka rama             0.6008             0.5656
 vista.ue                0.5812             0.6008
 na14                    0.5920             0.5663




           0.3854
           0.4664


 with peers

                    Hindi
     Facebook           Twitter
     Dataset            Dataset
     0.6081             0.4992
     0.6081             0.4992
     NA                 NA
     NA                 NA
     NA                 NA
     0.6420             0.40
     0.5951             0.4829
     0.6450             0.4853




                Figure 1. Heatmap on English Facebook Test Dataset Results.

   Table 13. Results Comparison with rest of team on FIRE 2018 IRMiDis Dataset.

     System              p@100    R@1000     MAP@100       MAP      nDCG @100        nDCG
     Our System          0.4      0.2002     0.0129        0.1471   0.4021           0.7492
     MIDAS-semiauto      0.9600   0.1148     0.0740        0.1345   0.6007           0.6899
     MIDAS-1             0.8800   0.1292     0.0581        0.1329   0.5649           0.6835
     FAST NU Run2        0.7000   0.0885     0.0396        0.0801   0.5723           0.6676
     UEM DataMining      0.6800   0.1427     0.0378        0.1178   0.5332           0.6396
     iitbhu irlab2       0.3900   0.0447     0.0144        0.0401   0.3272           0.6200



                      Figure 2. Heatmap on English Twitter Test Dataset Results.

           Table 14. Results Comparison with CNN model and Logistic Regression TRAC
           Facebook English Test Dataset

            Class             CNN model                    Logistic Regression     #Posts
                       P      R      Weighted F1    P         R      Weighted F1
            NAG        0.86   0.64   0.73           0.83      0.60   0.70          630
            CAG        0.28   0.46   0.35           0.23      0.54   0.32          142
            OAG        0.42   0.61   0.50           0.46      0.39   0.42          144
            overall    0.70   0.61   0.64           0.68      0.56   0.60          916



classifiers and ensemble of classifier with respect to weighted F1 - score on Facebook
English corpus and substantially outperforms on Twitter English corpus. By and large
similar results observed on code-mixed Hindi corpus as shown in table 9 and 10.
   Table 14 present the detailed comparative results of two classifiers: CNN model
with fastText pre-trained vector and the logistic regression with TF/IDF weighting
on TRAC Facebook English dataset. The CNN Model classify Facebook posts better
than logistic regression at the individual class level and overall. It has been quite
evident that posts belong to CAG class are hard to classify and Malmasi, et al. (2017)
reported that the same observation. Table 15 show posts which are miss-classified by
logistic regression however, CNN model correctly classified them into the CAG class.


7.1. Significance Test
To support our claim drawn in the previous section, significance tests, like Wilcoxon
signed-rank test and Student t-test were carried out by comparing Weighted F1 score
of each classifier for each text representation scheme with fastText pre-trained vector
scheme. Table 16 and 17 summarizes the p-values of statistical significance tests on
       Table 15. sample post for the CAG class.

         no     Post text                                               Gold Label     CNN      LR
         1      Mauni singh trying very hard to convince himself what   CAG            CAG      NAG
                is written in script... body language says it all
         2      Indian govt is all Abt giving money to Bangladesh on    CAG            CAG      NAG
                the terms of Bangladesh ll give that all projects to
                amabani n adani for thier benefits lol who cares Abt
                soldiers or India they r just puppets of thier owners
                feku or pappu
         3      When asked to speak in Parliament ran away. Speaks      CAG            CAG      NAG
                only in TV,radio or in election rally. Can we expect
                Another crying drama after Demonetisation disaster ?
                #cryBaby”


   Table 16. p-values of Significance test on F1 -score on

       Text Rep. scheme                Facebook English
                               Wilcoxon           T-test
       Count Vector            0.001              0.12
       TF/IDF                  0.0001             0.1465
       Word2vec                0.00002            0.0001
       Glove                   0.00001            0.000002
       fastText                0.00003            0.0004
       doc2vec-dmc             0.000009           0.0001
       doc2vec-dbow            0.000009           0.00003
       p-Word2vec              0.00001            0.0006
       P-Glove                 0.0002             0.02







 TRAC Facebook English Dataset

                    Twitter English
          Wilcoxon           T-test
          0.004              0.016
          0.0061             0.0391
          0.69               0.30
          0.01               0.0009
          0.12               0.08
          0.0004             0.000004
          0.0004             0.000006
          0.02               0.01
          0.08               0.30




English and Hindi Dataset respectively. In Wilcoxon signed-rank test, p-values of the
results is less than 0.05 for Facebook English dataset and Twitter Hindi Dataset.
However, On Twitter English dataset and Facebook Hindi Dataset, some of the p-
values are higher than 0.05. In student t-test, we get mixed bag results. By and large,
our results are statistically significant.
   In the following subsection, we will try to answer all the research questions framed
during the experiments were planned.


7.2.    Best Text Representation scheme to model the text from Social web
Text Representation is the primary task for to address any NLP task like Ques-
tion/answering, classification etc. As dicusses in section 4, There are basically two
text representing scheme:Bag-of-Word(BoW) with countvector, TF/IDF weighting
and word embedding. Word2Vec (Mikolov et.al , 2013), Glove (Pennington et.al. ,
2014), and fastText (Mikolov et al. , 2018), an extension of Word2vec are popular
word embedding techniques.

   Table 17. p-values of Significance test on F1 -score on TRAC code-mixed Hindi Test Dataset

       Text Rep. scheme           Facebook Code-mixed Hindi             Twitter Code-mixed Hindi
                               Wilcoxon           T-test            Wilcoxon           T-test
       Count Vector            0.003              0.05              0.01               0.20
       TF/IDF                  0.003              0.14              0.007              0.12
       Word2vec                0.40               0.17              0.043              0.017
       Glove                   0.001              0.0005            0.015              0.004
       fastText                0.35               0.15              0.022              0.006
       doc2vec-dmc             0.005              0.00001           0.001              0.0008
       doc2vec-dbow            0.003              0.002             0.001              0.002


   Results clearly show that models with fastText pre-trained vector outperform Glove
pre-trained vector on Facebook test dataset as well as the Twitter test dataset. the
main reason behind the outperformance of fastText over Glove and Word2vec is that
The fastText consider each word as N-gram characters. A word vector for a word
is computed from the sum of the n-gram characters. Glove and Word2vec consider
each word as a single unit and provide a word vector for each word. Since Facebook
users make a lot of mistakes in spelling, typos, fastText is more convenient than Glove
(Majumder et al., 2018). from Figure 1 and 2 shows that BoW is still effective text
representation scheme for the standard machine learning classier which takes hand-
crafted feature and n-grams as inputs. Logistic Regression and Support Vector perform
better than other classifiers in English as well as Hindi Dataset. Adaboost performs
better than LR and SVC on Facebook English Dataset but substantially underperform
them on rest of three Datsets. Our participation (Majumder et al., 2018) in TRAC
competition (Kumar Ritesh et al., 2018) FIRE Information Retrieval from Microblogs
during Disasters (Basu et al. , 2018) track where our team performed well and secured
top position.


7.3. Transfer Learning Model vs Pre-trained Word Embedding Model
Transfer learning is focused on storing knowledge gained while solving one problem and
applying it to a different but related problem. On many occasion, NLP researchers face
the problem of unavailability of sufficient labeled data to train the model. With the
advent of new transfer learning method like ELMO (Peters et al. , 2018) and Universal
language model fine-tuning for Text Classification (ULMFiT) (Howaard et al., 2018)
attract interest among NLP Researchers. These models are trained or large text corpus.
Howaard et al. (2018) claimed that these model can be fine-tuned on the task-specific
corpus. We have used these transfer learning model on TRAC English dataset Kumar
et al. (2018) and results are presented in table 11. one can observe that results are
substantially lower than the results reported in Table 6, 7, and 8 where pre-trained
word vectors are used to initialize the first layer of deep neural model and rest of the
network is trained from scratch achieve better results than transfer learning model.
Howaard et al. (2018) termed use of pre-trained vector as shallow representation.
   In these experiments, we trained different classifier models on Facebook posts. Table
7 10 shows the results on Twitter dataset Kumar et al. (2018). There is lexical
difference between Facebook and Twitter posts. From the results shown in Table 7 8
and table 10, one can conclude that weighted F1 score of standard machine learning
classifiers are substantially lower in Twitter Dataset as compare to Facebook Dataset.
While deep learning models perform better than machine learning classifiers for the
Twitter Dataset. Thus, Deep learning models are more robust than machine learning
classifier across diverse datasets.


7.4. Does Deeper Neural Net make Sense
To answer this question, we designed first CNN model with one convolution layer and
other CNN model with 3 convolution layer with different filters height. As we look
at results shown in Table 6,7,8, and 9, one can conclude that by and large weighted
F1 score lower for CNN model with multiple convolution layer than CNN model with
single convolution layer.


8. Conclusion

In this Paper, Multilingual Social media stream is studied with special kind of text
features: Aggression and fact perspective. Exhaustive experiments are performed to
benchmark the text representation scheme on machine learning classifiers and deep
neural nets. From the results, we conclude that deep Neural model with pre-trained
word embedding is the better choice than machine earning classifier and transfer learn-
ing model. Word embedding is the better text representative scheme than Bag-of-words
for the deep neural models. In fact, performance can be improved with the help of fast-
Text pre-trained vector. However, machine learning classifiers perform better in BoW
with TF/IDF weighting than word embedding. We also concluded that higher drop
out will help to counter model overfitting and improvise a standard evaluation metrics.
CNN and LSTM are the better models for these datasets. On the English test corpus,
we obtained a better weighted F1 score for NAG class and poor weighted F1 score
for CAG class which supports the previous (Malmasi, et al. , 2017) findings. For the
Facebook Hindi test corpus, the same seems not to be true. We obtained a better F1
score for CAG class than NAG class. It is also to be noted that the model leads to
poor result on Twitter test data since the training corpus was created from Facebook.
In such cases, deep neural models substantially outperform machine learning classi-
fiers. Significance test confirms these claims with 95 % confidence interval in most the
cases. Our work shows what kind of problems are moving into the center of attention
for research in machine learning. Using deep learning models, there is great potential
to solve some of these problems, yet still, the performance is far from perfect. Model
transfer between problems and the application of derived knowledge in user interfaces
are areas directions for future work.


References

Aroyehun, Segun Taofeek and Gelbukh, Alexander (2018). Aggression detection in social media:
  Using deep neural networks, data augmentation, and pseudolabeling.Proceedings of the First
  Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018), pp.90–97.
Arroyo-Fernández, Ignacio and Forest, Dominic and Torres-Moreno, Juan-Manuel and
  Carrasco-Ruiz, Mauricio and Legeleux, Thomas and Joannette,Karen (2018). Cyberbullying
  Detection Task: the EBSI-LIA-UNAM System (ELU) at COLING’18 TRAC-1.Proceedings
  of the First Workshop on Trolling,Aggression and Cyberbullying (TRAC-2018), pp 140–149.
Basu, Moumitaand Ghosh, Saptarshi and Ghosh, Kripabandhu (2018). Overview of the FIRE
  2018 track: Information Retrieval from Microblogs during Disasters (IRMiDis). Proceedings
  of FIRE 2018 - Forum for Information Retrieval Evaluation, Gujrat, India, December .
Baziotis, Christos and Pelekis, Nikos and Doulkeridis, Christos (2017). Datastories at semeval-
  2017 task 4: Deep lstm with attention for message-level and topic-based sentiment analysis.
  Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017),
  pp.747–754
Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas (2017) En-
  riching word vectors with subword information, Transactions of the Association for Compu-
  tational Linguistics, vol-5,pp.135–146, MIT Press.
Burnap, Pete and Williams, Matthew Ltitle (2015). Cyber hate speech on twitter: An applica-
  tion of machine classification and statistical modeling for policy and decision makingPolicy
  & Internet, vol-7 number 2, pp.223–242, Wiley Online Library.
Conover, Michael and Ratkiewicz, Jacob and Francisco, Matthew R and Gonçalves , Bruno and
  Menczer, Filippo and Flammini, Alessandro (2011). Political polarization on twitter.,Icwsm,
  vol-133, pp.89–96.


</Results>
<conclusion>
8. Conclusion

In this Paper, Multilingual Social media stream is studied with special kind of text
features: Aggression and fact perspective. Exhaustive experiments are performed to
benchmark the text representation scheme on machine learning classifiers and deep
neural nets. From the results, we conclude that deep Neural model with pre-trained
word embedding is the better choice than machine earning classifier and transfer learn-
ing model. Word embedding is the better text representative scheme than Bag-of-words
for the deep neural models. In fact, performance can be improved with the help of fast-
Text pre-trained vector. However, machine learning classifiers perform better in BoW
with TF/IDF weighting than word embedding. We also concluded that higher drop
out will help to counter model overfitting and improvise a standard evaluation metrics.
CNN and LSTM are the better models for these datasets. On the English test corpus,
we obtained a better weighted F1 score for NAG class and poor weighted F1 score
for CAG class which supports the previous (Malmasi, et al. , 2017) findings. For the
Facebook Hindi test corpus, the same seems not to be true. We obtained a better F1
score for CAG class than NAG class. It is also to be noted that the model leads to
poor result on Twitter test data since the training corpus was created from Facebook.
In such cases, deep neural models substantially outperform machine learning classi-
fiers. Significance test confirms these claims with 95 % confidence interval in most the
cases. Our work shows what kind of problems are moving into the center of attention
for research in machine learning. Using deep learning models, there is great potential
to solve some of these problems, yet still, the performance is far from perfect. Model
transfer between problems and the application of derived knowledge in user interfaces
are areas directions for future work.

</conclusion>
<Reference : >
References

Aroyehun, Segun Taofeek and Gelbukh, Alexander (2018). Aggression detection in social media:
  Using deep neural networks, data augmentation, and pseudolabeling.Proceedings of the First
  Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018), pp.90–97.
Arroyo-Fernández, Ignacio and Forest, Dominic and Torres-Moreno, Juan-Manuel and
  Carrasco-Ruiz, Mauricio and Legeleux, Thomas and Joannette,Karen (2018). Cyberbullying
  Detection Task: the EBSI-LIA-UNAM System (ELU) at COLING’18 TRAC-1.Proceedings
  of the First Workshop on Trolling,Aggression and Cyberbullying (TRAC-2018), pp 140–149.
Basu, Moumitaand Ghosh, Saptarshi and Ghosh, Kripabandhu (2018). Overview of the FIRE
  2018 track: Information Retrieval from Microblogs during Disasters (IRMiDis). Proceedings
  of FIRE 2018 - Forum for Information Retrieval Evaluation, Gujrat, India, December .
Baziotis, Christos and Pelekis, Nikos and Doulkeridis, Christos (2017). Datastories at semeval-
  2017 task 4: Deep lstm with attention for message-level and topic-based sentiment analysis.
  Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017),
  pp.747–754
Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas (2017) En-
  riching word vectors with subword information, Transactions of the Association for Compu-
  tational Linguistics, vol-5,pp.135–146, MIT Press.
Burnap, Pete and Williams, Matthew Ltitle (2015). Cyber hate speech on twitter: An applica-
  tion of machine classification and statistical modeling for policy and decision makingPolicy
  & Internet, vol-7 number 2, pp.223–242, Wiley Online Library.
Conover, Michael and Ratkiewicz, Jacob and Francisco, Matthew R and Gonçalves , Bruno and
  Menczer, Filippo and Flammini, Alessandro (2011). Political polarization on twitter.,Icwsm,
  vol-133, pp.89–96.

Conover, Michael D and Gonçalves, Bruno and Ratkiewicz, Jacob and Flammini, Alessandro
  and Menczer, Filippo (2011). Predicting the political alignment of twitter users, Privacy,
  Security, Risk and Trust (PASSAT) and 2011 IEEE Third Inernational Conference on Social
  Computing (SocialCom), 2011 IEEE Third International Conference on, pp.192–199.
Davidson, Thomas and Warmsley, Dana and Macy, Michael and Weber, Ingmar (2017). Au-
  tomated Hate Speech Detection and the Problem of Offensive Langemuage. Proceedings of
  ICWSM.
Deriu, Jan and Gonzenbach, Maurice and Uzdilli, Fatih and Lucchi, Aurelien and Luca, Valeria
  De and Jaggi, Martin (2016). Swisscheese at semeval-2016 task 4: Sentiment classification
  using an ensemble of convolutional neural networks with distant supervision. Proceedings of
  the 10th international workshop on semantic evaluation,pp.1124–1128.
Hltcoe, J (2013).Semeval-2013 task 2: Sentiment analysis in Twitter,vol-312 Atlanta, Georgia,
  USA.
Howard, Jeremy & Ruder, Sebastian 2018. Universal language model fine-tuning for text clas-
  sification”,arXiv preprint arXiv:1801.06146,
Harris, Zellig S (1954),Distributional structure Word,10, number=2-3,pp.146–162, 1954, Taylor
  & Francis.
Kumar, Ritesh and Reganti, Aishwarya N. and Bhatia, Akshit and Maheshwari,Tushar (2018),
  Aggression-annotated Corpus of Hindi-English Code-mixed Data, Proceedings of the 11th
  Language Resources and Evaluation Conference (LREC), Miyazaki, Japan.
Kumar, Ritesh and Ojha, Atul Kr. and Malmasi, Shervin and Zampieri Marcos (2018). Bench-
  marking Aggression Identification in Social Media, Proceedings of the First Workshop on
  Trolling, Aggression and Cyberbulling (TRAC), Santa Fe, USA
Kwok, Irene and Wang, Yuzhou (2013),Locate the hate: Detecting Tweets Against Blacks
  ,Twenty-Seventh AAAI Conference on Artificial Intelligence.
Lau, Jey Han and Baldwin, Timothy (2016). An empirical evaluation of doc2vec with practical
  insights into document embedding generation. arXiv preprint arXiv:1607.05368.
Le, Quoc and Mikolov, Tomas (2014) Distributed representations of sentences and docu-
  ments.International Conference on Machine Learning, pp.1188–1196
Majumder, Prasenjit and Mandl, Thomas and Modha Sandip (2018)Filtering Aggression from
  the Multilingual Social Media Feed Proceedings of the First Workshop on Trolling, Aggres-
  sion and Cyberbullying (TRAC-2018), pp. 199–207
Malmasi, Shervin and Zampieri, Marcos (2017)Detecting Hate Speech in Social Media Pro-
  ceedings of the International Conference Recent Advances in Natural Language Processing
  (RANLP), pp.467–472.
Malmasi, Shervin and Zampieri, Marcos (2018).Challenges in Discriminating Profanity from
  Hate Speech Journal of Experimental & Theoretical Artificial Intelligence pp.1–16, vol-30,
  issue-2,Taylor & Francis.
Maynard, Diana and Funk, Adam (2011) Automatic detection of political opinionsin tweets,
  Extended Semantic Web Conference,pp. 88–99, Springer.
Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey (2013). Efficient estima-
  tion of word representations in vector space,arXiv preprint arXiv:1301.3781.
Mikolov, Tomas and Grave, Edouard and Bojanowski, Piotr and Puhrsch, Christian and Joulin,
  Armand (2018). Advances in Pre-Training Distributed Word Representations,Proceedings of
  the International Conference on Language Resources and Evaluation (LREC 2018).
Modha, Sandip and Agrawal, Krati and Verma, Deepali and Majumder, Prasenjit and Man-
  dalia, Chintak 2016. DAIICT at TREC RTS 2016: Live Push Notification and Email Di-
  gest.,TREC.
Mohammad, Saif M and Kiritchenko, Svetlana and Zhu, Xiaodan (2013). NRC-Canada: Build-
  ing the state-of-the-art in sentiment analysis of tweets.arXiv preprint arXiv:1308.6242.
Nakov, Preslav and Ritter, Alan and Rosenthal, Sara and Sebastiani, Fabrizio and Stoyanov,
  Veselin (2016). SemEval-2016 task 4: Sentiment analysis in Twitter,Proceedings of the 10th
  international workshop on semantic evaluation (semeval-2016), pp. 1–18.
Pennington, Jeffrey and Socher, Richard and Manning, Christopher Glove: Global vectors for


</Reference : >
