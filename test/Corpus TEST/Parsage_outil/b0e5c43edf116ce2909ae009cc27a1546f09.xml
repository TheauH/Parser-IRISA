<preamble>
b0e5c43edf116ce2909ae009cc27a1546f09.pdf
</preamble>
<titre>
Inclusive yet Selective: Supervised Distributional Hypernymy Detection
</titre>
<auteurs>
<auteur>Stephen Roller (roller@cs.utexas.edu,)</auteur>
<affiliation>
∗
Department of Computer Science
†
Department of Linguistics
The University of Texas at Austin
</affiliation>
<auteur>Katrin Erk (gemma.boleda@upf.edu)</auteur>
<affiliation>
Department of Computer Science
†
Department of Linguistics
The University of Texas at Austin
</affiliation>
<auteur>Gemma Boleda (katrin.erk@mail.utexas.edu,)</auteur>
<affiliation>
Department of Computer Science
†
Department of Linguistics
The University of Texas at Austin
</affiliation>
</auteurs>
<abstract>
                                                      Abstract

     We test the Distributional Inclusion Hypothesis, which states that hypernyms tend to occur in
     a superset of contexts in which their hyponyms are found. We find that this hypothesis only
     holds when it is applied to relevant dimensions. We propose a robust supervised approach that
     achieves accuracies of .84 and .85 on two existing datasets and that can be interpreted as selecting
     the dimensions that are relevant for distributional inclusion.

</abstract>
<introduction>
1    Introduction
One of the main criticisms of distributional models has been that they fail to distinguish between semantic
relations: Typical nearest neighbors of dog are words like cat, animal, puppy, tail, or owner, all obviously
related to dog, but through very different types of semantic relations. On these grounds, Murphy (2002)
argues that distributional models cannot be a valid model of conceptual representation. Distinguishing
semantic relations are also crucial for drawing inferences from distributional data, as different semantic
relations lead to different inference rules (Lenci, 2008). This is of practical import for tasks such as
Recognizing Textual Entailment or RTE (Geffet and Dagan, 2004).
   For these reasons, research has in recent years started to attempt the detection of specific semantic
relationships, and current results suggest that distributional models can, in fact, distinguish between
semantic relations, given the right similarity measures (Weeds et al., 2004; Kotlerman et al., 2010; Lenci
and Benotto, 2012; Herbelot and Ganesalingam, 2013; Santus, 2013). Because of its relevance for RTE
and other tasks, much of this work has focused on hypernymy. Hypernymy is the semantic relation
between a superordinate term in a taxonomy (e.g. animal) and a subordinate term (e.g. dog).
   Distributional approaches to date for detecting hypernymy, and the related but broader relation of
lexical entailment, have been unsupervised (except for Baroni et al. (2012)) and have mostly been based
on the Distributional Inclusion Hypothesis (Zhitomirsky-Geffet and Dagan, 2005; Zhitomirsky-Geffet
and Dagan, 2009), which states that more specific terms appear in a subset of the distributional contexts
in which more general terms appear. So, animal can occur in all the contexts in which dog can occur,
plus some contexts in which dog cannot – for instance, rights can be a typical cooccurrence for animal
(e.g. “animal rights”), but not so much for dog (e.g. #“dog rights”).
   This paper takes a closer look at the Distributional Inclusion Hypothesis for hypernymy detection. We
show that the current best unsupervised approach is brittle in that their performance depends on the space
they are applied to. This raises the question of whether the Distributional Inclusion Hypothesis is correct,
and if so, under what circumstances it holds. We use a simple supervised approach to relation detection
that has good performance (accuracy .84 on B LESS, .85 on the lexical entailment dataset of Baroni et
al. (2012)) and works well across different spaces.1 Furthermore, we show that it can be interpreted
as selecting dimensions for which the Distributional Inclusion Hypothesis does hold. So, our answer is
to propose the Selective Distributional Inclusion Hypothesis: The Distributional Inclusion Hypothesis
holds, but only for relevant dimensions.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
    1
      Code and data are available at http://stephenroller.com/research/coling14.


                                                         1025
     Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,2   Background
Distributional models. Distributional models represent a word through the contexts in which it has
been observed, usually in the form of a vector representation (Turney and Pantel, 2010). A target word
is represented as a vector in a high-dimensional space in which the dimensions are context items (for
example, other words) and the coordinates of the vector indicate the target’s degree of association with
each context item. In this paper, we also use dimensionality reduced spaces in which dimensions do not
stand for individual context items anymore.
Pattern-based approaches to inducing semantic relations. Early work on automatically inducing se-
mantic relations between words, starting with Hearst (1992), uses textual patterns. For example, “[NP1 ]
and other [NP2 ]” implies that NP2 is a hypernym of NP1 . Pattern-based approaches have been applied
to meronymy (Berland and Charniak, 1999; Girju et al., 2003; Girju et al., 2006), synonymy (Lin et al.,
2003), co-hyponymy (Snow et al., 2005), hypernymy (Cimiano et al., 2005), and several relations be-
tween verbs (Chklovski and Pantel, 2004). Pantel and Pennachiotti (2006) generalize the idea to a wide
variety of relations. Turney (2006) uses vectors of patterns to determine similarity of semantic relations.
A task related to semantic relation induction is the extension of an existing taxonomy (Buitelaar et al.,
2005). Snow et al. (2006) do this by using hypernymy and co-hyponymy detectors.
Lexical entailment, hypernymy, and the Distributional Inclusion Hypothesis. Weeds et al. (2004)
introduce the notion of distributional generality, where v is distributionally more general than u if u
appears in a subset of the contexts in which v is found, and speculate that hypernyms (v) should be more
distributionally general than hyponyms (u). Zhitomirsky-Geffet and Dagan (2005; 2009) introduce the
term Distributional Inclusion Hypothesis for the idea that distributional generality encodes hypernymy
or the more loosely defined relation of lexical entailment.
   Weeds and Weir (2003) measure distributional generality using a notion of precision (eq. 1). Here and
in all equations below, u is the narrower term, and v the more general one. Abusing notation, we write u
for both a word and its associated vector hu1 , . . . , un i. Kotlerman et al. (2010) predict lexical entailment
with the balAPinc measure, a modification of the Average Precision (AP) measure (eq. 2). The general
notion is that scores should increase with the number of dimensions of v that u shares, and also give more
weight to the highly ranked dimensions (i.e. largest magnitude) of the narrower term u. This is captured
in APinc by computing precision P (r) at every rank r among u’s dimensions – where precision is the
fraction of dimensions shared with v –, and weighting by the rank of the same dimension in the broader
term, rel0 (v, r, u). The final measure, balAPinc, smooths using the LIN similarity measure (Lin, 1998).
(We only sketch this measure here due to its complexity; details are given in Kotlerman et al. (2010).)
                                                    (
                                                        1 if x &gt; 0;
                                          1(x) =
                                                        0 otherwise
                                                              Pn
                                                                      u · 1(vi )
                                   W eedsP rec(u, v) = i=1        Pn i                                       (1)
                                                                      i=1 ui
                                                 P|1(u)|
                                                             P (r) · rel0 (v, r, u))
                                 APinc(u, v) = r=1                                                           (2)
                                                                |1(u)|
                                                     p
                                balAPinc(u, v) = APinc(u, v) · LIN(u, v)
  The ClarkeDE measure (Clarke, 2009) computes degree of entailment as the degree to which the nar-
rower term u has lower values than v across all dimensions (eq. 3). Lenci and Benotto (2012) introduce
the invCL measure, which uses ClarkeDE to measure both distributional inclusion of u in v and distri-
butional non-inclusion of v in u (eq. 4). While all other measures interpret the Distributional Inclusion
Hypothesis as the degree to which a ⊆ relation holds, Lenci and Benotto test the degree to which proper
inclusion ( holds. They consider not only the degree to which the contexts of the narrower terms are
included in the contexts of the wider term, but also determine the degree to which the wider term has
contexts that the narrower term does not have.


</introduction>
<Results>
Table 2 shows the performance of the two classifiers, Concat and Diff, on both the B LESS and E NTAIL -
MENT datasets, using three underlying spaces. We use the reduced versions of the three spaces, indicated
by the subscript 300 . Note that the Concat classifier could not converge using features from TypeDM300 ,
so we omit the result. With both methods, we obtain a high accuracy on the two datasets, with results
around .8 against baselines around .5. Our best result is .84 on B LESS and .85 on E NTAILMENT. More-
over, both approaches are in general robust to changes in space parameters (with TypeDM/Concat an
outlier). Still, the U+W2300 space seems to be the best for this task: Its scores are significantly6 higher
than the rest, except for TypeDM on E NTAILMENT, which achieves the same score as U+W2300 . Diff
achieves significantly higher results than Concat.
   When provided more information, Concat outperforms Diff. For instance, if cross-validation is done
over all pairs in B LESS in the U+W2300 space, Concat achieves .98 accuracy, while Diff obtains .90.
However, in this setting the same words appear in the training and test sets (albeit in different pairs).
We take this to mean that Concat is memorizing, rather than learning the hypernymy relation. This
emphasizes the need for our stricter evaluation that prevents repetition between training and test sets.
   Clearly, both classifiers do fairly well at predicting hypernymy relations between words, regardless
of space. Naturally, one should ask what are the classifiers capturing that the unsupervised measures
are missing? We propose that the supervised classifiers perform essentially the same operation as the
unsupervised measures, but are learning to determine the relevance of dimensions. In particular, Diff
</Results>
<conclusion>
7   Conclusion
In this paper, we have tested the Distributional Inclusion Hypothesis, the basis for distributional ap-
proaches to hypernymy. We have found that the hypothesis only works if inclusion is selectively applied
to a set of relevant dimensions.
just by restricting to a smaller space, we evaluated the similarity measures on a variation of the U+W2 space which uses 500
randomly selected dimensions from the original space. The results are approximately unchanged from those on the original
U+W2 space.

   We have tested two simple supervised approaches to distributional hypernymy detection and have
found that they show good performance, and are robust to changes in the underlying space. Our best
classifier achieves .84 accuracy on B LESS and .85 on the E NTAILMENT dataset of Baroni et al. (2012). It
uses features that encode dimension-wise difference between vectors. This classifier can be interpreted
as selecting the dimensions necessary for the Distributional Inclusion Hypothesis to work, thus as an
effective way to implement selective distributional inclusion.
   The next natural step is to use the supervised features to guide development of an unsupervised mea-
sure for hypernymy detection: Now that we have examples, we hope to propose a method which selects
relevant features automatically. We also would like to explore detection of other relationships, such
as meronymy. Finally, we would like to perform an extrinsic evaluation of our hypernymy detection
approach in an actual RTE system.
</conclusion>
<Reference : >
References
Marco Baroni and Alessandro Lenci. 2011. How we BLESSed distributional semantic evaluation. In Proceedings
 of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, pages 1–10, Edinburgh,
 UK, July. Association for Computational Linguistics.

Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky wide web: A
 collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation,
 43(3):209–226.

Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and Chung-chieh Shan. 2012. Entailment above the word
 level in distributional semantics. In Proceedings of the 13th Conference of the European Chapter of the As-
 sociation for Computational Linguistics, pages 23–32, Avignon, France, April. Association for Computational
 Linguistics.

Matthew Berland and Eugene Charniak. 1999. Finding parts in very large corpora. In Proceedings of the 37th
 Annual Meeting of the Association for Computational Linguistics, pages 57–64, College Park, Maryland, USA,
 June. Association for Computational Linguistics.

Paul Buitelaar, Philipp Cimiano, and Bernardo Magnini. 2005. Ontology Learning from Text: Methods, Evaluation
  and Applications. Frontiers in Artificial Intelligence and Applications Series. IOS Press, Amsterdam.

Timothy Chklovski and Patrick Pantel. 2004. Verbocean: Mining the web for fine-grained semantic verb relations.
  In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 33–40.

Philipp Cimiano, Aleksander Pivk, Lars Schmidt-Thieme, and Steffen Staab. 2005. Learning taxonomic relations
  from heterogeneous sources of evidence. Ontology Learning from Text: Methods, evaluation and applications.

Daoud Clarke. 2009. Context-theoretic semantics for natural language: an overview. In Proceedings of the
  Workshop on Geometrical Models of Natural Language Semantics, pages 112–119, Athens, Greece, March.
  Association for Computational Linguistics.

Maayan Geffet and Ido Dagan. 2004. Feature vector quality and distributional similarity. In Proceedings of the
 20th International Conference on Computational Linguistics, page 247. Association for Computational Linguis-
 tics.

Roxana Girju, Adriana Badulescu, and Dan Moldovan. 2003. Learning semantic constraints for the automatic
  discovery of part-whole relations. In Proceedings of the 2003 Conference of the North American Chapter of the
  Association for Computational Linguistics on Human Language Technology-Volume 1, pages 1–8. Association
  for Computational Linguistics.
  10
       http://www.tacc.utexas.edu

Roxana Girju, Adriana Badulescu, and Dan Moldovan. 2006. Automatic discovery of part-whole relations. Com-
  putational Linguistics, 32(1):83–135.

Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th
 Conference on Computational Linguistics, pages 539–545, Stroudsburg, PA, USA. Association for Computa-
 tional Linguistics.

Aurélie Herbelot and Mohan Ganesalingam. 2013. Measuring semantic content in distributional vectors. In
  Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short
  Papers), pages 440–445, Sofia, Bulgaria, August. Association for Computational Linguistics.

Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-geffet. 2010. Directional distributional
   similarity for lexical inference. Natural Language Engineering, 16:359–389, 10.

Alessandro Lenci and Giulia Benotto. 2012. Identifying hypernyms in distributional semantic spaces. In *SEM
  2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the
  main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Se-
  mantic Evaluation (SemEval 2012), pages 75–79, Montréal, Canada, 7-8 June. Association for Computational
  Linguistics.

Alessandro Lenci. 2008. Distributional approaches in linguistic and cognitive research. Italian Journal of Lin-
  guistics, 20(1):1–31.

Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou. 2003. Identifying synonyms among distributionally
  similar words. In Proceedings of the 18th international Joint Conference on Artificial intelligence, pages 1492–
  1493.

Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th International
  Conference on Machine Learning, volume 98, pages 296–304.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word rep-
  resentations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for
  Computational Linguistics: Human Language Technologies, pages 746–751, Atlanta, Georgia, June. Associa-
  tion for Computational Linguistics.

Gregory L. Murphy. 2002. The Big Book of Concepts. MIT Press, Boston, MA.

Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting
  semantic relations. In Proceedings of the 21st International Conference on Computational Linguistics and the
  44th annual meeting of the Association for Computational Linguistics.

Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertran Thirion, Olivier Grisel, Mathieu
  Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Courna-
  peau, Matthieu Brucher, MMatthieu Perrot, and Édouard Duchesnay. 2011. Scikit-learn: Machine learning in
  Python. Journal of Machine Learning Research, 12:2825–2830.

Enrico Santus. 2013. SLQS: An entropy measure. Master’s thesis, University of Pisa.

Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005. Learning syntactic patterns for automatic hypernym dis-
  covery. In Lawrence K. Saul, Yair Weiss, and Léon Bottou, editors, Advances in Neural Information Processing
  Systems 17, pages 1297–1304, Cambridge, MA. MIT Press.

Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006. Semantic taxonomy induction from heterogenous evidence.
  In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting
  of the Association for Computational Linguistics, ACL-44, pages 801–808, Stroudsburg, PA, USA. Association
  for Computational Linguistics.

Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal
  of Artificial Intelligence Research, 37:141–188.

Peter D. Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3):379–416.

Julie Weeds and David Weir. 2003. A general framework for distributional similarity. In Michael Collins and Mark
   Steedman, editors, Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,
   pages 81–88.


</Reference : >
