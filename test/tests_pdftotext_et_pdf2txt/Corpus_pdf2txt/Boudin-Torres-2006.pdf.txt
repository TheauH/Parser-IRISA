Coling 2008: Companion volume – Posters and Demonstrations, pages 23–26

Manchester, August 2008

23

AScalableMMRApproachtoSentenceScoringforMulti-DocumentUpdateSummarizationFlorianBoudin\andMarcEl-B`eze\\LaboratoireInformatiqued’Avignon339chemindesMeinajaries,BP1228,84911AvignonCedex9,France.florian.boudin@univ-avignon.frmarc.elbeze@univ-avignon.frJuan-ManuelTorres-Moreno\,[[´EcolePolytechniquedeMontr´ealCP6079Succ.CentreVilleH3C3A7Montr´eal(Qu´ebec),Canada.juan-manuel.torres@univ-avignon.frAbstractWepresentSMMR,ascalablesentencescoringmethodforquery-orientedup-datesummarization.Sentencesarescoredthankstoacriterioncombiningqueryrele-vanceanddissimilaritywithalreadyreaddocuments(history).Astheamountofdatainhistoryincreases,non-redundancyisprioritizedoverquery-relevance.WeshowthatSMMRachievespromisingre-sultsontheDUC2007updatecorpus.1IntroductionExtensiveexperimentsonquery-orientedmulti-documentsummarizationhavebeencarriedoutoverthepastfewyears.Mostofthestrategiestoproducesummariesarebasedonanextrac-tionmethod,whichidentiﬁessalienttextualseg-ments,mostoftensentences,indocuments.Sen-tencescontainingthemostsalientconceptsarese-lected,orderedandassembledaccordingtotheirrelevancetoproducesummaries(alsocalledex-tracts)(ManiandMaybury,1999).RecentlyemergedfromtheDocumentUnder-standingConference(DUC)20071,updatesum-marizationattemptstoenhancesummarizationwhenmoreinformationaboutknowledgeacquiredbytheuserisavailable.Itasksthefollowingques-tion:hastheuseralreadyreaddocumentsonthetopic?Inthecaseofapositiveanswer,producinganextractfocusingononlynewfactsisofinter-est.Inthisway,animportantissueisintroduced:c(cid:13)2008.LicensedundertheCreativeCommonsAttribution-Noncommercial-ShareAlike3.0Unportedli-cense(http://creativecommons.org/licenses/by-nc-sa/3.0/).Somerightsreserved.1DocumentUnderstandingConferencesareconductedsince2000bytheNationalInstituteofStandardsandTech-nology(NIST),http://www-nlpir.nist.govredundancywithpreviouslyreaddocuments(his-tory)hastoberemovedfromtheextract.Anaturalwaytogoaboutupdatesummarizationwouldbeextractingtemporaltags(dates,elapsedtimes,temporalexpressions...)(ManiandWilson,2000)ortoautomaticallyconstructthetimelinefromdocuments(SwanandAllan,2000).Thesetemporalmarkscouldbeusedtofocusextractsonthemostrecentlywrittenfacts.However,mostre-centlywrittenfactsarenotnecessarilynewfacts.MachineReading(MR)wasusedby(Hickletal.,2007)toconstructknowledgerepresentationsfromclustersofdocuments.Sentencescontain-ing“new”information(i.e.thatcouldnotbein-ferredbyanypreviouslyconsidereddocument)areselectedtogeneratesummary.However,thishighlyefﬁcientapproach(bestsysteminDUC2007update)requireslargelinguisticresources.(Witteetal.,2007)proposearule-basedsystembasedonfuzzycoreferenceclustergraphs.Again,thisapproachrequirestomanuallywritethesen-tencerankingscheme.Severalstrategiesremain-ingonpost-processingredundancyremovaltech-niqueshavebeensuggested.Extractsconstructedfromhistorywereusedby(BoudinandTorres-Moreno,2007)tominimizehistory’sredundancy.(Linetal.,2007)haveproposedamodiﬁedMax-imalMarginalRelevance(MMR)(CarbonellandGoldstein,1998)re-rankerduringsentenceselec-tion,constructingthesummarybyincrementallyre-rankingsentences.Inthispaper,weproposeascalablesentencescoringmethodforupdatesummarizationderivedfromMMR.Motivatedbytheneedforrelevantnovelty,candidatesentencesareselectedaccord-ingtoacombinedcriterionofqueryrelevanceanddissimilaritywithpreviouslyreadsentences.Therestofthepaperisorganizedasfollows.Section224

introducesourproposedsentencescoringmethodandSection3presentsexperimentsandevaluatesourapproach.2MethodTheunderlyingideaofourmethodisthatasthenumberofsentencesinthehistoryincreases,thelikelihoodtohaveredundantinformationwithincandidatesentencesalsoincreases.WeproposeascalablesentencescoringmethodderivedfromMMRthat,asthesizeofthehistoryincreases,givesmoreimportancetonon-redundancythattoqueryrelevance.WedeﬁneHtorepresentthepre-viouslyreaddocuments(history),Qtorepresentthequeryandsthecandidatesentence.Thefol-lowingsubsectionsformallydeﬁnethesimilaritymeasuresandthescalableMMRscoringmethod.2.1Aquery-orientedmulti-documentsummarizerWehaveﬁrststartedbyimplementingasimplesummarizerforwhichthetaskistoproducequery-focusedsummariesfromclustersofdocuments.Eachdocumentispre-processed:documentsaresegmentedintosentences,sentencesareﬁltered(wordswhichdonotcarrymeaningareremovedsuchasfunctionalwordsorcommonwords)andnormalizedusingalemmasdatabase(i.e.inﬂectedforms“go”,“goes”,“went”,“gone”...arereplacedby“go”).AnN-dimensionalterm-spaceΓ,whereNisthenumberofdifferenttermsfoundinthecluster,isconstructed.SentencesarerepresentedinΓbyvectorsinwhicheachcomponentisthetermfrequencywithinthesentence.Sentencescor-ingcanbeseenasapassageretrievaltaskinInfor-mationRetrieval(IR).Eachsentencesisscoredbycomputingacombinationoftwosimilaritymea-suresbetweenthesentenceandthequery.Theﬁrstmeasureisthewellknowncosineangle(Saltonetal.,1975)betweenthesentenceandthequeryvec-torialrepresentationsinΓ(denotedrespectively~sand~Q).ThesecondsimilaritymeasureisbasedontheJaro-Winklerdistance(Winkler,1999).TheoriginalJaro-Winklermeasure,denotedJW,usesthenumberofmatchingcharactersandtransposi-tionstocomputeasimilarityscorebetweentwoterms,givingmorefavourableratingstotermsthatmatchfromthebeginning.WehaveextendedthismeasuretocalculatethesimilaritybetweenthesentencesandthequeryQ:JWe(s,Q)=1|Q|·Xq∈Qmaxm∈S0JW(q,m)(1)whereS0isthetermsetofsinwhichthetermsmthatalreadyhavemaximizedJW(q,m)arere-moved.TheuseofJWesmoothsnormalizationandmisspellingerrors.Eachsentencesisscoredusingthelinearcombination:Sim1(s,Q)=α·cosine(~s,~Q)+(1−α)·JWe(s,Q)(2)whereα=0.7,optimallytunedonthepastDUCsdata(2005and2006).Thesystemproducesalistofrankedsentencesfromwhichthesummaryisconstructedbyarrangingthehighscoredsentencesuntilthedesiredsizeisreached.2.2AscalableMMRapproachMMRre-rankingalgorithmhasbeensuccessfullyusedinquery-orientedsummarization(Yeetal.,2005).Itstrivestoreduceredundancywhilemain-tainingqueryrelevanceinselectedsentences.Thesummaryisconstructedincrementallyfromalistofrankedsentences,ateachiterationthesentencewhichmaximizesMMRischosen:MMR=argmaxs∈S[λ·Sim1(s,Q)−(1−λ)·maxsj∈ESim2(s,sj)](3)whereSisthesetofcandidatessentencesandEisthesetofselectedsentences.λrepresentsaninterpolationcoefﬁcientbetweensentence’srele-vanceandnon-redundancy.Sim2(s,sj)isanor-malizedLongestCommonSubstring(LCS)mea-surebetweensentencessandsj.Detectingsen-tencerehearsals,LCSiswelladaptedforredun-dancyremoval.WeproposeaninterpretationofMMRtotackletheupdatesummarizationissue.SinceSim1andSim2arerangedin[0,1],theycanbeseenasprob-abilitieseventhoughtheyarenot.Justasrewriting(3)as(NRstandsforNoveltyRelevance):NR=argmaxs∈S[λ·Sim1(s,Q)+(1−λ)·(1−maxsh∈HSim2(s,sh))](4)Wecanunderstandthat(4)equatestoanORcom-bination.Butaswearelookingforamoreintu-itiveANDandsincethesimilaritiesareindepen-dent,wehavetousetheproductcombination.The25

scoringmethoddeﬁnedin(2)ismodiﬁedintoadoublemaximizationcriterioninwhichthebestrankedsentencewillbethemostrelevanttothequeryANDthemostdifferenttothesentencesinH.SMMR(s)=Sim1(s,Q)·(cid:18)1−maxsh∈HSim2(s,sh)(cid:19)f(H)(5)Decreasingλin(3)withthelengthofthesum-marywassuggestedby(Murrayetal.,2005)andsuccessfullyusedintheDUC2005by(Hacheyetal.,2005),therebyemphasizingtherelevanceattheoutsetbutincreasinglyprioritizingredun-dancyremovalastheprocesscontinues.Sim-ilarly,weproposetofollowthisassumptioninSMMRusingafunctiondenotedfthatastheamountofdatainhistoryincreases,prioritizenon-redundancy(f(H)→0).3ExperimentsThemethoddescribedintheprevioussectionhasbeenimplementedandevaluatedbyusingtheDUC2007updatecorpus2.Thefollowingsubsec-tionspresentdetailsofthedifferentexperimentswehaveconducted.3.1TheDUC2007updatecorpusWeusedforourexperimentstheDUC2007up-datecompetitiondataset.Thecorpusiscomposedof10topics,with25documentspertopic.Theup-datetaskgoalwastoproduceshort(∼100words)multi-documentupdatesummariesofnewswirear-ticlesundertheassumptionthattheuserhasal-readyreadasetofearlierarticles.Thepurposeofeachupdatesummarywillbetoinformthereaderofnewinformationaboutaparticulartopic.GivenaDUCtopicandits3documentclusters:A(10documents),B(8documents)andC(7doc-uments),thetaskistocreatefromthedocumentsthreebrief,ﬂuentsummariesthatcontributetosat-isfyingtheinformationneedexpressedinthetopicstatement.1.AsummaryofdocumentsinclusterA.2.AnupdatesummaryofdocumentsinB,un-dertheassumptionthatthereaderhasalreadyreaddocumentsinA.2MoreinformationabouttheDUC2007corpusisavail-ableathttp://duc.nist.gov/.3.AnupdatesummaryofdocumentsinC,un-dertheassumptionthatthereaderhasalreadyreaddocumentsinAandB.Withinatopic,thedocumentclustersmustbepro-cessedinchronologicalorder.Oursystemgener-atesasummaryforeachclusterbyarrangingthehighrankedsentencesuntilthelimitof100wordsisreached.3.2EvaluationMostexistingautomatedevaluationmethodsworkbycomparingthegeneratedsummariestooneormorereferencesummaries(ideally,producedbyhumans).Toevaluatethequalityofourgeneratedsummaries,wechoosetousetheROUGE3(Lin,2004)evaluationtoolkit,thathasbeenfoundtobehighlycorrelatedwithhumanjudgments.ROUGE-Nisan-gramrecallmeasurecalculatedbetweenacandidatesummaryandasetofreferencesum-maries.InourexperimentsROUGE-1,ROUGE-2andROUGE-SU4willbecomputed.3.3ResultsTable1reportstheresultsobtainedontheDUC2007updatedatasetfordifferentsentencescor-ingmethods.cosine+JWestandsforthescor-ingmethoddeﬁnedin(2)andNRimprovesitwithsentencere-rankingdeﬁnedinequation(4).SMMRisthecombinedadaptationwehavepro-posedin(5).Thefunctionf(H)usedinSMMRisthesimplerationalfunction1H,whereHincreaseswiththenumberofpreviousclusters(f(H)=1forclusterA,12forclusterBand13forclusterC).Thisfunctionallowstosimplytesttheassumptionthatnon-redundancyhavetobefavouredasthesizeofhistorygrows.Baselineresultsareobtainedonsummariesgeneratedbytakingtheleadingsen-tencesofthemostrecentdocumentsofthecluster,upto100words(ofﬁcialbaselineofDUC).ThetablealsoliststhethreetopperformingsystemsatDUC2007andthelowestscoredhumanreference.Aswecanseefromtheseresults,SMMRout-performstheothersentencescoringmethods.BywaysofcomparisonoursystemwouldhavebeenrankedsecondattheDUC2007updatecompeti-tion.Moreover,nopost-processingwasappliedtotheselectedsentencesleavinganimportantmarginofprogress.Anotherinterestingresultisthehighperformanceofthenon-updatespeciﬁcmethod(cosine+JWe)thatcouldbeduetothesmallsize3ROUGEisavailableathttp://haydn.isi.edu/ROUGE/.26

ofthecorpus(littleredundancybetweenclusters).ROUGE-1ROUGE-2ROUGE-SU4Baseline0.262320.045430.082473rdsystem0.357150.096220.132452ndsystem0.369650.098510.13509cosine+JWe0.359050.101610.13701NR0.362070.100420.13781SMMR0.363230.102230.138861stsystem0.370320.111890.14306Worsthuman0.404970.105110.14779Table1:ROUGEaveragerecallscorescomputedontheDUC2007updatecorpus.4DiscussionandFutureWorkInthispaperwehavedescribedSMMR,ascal-ablesentencescoringmethodbasedonMMRthatachievesverypromisingresults.Animportantas-pectofoursentencescoringmethodisthatitdoesnotrequiresre-rankingnorlinguisticknowledge,whichmakesitasimpleandfastapproachtotheissueofupdatesummarization.ItwaspointedoutattheDUC2007workshopthatQuestionAnswer-ingandquery-orientedsummarizationhavebeenconvergingonacommontask.Thevalueaddedbysummarizationliesinthelinguisticquality.Ap-proachesmixingIRtechniquesarewellsuitedforquery-orientedsummarizationbuttheyrequirein-tensiveworkformakingthesummaryﬂuentandcoherent.Amongtheothers,thisisapointthatwethinkisworthyoffurtherinvestigation.AcknowledgmentsThisworkwassupportedbytheAgenceNationaledelaRecherche,France,projectRPM2.ReferencesBoudin,F.andJ.M.Torres-Moreno.2007.ACo-sineMaximization-MinimizationapproachforUser-OrientedMulti-DocumentUpdateSummarization.InRecentAdvancesinNaturalLanguageProcessing(RANLP),pages81–87.Carbonell,J.andJ.Goldstein.1998.TheuseofMMR,diversity-basedrerankingforreorderingdocumentsandproducingsummaries.In21stannualinterna-tionalACMSIGIRconferenceonResearchandde-velopmentininformationretrieval,pages335–336.ACMPressNewYork,NY,USA.Hachey,B.,G.Murray,andD.Reitter.2005.TheEmbraSystematDUC2005:Query-orientedMulti-documentSummarizationwithaVeryLargeLatentSemanticSpace.InDocumentUnderstandingCon-ference(DUC).Hickl,A.,K.Roberts,andF.Lacatusu.2007.LCC’sGISTexteratDUC2007:MachineReadingforUp-dateSummarization.InDocumentUnderstandingConference(DUC).Lin,Z.,T.S.Chua,M.Y.Kan,W.S.Lee,L.Qiu,andS.Ye.2007.NUSatDUC2007:UsingEvolu-tionaryModelsofText.InDocumentUnderstandingConference(DUC).Lin,C.Y.2004.Rouge:APackageforAutomaticEvaluationofSummaries.InWorkshoponTextSum-marizationBranchesOut,pages25–26.Mani,I.andM.T.Maybury.1999.AdvancesinAuto-maticTextSummarization.MITPress.Mani,I.andG.Wilson.2000.Robusttemporalpro-cessingofnews.In38thAnnualMeetingonAsso-ciationforComputationalLinguistics,pages69–76.AssociationforComputationalLinguisticsMorris-town,NJ,USA.Murray,G.,S.Renals,andJ.Carletta.2005.ExtractiveSummarizationofMeetingRecordings.InNinthEu-ropeanConferenceonSpeechCommunicationandTechnology.ISCA.Salton,G.,A.Wong,andC.S.Yang.1975.Avectorspacemodelforautomaticindexing.Communica-tionsoftheACM,18(11):613–620.Swan,R.andJ.Allan.2000.Automaticgenerationofoverviewtimelines.In23rdannualinternationalACMSIGIRconferenceonResearchanddevelop-mentininformationretrieval,pages49–56.Winkler,W.E.1999.Thestateofrecordlinkageandcurrentresearchproblems.InSurveyMethodsSec-tion,pages73–79.Witte,R.,R.Krestel,andS.Bergler.2007.Generat-ingUpdateSummariesforDUC2007.InDocumentUnderstandingConference(DUC).Ye,S.,L.Qiu,T.S.Chua,andM.Y.Kan.2005.NUSatDUC2005:Understandingdocumentsviacon-ceptlinks.InDocumentUnderstandingConference(DUC).