              WiSeBE: Window-Based Sentence
                   Boundary Evaluation

 Carlos-Emiliano González-Gallardo1,2(B) and Juan-Manuel Torres-Moreno1,2
 1
     LIA - Université d’Avignon et des Pays de Vaucluse, 339 chemin des Meinajaries,
                                 84140 Avignon, France
           carlos-emiliano.gonzalez-gallardo@alumni.univ-avignon.fr,
                         juan-manuel.torres@univ-avignon.fr
              2
                Département de GIGL, École Polytechnique de Montréal,
           C.P. 6079, succ. Centre-ville, Montréal, Québec H3C 3A7, Canada


        Abstract. Sentence Boundary Detection (SBD) has been a major
        research topic since Automatic Speech Recognition transcripts have been
        used for further Natural Language Processing tasks like Part of Speech
        Tagging, Question Answering or Automatic Summarization. But what
        about evaluation? Do standard evaluation metrics like precision, recall,
        F-score or classiﬁcation error; and more important, evaluating an auto-
        matic system against a unique reference is enough to conclude how well
        a SBD system is performing given the ﬁnal application of the transcript?
        In this paper we propose Window-based Sentence Boundary Evaluation
        (WiSeBE), a semi-supervised metric for evaluating Sentence Boundary
        Detection systems based on multi-reference (dis)agreement. We evalu-
        ate and compare the performance of diﬀerent SBD systems over a set
        of Youtube transcripts using WiSeBE and standard metrics. This dou-
        ble evaluation gives an understanding of how WiSeBE is a more reliable
        metric for the SBD task.


        Keywords: Sentence Boundary Detection            · Evaluation
        Transcripts · Human judgment


1     Introduction
The goal of Automatic Speech Recognition (ASR) is to transform spoken data
into a written representation, thus enabling natural human-machine interaction
[33] with further Natural Language Processing (NLP) tasks. Machine transla-
tion, question answering, semantic parsing, POS tagging, sentiment analysis and
automatic text summarization; originally developed to work with formal writ-
ten texts, can be applied over the transcripts made by ASR systems [2,25,31].
However, before applying any of these NLP tasks a segmentation process called
Sentence Boundary Detection (SBD) should be performed over ASR transcripts
to reach a minimal syntactic information in the text.
    To measure the performance of a SBD system, the automatically segmented
transcript is evaluated against a single reference normally done by a human. But
c Springer Nature Switzerland AG 2018
I. Batyrshin et al. (Eds.): MICAI 2018, LNAI 11289, pp. 119–131, 2018.
https://doi.org/10.1007/978-3-030-04497-8_10120       C.-E. González-Gallardo and J.-M. Torres-Moreno

given a transcript, does it exist a unique reference? Or, is it possible that the
same transcript could be segmented in ﬁve diﬀerent ways by ﬁve diﬀerent people
in the same conditions? If so, which one is correct; and more important, how
to fairly evaluate the automatically segmented transcript? These questions are
the foundations of Window-based Sentence Boundary Evaluation (WiSeBE), a
new semi-supervised metric for evaluating SBD systems based on multi-reference
(dis)agreement.
    The rest of this article is organized as follows. In Sect. 2 we set the frame of
SBD and how it is normally evaluated. WiSeBE is formally described in Sect. 3,
followed by a multi-reference evaluation in Sect. 4. Further analysis of WiSeBE
and discussion over the method and alternative multi-reference evaluation is
presented in Sect. 5. Finally, Sect. 6 concludes the paper.


2      Sentence Boundary Detection
Sentence Boundary Detection (SBD) has been a major research topic science
ASR moved to more general domains as conversational speech [17,24,26]. Per-
formance of ASR systems has improved over the years with the inclusion and
combination of new Deep Neural Networks methods [5,9,33]. As a general rule,
the output of ASR systems lacks of any syntactic information such as capital-
ization and sentence boundaries, showing the interest of ASR systems to obtain
the correct sequence of words with almost no concern of the overall structure of
the document [8].
    Similar to SBD is the Punctuation Marks Disambiguation (PMD) or Sentence
Boundary Disambiguation. This task aims to segment a formal written text into
well formed sentences based on the existent punctuation marks [11,19,20,29]. In
this context a sentence is deﬁned (for English) by the Cambridge Dictionary1
as:

      “a group of words, usually containing a verb, that expresses a thought in
      the form of a statement, question, instruction, or exclamation and starts
                         with a capital letter when written”.

   PMD carries certain complications, some given the ambiguity of punctuation
marks within a sentence. A period can denote an acronym, an abbreviation, the
end of the sentence or a combination of them as in the following example:

         The U.S. president, Mr. Donald Trump, is meeting with the F.B.I.
              director Christopher A. Wray next Thursday at 8 p.m.

   However its diﬃculties, DPM proﬁts of morphological and lexical information
to achieve a correct sentence segmentation. By contrast, segmenting an ASR
transcript should be done without any (or almost any) lexical information and
a ﬂurry deﬁnition of sentence.
1
    https://dictionary.cambridge.org/.                      WiSeBE: Window-Based Sentence Boundary Evaluation            121

    The obvious division in spoken language may be considered speaker utter-
ances. However, in a normal conversation or even in a monologue, the way ideas
are organized diﬀers largely from written text. This diﬀerences, added to disﬂu-
encies like revisions, repetitions, restarts, interruptions and hesitations make the
deﬁnition of a sentence unclear thus complicating the segmentation task [27].
Table 1 exempliﬁes some of the diﬃculties that are present when working with
spoken language.

                   Table 1. Sentence Boundary Detection example

        Speech transcript                     SBD applied to transcript
        two two women can look out after      two // two women can look out
        a kid so bad as a man and a           after a kid so bad as a man and a
        woman can so you can have a you       woman can // so you can have a
        can have a mother and a father        // you can have a mother and a
        that that still don’t do right with   father that // that still don’t do
        the kid and you can have to men       right with the kid and you can
        that can so as long as the love       have to men that can // so as
        each other as long as they love       long as the love each other // as
        each other it doesn’t matter          long as they love each other it
                                              doesn’t matter//



    Stolcke and Shriberg [26] considered a set of linguistic structures as segments
including the following list:

–   Complete sentences
–   Stand-alone sentences
–   Disﬂuent sentences aborted in mid-utterance
–   Interjections
–   Back-channel responses.

    In [17], Meteer and Iyer divided speaker utterances into segments, consisting
each of a single independent clause. A segment was considered to begin either
at the beginning of an utterance, or after the end of the preceding segment. Any
dysﬂuency between the end of the previous segments and the begging of current
one was considered part of the current segments.
    Rott and Červa [23] aimed to summarize news delivered orally segmenting the
transcripts into “something that is similar to sentences”. They used a syntactic
analyzer to identify the phrases within the text.
    A wide study focused in unbalanced data for the SBD task was performed
by Liu et al. [15]. During this study they followed the segmentation scheme pro-
posed by the Linguistic Data Consortium2 on the Simple Metadata Annotation
Speciﬁcation V5.0 guideline (SimpleMDE V5.0) [27], dividing the transcripts in
Semantic Units.
2
    https://www.ldc.upenn.edu/.122    C.-E. González-Gallardo and J.-M. Torres-Moreno

    A Semantic Unit (SU) is considered to be an atomic element of the transcript
that manages to express a complete thought or idea on the part of the speaker
[27]. Sometimes a SU corresponds to the equivalent of a sentence in written text,
but other times (the most part of them) a SU corresponds to a phrase or a single
word.
    SUs seem to be an inclusive conception of a segment, they embrace diﬀerent
previous segment deﬁnitions and are ﬂexible enough to deal with the majority
of spoken language troubles. For these reasons we will adopt SUs as our segment
deﬁnition.


2.1   Sentence Boundary Evaluation

SBD research has been focused on two diﬀerent aspects; features and methods.
Regarding the features, some work focused on acoustic elements like pauses
duration, fundamental frequencies, energy, rate of speech, volume change and
speaker turn [10,12,14].
    The other kind of features used in SBD are textual or lexical features. They
rely on the transcript content to extract features like bag-of-word, POS tags or
word embeddings [7,12,16,18,23,26,30]. Mixture of acoustic and lexical features
have also been explored [1,13,14,32], which is advantageous when both audio
signal and transcript are available.
    With respect to the methods used for SBD, they mostly rely on statisti-
cal/neural machine translation [12,22], language models [8,15,18,26], conditional
random ﬁelds [16,30] and deep neural networks [3,7,29].
    Despite their diﬀerences in features and/or methodology, almost all previous
cited research share a common element; the evaluation methodology. Metrics as
Precision, Recall, F1-score, Classiﬁcation Error Rate and Slot Error Rate (SER)
are used to evaluate the proposed system against one reference. As discussed
in Sect. 1, further NLP tasks rely on the result of SBD, meaning that is crucial
to have a good segmentation. But comparing the output of a system against a
unique reference will provide a reliable score to decide if the system is good or
bad?
    Bohac et al. [1] compared the human ability to punctuate recognized spon-
taneous speech. They asked 10 people (correctors) to punctuate about 30 min of
ASR transcripts in Czech. For an average of 3,962 words, the punctuation marks
placed by correctors varied between 557 and 801; this means a diﬀerence of 244
segments for the same transcript. Over all correctors, the absolute consensus for
period (.) was only 4.6% caused by the replacement of other punctuation marks
as semicolons (;) and exclamation marks (!). These results are understandable if
we consider the diﬃculties presented previously in this section.
    To our knowledge, the amount of studies that have tried to target the sentence
boundary evaluation with a multi-reference approach is very small. In [1], Bohac
et al. evaluated the overall punctuation accuracy for Czech in a straightforward
multi-reference framework. They considered a period (.) valid if at least ﬁve of
their 10 correctors agreed on its position.                      WiSeBE: Window-Based Sentence Boundary Evaluation              123

    Kolář and Lamel [13] considered two independent references to evaluate their
system and proposed two approaches. The ﬁst one was to calculate the SER for
each of one the two available references and then compute their mean. They
found this approach to be very strict because for those boundaries where no
agreement between references existed, the system was going to be partially wrong
even the fact that it has correctly predicted the boundary. Their second app-
roach tried to moderate the number of unjust penalizations. For this case, a
classiﬁcation was considered incorrect only if it didn’t match either of the two
references.
    These two examples exemplify the real need and some straightforward solu-
tions for multi-reference evaluation metrics. However, we think that it is possible
to consider in a more inclusive approach the similarities and diﬀerences that mul-
tiple references could provide into a sentence boundary evaluation protocol.


3      Window-Based Sentence Boundary Evaluation

Window-Based Sentence Boundary Evaluation (WiSeBE) is a semi-automatic
multi-reference sentence boundary evaluation protocol which considers the per-
formance of a candidate segmentation over a set of segmentation references and
the agreement between those references.
    Let R = {R1 , R2 , ..., Rm } be the set of all available references given a tran-
script T = {t1 , t2 , ..., tn }, where tj is the j th word in the transcript; a reference
Ri is deﬁned as a binary vector in terms of the existent SU boundaries in T .

                                   Ri = {b1 , b2 , ..., bn }                         (1)
      where                        
                                       1 if tj is a boundary
                            bj =
                                       0 otherwise
Given a transcript T , the candidate segmentation CT is deﬁned similar to Ri .

                                 CT = {b1 , b2 , ..., bn }                           (2)
      where                        
                                       1 if tj is a boundary
                            bj =
                                       0 otherwise

3.1     General Reference and Agreement Ratio

A General Reference (RG ) is then constructed to calculate the agreement ratio
between all references in. It is deﬁned by the boundary frequencies of each ref-
erence Ri ∈ R.

                                 RG = {d1 , d2 , ..., dn }                           (3)
      where124    C.-E. González-Gallardo and J.-M. Torres-Moreno


                             
                             m
                      dj =         tij   ∀tj ∈ T,     dj = [0, m]             (4)
                             i=1

The Agreement Ratio (RGAR ) is needed to get a numerical value of the dis-
tribution of SU boundaries over R. A value of RGAR close to 0 means a low
agreement between references in R, while RGAR = 1 means a perfect agreement
(∀Ri ∈ R, Ri = Ri+1 |i = 1, ..., m − 1) in R.
                                                R GP B
                                    RGAR =                                    (5)
                                                RGHA
In the equation above, RGP B corresponds to the ponderated common boundaries
of RG and RGHA to its hypothetical maximum agreement.

                                          
                                          n
                             R GP B =           dj [dj ≥ 2]                   (6)
                                          j=1
                                             
                         RGHA = m ×                  1 [dj = 0]              (7)
                                            dj ∈RG



3.2   Window-Boundaries Reference

In Sect. 2 we discussed about how disﬂuencies complicate






 SU segmentation. In a
multi-reference environment this causes disagreement between references around
a same SU boundary. The way WiSeBE handle disagreements produced by dis-
ﬂuencies is with a Window-boundaries Reference (RW ) deﬁned as:

                              RW = {w1 , w2 , ..., wp }                       (8)
   where each window wk considers one or more boundaries dj from RG with a
window separation limit equal to RWl .

                             wk = {dj , dj+1 , dj+2 , ...}                    (9)

3.3   W iSeBE

WiSeBE is a normalized score dependent of (1) the performance of CT over RW
and (2) the agreement between all references in R. It is deﬁned as:

                W iSeBE = F 1RW × RGAR                W iSeBE = [0, 1]       (10)
   where F 1RW corresponds to the harmonic mean of precision and recall of CT
with respect to RW (Eq. 11), while RGAR is the agreement ratio deﬁned in (5).
RGAR can be interpreted as a scaling factor; a low value will penalize the overall
WiSeBE score given the low agreement between references. By contrast, for a
high agreement in R (RGAR ≈ 1), W iSeBE ≈ F 1RW .                    WiSeBE: Window-Based Sentence Boundary Evaluation           125


                                    precisionRW × recallRW
                     F 1RW = 2 ×                                               (11)
                                    precisionRW + recallRW
                             
                                 bj ∈CT   1 [bj = 1, bj ∈ w ∀w ∈ RW ]
            precisionRW =                                                     (12)
                                            bj ∈CT 1 [bj = 1]
                               
                                        1 [wk  b ∀b ∈ CT ]
                                   wk ∈RW
                  recallRW =                                              (13)
                                              p
Equations 12 and 13 describe precision and recall of CT with respect to RW .
Precision is the number of boundaries bj inside any window wk from RW divided
by the total number of boundaries bj in CT . Recall corresponds to the number
of windows w with at least one boundary b divided by the number of windows
w in RW .

4     Evaluating with W iSeBE
To exemplify the W iSeBE score we evaluated and compared the performance
of two diﬀerent SBD systems over a set of YouTube videos in a multi-reference
environment. The ﬁrst system (S1) employs a Convolutional Neural Network to
determine if the middle word of a sliding window corresponds to a SU bound-
ary or not [6]. The second approach (S2) by contrast, introduces a bidirectional
Recurrent Neural Network model with attention mechanism for boundary detec-
tion [28].
    In a ﬁrst glance we performed the evaluation of the systems against each
one of the references independently. Then, we implemented a multi-reference
evaluation with W iSeBE.

4.1   Dataset
We focused evaluation over a small but diversiﬁed dataset composed by 10
YouTube videos in the English language in the news context. The selected videos
cover diﬀerent topics like technology, human rights, terrorism and politics with
a length variation between 2 and 10 min. To encourage the diversity of content
format we included newscasts, interviews, reports and round tables.
    During the transcription phase we opted for a manual transcription process
because we observed that using transcripts from an ASR system will diﬃcult
in a large degree the manual segmentation process. The number of words per
transcript oscilate between 271 and 1,602 with a total number of 8,080.
    We gave clear instructions to three evaluators (ref1 , ref2 , ref3 ) of how seg-
mentation was needed to be perform, including the SU concept and how punctu-
ation marks were going to be taken into account. Periods (.), question marks (?),
exclamation marks (!) and semicolons (;) were considered SU delimiters (bound-
aries) while colons (:) and commas (,) were considered as internal SU marks.
The number of segments per transcript and reference can be seen in Table 2. An
interesting remark is that ref3 assigns about 43% less boundaries than the mean
of the other two references.126     C.-E. González-Gallardo and J.-M. Torres-Moreno

                      Table 2. Manual dataset segmentation

              Reference v1 v2 v3 v4 v5 v6 v7         v8 v9 v10 Total
              ref1       38 42 17 11 55 87 109 72 55 16           502
              ref2       33 42 16 14 54 98       92 65 51 20      485
              ref3       23 20 10    6 39 39     76 30 29     9   281



4.2   Evaluation
We ran both systems (S1 & S2) over the manually transcribed videos obtaining
the number of boundaries shown in Table 3. In general, it can be seen that S1
predicts 27% more segments than S2. This diﬀerence can aﬀect the performance
of S1, increasing its probabilities of false positives.

                     Table 3. Automatic dataset segmentation

              System v1 v2 v3 v4 v5 v6          v7   v8 v9 v10 Total
              S1       53 38 15 13 54 108 106 70 71 11            539
              S2       38 37 12 11 36      92    86 46 53 13      424



    Table 4 condenses the performance of both systems evaluated against each
one of the references independently. If we focus on F1 scores, performance of both
systems varies depending of the reference. For ref1 , S1 was better in 5 occasions
with respect of S2; S1 was better in 2 occasions only for ref2 ; S1 overperformed
S2 in 3 occasions concerning ref3 and in 4 occasions for mean (bold).
    Also from Table 4 we can observe that ref1 has a bigger similarity to S1 in
5 occasions compared to other two references, while ref2 is more similar to S2
in 7 transcripts (underline).
    After computing the mean F1 scores over the transcripts, it can be concluded
that in average S2 had a better performance segmenting the dataset compared
to S1, obtaining a F1 score equal to 0.510. But... What about the complexity of
the dataset? Regardless all references have been considered, nor agreement or
disagreement between them has been taken into account.
    All values related to the W iSeBE score are displayed in Table 5. The Agree-
ment Ratio (RGAR ) between references oscillates between 0.525 for v8 and 0.767
for v5 . The lower the RGAR , the bigger the penalization W iSeBE will give to
the ﬁnal score. A good example is S2 for transcript v4 where F 1RW reaches a
value of 0.800, but after considering RGAR the W iSeBE score falls to 0.462.
    It is feasible to think that if all references are taken into account at the same
time during evaluation (F 1RW ), the score will be bigger compared to an average
of independent evaluations (F 1mean ); however this is not always true. That is
the case of S1 in v10, which present a slight decrease for F 1RW compared to
F 1mean .                        WiSeBE: Window-Based Sentence B

                   Table 4. Independent multi-reference

Transcript   System ref1               ref2
                    P      R   F1      P      R   F1
v1           S1     0.396 0.553 0.462 0.377 0.606 0.465
             S2     0.474 0.474 0.474 0.474 0.545 0.507
v2           S1     0.605 0.548 0.575 0.711 0.643 0.675
             S2     0.595 0.524 0.557 0.676 0.595 0.633
v3           S1     0.333 0.294 0.313 0.267 0.250 0.258
             S2     0.417 0.294 0.345 0.417 0.313 0.357
v4           S1     0.615 0.571 0.593 0.462 0.545 0.500
             S2     0.909 0.714 0.800 0.818 0.818 0.818
v5           S1     0.630 0.618 0.624 0.593 0.593 0.593
             S2     0.667 0.436 0.527 0.611 0.407 0.489
v6           S1     0.491 0.541 0.515 0.454 0.563 0.503
             S2     0.500 0.469 0.484 0.522 0.552 0.536
v7           S1     0.594 0.578 0.586 0.462 0.533 0.495
             S2     0.663 0.523 0.585 0.558 0.522 0.539
v8           S1     0.443 0.477 0.459 0.514 0.500 0.507
             S2     0.609 0.431 0.505 0.652 0.417 0.508
v9           S1     0.437 0.564 0.492 0.451 0.627 0.525
             S2     0.623 0.600 0.611 0.585 0.608 0.596
v10          S1     0.818 0.450 0.581 0.818 0.450 0.581
             S2     0.692 0.450 0.545 0.615 0.500 0.552
Mean scores S1             —   0.520          —   0.510
             S2            —   0.543          —   0.554




oundary Evaluation                     127

 evaluation

   ref3               M ean
   P      R   F1      P       R   F1
 0.264 0.609 0.368 0.346 0.589 0.432
 0.368 0.6087 0.459 0.439 0.543 0.480
 0.368 0.700 0.483 0.561 0.630 0.578
 0.351 0.650 0.456 0.541 0.590 0.549
 0.200 0.300 0.240 0.267 0.281 0.270
 0.250 0.300 0.273 0.361 0.302 0.325
 0.308 0.667 0.421 0.462 0.595 0.505
 0.455 0.833 0.588 0.727 0.789 0.735
 0.481 0.667 0.560 0.568 0.626 0.592
 0.500 0.462 0.480 0.593 0.435 0.499
 0.213 0.590 0.313 0.386 0.565 0.443
 0.250 0.590 0.351 0.4234 0.537 0.457
 0.406 0.566 0.473 0.487 0.559 0.518
 0.465 0.526 0.494 0.562 0.524 0.539
 0.229 0.533 0.320 0.395 0.503 0.429
 0.370 0.567 0.447 0.543 0.471 0.487
 0.254 0.621 0.360 0.380 0.603 0.459
 0.321 0.586 0.414 0.509 0.598 0.541
 0.455 0.556 0.500 0.697 0.523 0.582
 0.308 0.444 0.364 0.538 0.4645 0.487
          —   0.404       —       0.481
          —   0.433       —       0.510




   An important remark is the behavior of S1 and S2 concerning v6 . If evalu-
ated without considering any (dis)agreement between references (F 1mean ), S2
overperforms S1; this is inverted once the systems are evaluated with W iSeBE.

5     Discussion
5.1   RG A R and Fleiss’ Kappa correlation
In Sect. 3 we described the W iSeBE score and how it relies on the RGAR value
to scale the performance of CT over RW . RGAR can intuitively be consider an
agreement value over all elements of R. To test this hypothesis, we computed
the Pearson correlation coeﬃcient (P CC) [21] between RGAR and the Fleiss’
Kappa [4] of each video in the dataset (κR ).
    A linear correlation between RGAR and κR can be observed in Table 6. This
is conﬁrmed by a P CC value equal to 0.890, which means a very strong positive
linear correlation between them.

5.2   F 1m ean vs. W iSeBE
Results form Table 5 may give an idea that W iSeBE is just an scaled F 1mean .
While it is true that they show a linear correlation, W iSeBE may produce a128        C.-E. González-G



                 Transcript
                 v1

                 v2

                 v3

                 v4

                 v5

                 v6

                 v7

                 v8

                 v9

                 v10

                 Mean scores




      Agreement metric v1
      RGAR             0.691
      κR               0.776




diﬀerent system ranking than
it follows. However, what we
allardo and J.-M. Torres-Moreno

    Table 5. W iSeBE evaluation

     System F 1mean F 1RW RGAR W iSeBE
     S1        0.432   0.495        0.691   0.342
     S2        0.480   0.513                0.354
     S1        0.578   0.659        0.688   0.453
     S2        0.549   0.595                0.409
     S1        0.270   0.303        0.684   0.207
     S2        0.325   0.400                0.274
     S1        0.505   0.593        0.578   0.342
     S2        0.735   0.800                0.462
     S1        0.592   0.614        0.767   0.471
     S2        0.499   0.500                0.383
     S1        0.443   0.550        0.541   0.298
     S2        0.457   0.535                0.289
     S1        0.518   0.592        0.617   0.366
     S2        0.539   0.606                0.374
     S1        0.429   0.494        0.525   0.259
     S2        0.487   0.508                0.267
     S1        0.459   0.569        0.604   0.344
     S2        0.541   0.667                0.403
     S1        0.582   0.581        0.619   0.359
     S2        0.487   0.545                0.338
 S1            0.481   0.545        0.631   0.344
 S2            0.510   0.567                0.355

  Table 6. Agreement within dataset

    v2    v3     v4    v5      v6      v7     v8    v9   v10
 0.688 0.684 0.578 0.767 0.541 0.617 0.525 0.604 0.619
 0.697 0.757 0.696 0.839 0.630 0.743 0.655 0.704 0.718




 F 1mean given the integral multi-reference principle
 consider the most proﬁtable about W iSeBE is the
twofold inclusion of all available references it performs. First, the construction of
RW to provide a more inclusive reference against to whom be evaluated and then,
the computation of RGAR , which scales the result depending of the agreement
between references.                     WiSeBE: Window-Based Sentence Boundary Evaluation               129

6    Conclusions
In this paper we presented WiSeBE, a semi-automatic multi-reference sentence
boundary evaluation protocol based on the necessity of having a more reliable
way for evaluating the SBD task. We showed how W iSeBE is an inclusive metric
which not only evaluates the performance of a system against all references, but
also takes into account the agreement between them. According to your point
of view, this inclusivity is very important given the diﬃculties that are present
when working with spoken language and the possible disagreements that a task
like SBD could provoke.
    W iSeBE shows to be correlated with standard SBD metrics, however we
want to measure its correlation with extrinsic evaluations techniques like auto-
matic summarization and machine translation.

Acknowledgments. We would like to acknowledge the support of CHIST-ERA for
funding this work through the Access Multilingual Information opinionS (AMIS),
(France - Europe) project.
    We also like to acknowledge the support given by the Prof. Hanifa Boucheneb from
VERIFORM Laboratory (École Polytechnique de Montréal).


References
 1. Bohac, M., Blavka, K., Kucharova, M., Skodova, S.: Post-processing of the recog-
    nized speech for web presentation of large audio archive. In: 2012 35th International
    Conference on Telecommunications and Signal Processing (TSP), pp. 441–445.
    IEEE (2012)
 2. Brum, H., Araujo, F., Kepler, F.: Sentiment analysis for Brazilian portuguese over
    a skewed class corpora. In: Silva, J., Ribeiro, R., Quaresma, P., Adami, A., Branco,
    A. (eds.) PROPOR 2016. LNCS (LNAI), vol. 9727, pp. 134–138. Springer, Cham
    (2016). https://doi.org/10.1007/978-3-319-41552-9 14
 3. Che, X., Wang, C., Yang, H., Meinel, C.: Punctuation prediction for unsegmented
    transcript based on word vector. In: LREC (2016)
 4. Fleiss, J.L.: Measuring nominal scale agreement among many raters. Psychol. Bull.
    76(5), 378 (1971)
 5. Fohr, D., Mella, O., Illina, I.: New paradigm in speech recognition: deep neural net-
    works. In: IEEE International Conference on Information Systems and Economic
    Intelligence (2017)
 6. González-Gallardo, C.E., Hajjem, M., SanJuan, E., Torres-Moreno, J.M.: Tran-
    scripts informativeness study: an approach based on automatic summarization. In:
    Conférence en Recherche d’Information et Applications (CORIA), Rennes, France,
    May (2018)
 7. González-Gallardo, C.E., Torres-Moreno, J.M.: Sentence boundary detection for
    French with subword-level information vectors and convolutional neural networks.
    arXiv preprint arXiv:1802.04559 (2018)
 8. Gotoh, Y., Renals, S.: Sentence boundary detection in broadcast speech transcripts.
    In: ASR2000-Automatic Speech Recognition: Challenges for the new Millenium
    ISCA Tutorial and Research Workshop (ITRW) (2000)130     C.-E. González-Gallardo and J.-M. Torres-Moreno

 9. Hinton, G., et al.: Deep neural networks for acoustic modeling in speech recogni-
    tion: the shared views of four research groups. IEEE Signal Process. Mag. 29(6),
    82–97 (2012)
10. Jamil, N., Ramli, M.I., Seman, N.: Sentence boundary detection without speech
    recognition: a case of an under-resourced language. J. Electr. Syst. 11(3), 308–318
    (2015)
11. Kiss, T., Strunk, J.: Unsupervised multilingual sentence boundary detection. Com-
    put. Linguist. 32(4), 485–525 (2006)
12. Klejch, O., Bell, P., Renals, S.: Punctuated transcription of multi-genre broadcasts
    using acoustic and lexical approaches. In: 2016 IEEE Spoken Language Technology
    Workshop (SLT), pp. 433–440. IEEE (2016)
13. Kolář, J., Lamel, L.: Development and evaluation of automatic punctuation for
    French and english speech-to-text. In: Thirteenth Annual Conference of the Inter-
    national Speech Communication Association (2012)
14. Kolář, J., Švec, J., Psutka, J.: Automatic punctuation annotation in Czech broad-
    cast news speech. In: SPECOM 2004 (2004)
15. Liu, Y., Chawla, N.V., Harper, M.P., Shriberg, E., Stolcke, A.: A study in machine
    learning from imbalanced data for sentence boundary detection in speech. Comput.
    Speech Lang. 20(4), 468–494 (2006)
16. Lu, W., Ng, H.T.: Better punctuation prediction with dynamic conditional ran-
    dom ﬁelds. In: Proceedings of the 2010 Conference on Empirical Methods in Natu-
    ral Language Processing. pp. 177–186. Association for Computational Linguistics
    (2010)
17. Meteer, M., Iyer, R.: Modeling conversational speech for speech recognition. In:
    Conference on Empirical Methods in Natural Language Processing (1996)
18. Mrozinski, J., Whittaker, E.W., Chatain, P., Furui, S.: Automatic sentence seg-
    mentation of speech for automatic summarization. In: 2006 IEEE International
    Conference on Acoustics Speech and Signal Processing Proceedings, vol. 1, p. I.
    IEEE (2006)
19. Palmer, D.D., Hearst, M.A.: Adaptive sentence boundary disambiguation. In: Pro-
    ceedings of the Fourth Conference on Applied Natural Language Processing, pp.
    78–83. ANLC 1994. Association for Computational Linguistics, Stroudsburg, PA,
    USA (1994)
20. Palmer, D.D., Hearst, M.A.: Adaptive multilingual sentence boundary disambigua-
    tion. Comput. Linguist. 23(2), 241–267 (1997)
21. Pearson, K.: Note on regression and inheritance in the case of two parents. Proc.
    R. Soc. Lond. 58, 240–242 (1895)
22. Peitz, S., Freitag, M., Ney, H.: Better punctuation prediction with hierarchical
    phrase-based translation. In: Proceedings of the International Workshop on Spoken
    Language Translation (IWSLT), South Lake Tahoe, CA, USA (2014)
23. Rott, M., Červa, P.: Speech-to-text summarization using automatic phrase extrac-
    tion from recognized text. In: Sojka, P., Horák, A., Kopeček, I., Pala, K. (eds.) TSD
    2016. LNCS (LNAI), vol. 9924, pp. 101–108. Springer, Cham (2016). https://doi.
    org/10.1007/978-3-319-45510-5 12
24. Shriberg, E., Stolcke, A.: Word predictability after hesitations: a corpus-based
    study. In: Proceedings of the Fourth International Conference on Spoken Language,
    1996. ICSLP 1996, vol. 3, pp. 1868–1871. IEEE (1996)
25. Stevenson, M., Gaizauskas, R.: Experiments on sentence boundary detection. In:
    Proceedings of the sixth conference on Applied natural language processing, pp.
    84–89. Association for Computational Linguistics (2000)                     WiSeBE: Window-Based Sentence Boundary Evaluation              131

26. Stolcke, A., Shriberg, E.: Automatic linguistic segmentation of conversational
    speech. In: Proceedings of the Fourth International Conference on Spoken Lan-
    guage, 1996. ICSLP 1996, vol. 2, pp. 1005–1008. IEEE (1996)
27. Strassel, S.: Simple metadata annotation speciﬁcation v5. 0, linguistic data consor-
    tium (2003). http://www.ldc.upenn.edu/projects/MDE/Guidelines/SimpleMDE
    V5.0.pdf
28. Tilk, O., Alumäe, T.: Bidirectional recurrent neural network with attention mech-
    anism for punctuation restoration. In: Interspeech 2016 (2016)
29. Treviso, M.V., Shulby, C.D., Aluisio, S.M.: Evaluating word embeddings for sen-
    tence boundary detection in speech transcripts. arXiv preprint arXiv:1708.04704
    (2017)
30. Ueﬃng, N., Bisani, M., Vozila, P.: Improved models for automatic punctuation
    prediction for spoken and written text. In: Interspeech, pp. 3097–3101 (2013)
31. Wang, W., Tur, G., Zheng, J., Ayan, N.F.: Automatic disﬂuency removal for
    improving spoken language translation. In: 2010 IEEE International Conference on
    Acoustics Speech and Signal Processing (ICASSP), pp. 5214–5217. IEEE (2010)
32. Xu, C., Xie, L., Huang, G., Xiao, X., Chng, E.S., Li, H.: A deep neural network
    approach for sentence boundary detection in broadcast news. In: Fifteenth Annual
    Conference of the International Speech Communication Association (2014)
33. Yu, D., Deng, L.: Automatic Speech Recognition. Springer, London (2015). https://
    doi.org/10.1007/978-1-4471-5779-3