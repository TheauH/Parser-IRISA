A word embedding approach to explore a collection
of discussions of people in psychological distress

1st R´emy Kessler

Universit´e Bretagne Sud

2nd Nicolas B´echet
Universit´e Bretagne Sud

CNRS 6074A

CNRS 6074A

56017 Vannes,France

remy.kessler@univ-ubs.fr

56017 Vannes,France
nicolas.bechet@irisa.fr

3rd Gudrun Ledegen
Universit´e Rennes II
PREFics, EA 4246
5043 Rennes, France

4rd Frederic Pugni`ere-Saavedra

Universit´e Bretagne Sud

PREFics, EA 4246
56017 Vannes, France

gudrun.ledegen@univ-rennes2.fr

frederic.pugniere-saavedra@univ-ubs.fr

Abstract—In order to better adapt to society, an association
has developed a web chat application that allows anyone to
express and share their concerns and anguishes. Several thousand
anonymous conversations have been gathered and form a new
corpus of stories about human distress and social violence. We
present a method of corpus analysis combining unsupervised
learning and word embedding in order to bring out the themes
of this particular collection. We compare this approach with a
standard algorithm of the literature on a labeled corpus and
obtain very good results. An interpretation of the obtained
clusters collection conﬁrms the interest of the method.

Keywords—word2vec, unsupervised learning, word embedding.

I. INTRODUCTION

Since the nineties, social suffering has been a theme that has
received much attention from public and associative action.
Among the consequences, there is an explosion of listening
places or socio-technical devices of communication whose
objectives consist in moderating the various forms of suffering
by the liberation of the speech for a therapeutic purpose [1]
[2]. As part of the METICS project, a suicide prevention
association developed an application of web chat
to meet
this need. The web chat is an area that allows anyone to
express and share with a volunteer listener their concerns and
anguishes. The main speciﬁcity of this device is its anonymous
nature. Protected by a pseudonym, the writers are invited
to discuss with a volunteer the problematic aspects of their
existence. Several
thousand anonymous conversations have
been gathered and form a corpus of unpublished stories about
human distress. The purpose of the METICS project is to make
visible the ordinary forms of suffering usually removed from
common spaces and to grasp both its modes of enunciation and
digital support. In this study, we want to automatically identify
the reason for coming on the web chat for each participant.
Indeed, even if the association provided us with the theme
of all the conversations (work, loneliness, violence, racism,
addictions, family, etc.),
the original reason has not been
preserved. In what follows, we ﬁrst review some of the related

work in Section II. Section III presents the resources used and
gives some statistics about the collection. An overview of the
system and the strategy for identify the reason for coming
on the web chat is given in Section IV. Section V presents
the experimental protocol, an evaluation of our system and an
interpretation of the ﬁnal results on the collection of human
distress.

II. RELATED WORKS

The main characteristic of the approach presented in this
paper is to only have to provide the labels of the classes to
be predicted. This method does not need to have a tagged
data set to predict the different classes, so it is closer to an
unsupervised (clustering) or semi-supervised learning method
than a supervised. The main idea of clustering is to group
untagged data into a number of clusters, such that similar ex-
amples are grouped together and different ones are separated.
In clustering, the number of classes and the distribution of
instances between classes are unknown and the goal is to ﬁnd
meaningful clusters.

One kind of clustering methods is the partitioning-based
one. The k-means algorithm [3] is one of the most popu-
lar partitioning-based algorithms because it provides a good
compromise between the quality of the solution obtained and
its computational complexity [4]. K-means aims to ﬁnd k
centroids, one for each cluster, minimizing the sum of the
distances of each instance of data from its respective centroid.
We can cite other partitioning-based algorithms such as k-
medoids or PAM (Partition Around Medoids), which is an
evolution of k-means [5]. Hierarchical approaches produce
clusters by recursively partitioning data backwards or upwards.
For example,
in a hierarchical ascending classiﬁcation or
CAH [6], each example from the initial dataset represents a
cluster. Then, the clusters are merged, according to a similarity
measure, until the desired tree structure is obtained. The result
of this clustering method is called a dendrogram. Density-
based methods like the EM algorithm [7] assume that the data
belonging to each cluster is derived from a speciﬁc probability

distribution [8]. The idea is to grow a cluster as the density in
the neighborhood of the cluster exceeds a predeﬁned threshold.
Model-based classiﬁcation methods like self-organizing
map - SOM [9] are focus on ﬁnding features to represent each
cluster. The most used methods are decision trees and neural
networks. Approaches based on semi-supervised learning such
as label propagation algorithm [10] are similar to the method
proposed in this paper because they consist in using a learning
dataset consisting of a few labelled data points to build a
model for labelling a larger number of unlabelled data. Closer
to the theme of our collection, [11] and [12] use supervised
approaches to automatically detect suicidal people in social
networks. They extract speciﬁc features like word distribution
statistics or sentiments to train different machine-learning clas-
siﬁers and compare performance of machine-learning models
against the judgments of psychiatric trainees and mental health
professionals. More recently, CLEF challenge in 2018 consists
of performing a task on early risk detection of depression on
texts written in Social Media1. However, these papers and this
task involve tagged data sets, which is the main difference
with our proposed approach (we do not have tagged data set).

III. RESOURCES AND STATISTICS

The association provided a collection of conversations be-
tween volunteers and callers between 2005 and 2015, which
is called “METICS collection” henceforth.

To reduce noise in the collection, we removed all

the
discussions containing fewer than 15 exchanges between a
caller and a person from the association, these exchanges are
generally unrepresentative (connection problem, request for
information, etc.). We observe particular linguistic phenomena
like emoticons2, acronyms, mistakes (spelling,
typography,
glued words) and an explosive lexical creativity [13]. These
phenomena have their origin in the mode of communication
(direct or semi-direct), the speed of the composition of the
message or in the technological constraints of input imposed
by the material (mobile terminal, tablet, etc.). In addition, we
used a subset of the collection of the French newspaper, Le
Monde to validate our method on a tagged corpus. We only
keep articles on television, politics, art, science or economics.
Figure 1 presents some descriptive statistics of these two
collections.

IV. METHODOLOGY

A. System Overview

Figure 2 presents an overview of the system, each step will
be detailed in the rest of the section. In the ﬁrst step (mod-
ule (cid:172)), we apply different linguistic pre-processing to each
discussion. The next module ((cid:173)) creates a word embedding
model with these discussions while the third module ((cid:174)) uses
this model to create speciﬁc vectors. The last module ((cid:175))
performs a prediction for each discussion before separating
the collection into clusters based on the predicted class.

1http://early.irlab.org/
2Symbols used in written messages to express emotions, e.g. smile or

sadness

Collection
Total number of documents

Without pre-processing

Total number of words
Total number of different words
Average words/document

With pre-processing

Total number of words
Total number of different words
Average words/document

METICS
17 594

Le-Monde
205 661

12 276 973
158 361
698

87 122 002
419 579
424

4 529 793
120 684
257

41 425 938
419 006
201

Fig. 1. Statistics of both collections.

Fig. 2. System overview

B. Normalization and pre-processing

We ﬁrst extract the textual content of each discussion. In
step (cid:172), a text normalization is performed to improve the
quality of the process. We remove accents, special characters
such as “-”,“/” or “()”. Different linguistic processes are used
to reduce noise in the model: we remove numbers (numeric
and/or textual), special symbols and terms contained in a stop-
list adapted to our problem. A lemmatization process was
incorporated during the ﬁrst experiments but it was inefﬁcient
considering the typographical variations described in Section
III.

C. word2vec model

In the next step we build a word embedding model using
word2vec [14]. We project each word of our corpus in a vector
space in order to obtain a semantic representation of these.
In this way, words appearing in similar contexts will have a
relatively close vector representation. In addition to semantic
information, one advantage of such modeling is the production
of vector representations of words, depending on the context
in which they are encountered. Some words close to a term t in
a model learned from a corpus c1 may be very different from
those from a model learned from a corpus c2. For example,
we observe in ﬁgure 3 that the ﬁrst eight words close to the
term “teen” vary according to the corpus used. This example
also shows that the use of a generic model like Le Monde in
French or Wikipedia is irrelevant in our case, since the corpus

  Conversationsor documentsWord2vecmodelPredictionClass 1pre-processingSpecific vectors creationModel 1①➁➂④Class 2Model 2Class nModel nCluster 1Cluster 2Cluster ncorpus
METICS

Le-Monde

words
teenager, young, 15years, kid,
school, problem , spoiled, teen,
sitcom, radio, compote, hearing
boy, styx, scamp, rebel

Fig. 3. Eight words closest to the term “teenager” according to the type of
corpus in learning.

of the association is noisy and contains a number of apocopes,
abbreviations or acronyms. Different parameters were tested
and the conﬁguration with the best results was kept3.

D. Speciﬁc vectors creation and cluster predictions

the k-means algorithm [3], commonly used in the literature,
as mentioned in Section II. To feed the k-means algorithm, we
transformed our initial collection into a bag of words matrix
[15] where each conversation is described by the frequency
of its words. Each of the experiments was evaluated using the
classic measures of Precision, Recall and F-measure, averaged
over all classes (with beta = 1 in order not to privilege
precision or recall [16]). Since the k-means algorithm does not
associate a tag with the ﬁnal clusters, we have exhaustively
calculated the set of solutions to keep only the one yielding
the highest F-score.

B. Results

n(cid:88)

In this step, we build vectors containing terms that are
selected using the word2vec model described in step IV-C.
For each theme in the collection, we build a speciﬁc linguistic
model by performing a word embedding to reconstruct the
linguistic context of each theme. We observe, for example,
that the terms closest to the thematic “work” are: “unemploy-
ment”, “job”, “stress”. Similarly, for the “addiction” theme,
we observe the terms: “cannabis”, “alcoholism”, “drugs” and
“heroin”. We used this context subsequently to construct a
vector, containing the distance distc(i) between each term i
and the theme c. Each of these models is independent, so
the same term can appear in several models. In this way,
we observed that the word “stress” is present in the vector
“suicide” and in that of “work”, however, the associated weight
is different. We varied the size of these vectors between 20
and 1000 and the best results were obtained with a size of 400.
In the last step (cid:175), the system computes an Sc score for each
discussion and for each cluster according to each linguistic
model such as:

Sc(d) =

tf (i) · distc(i)

(1)

i=1

with i the considered term, tf (i) frequency of i in the
collection, and distc(i) is the distance between the term i and
the thematic c. In the end, the class with the highest score is
chosen.

V. EXPERIMENTS AND RESULTS

A. Experimental protocol

To evaluate the quality of the obtained clusters, we used a
subset of the texts of the Le-Monde newspaper, described in
Section III, each article having a label according to the theme.
For these experiments, we conﬁgured the speciﬁc vectors (SV)
approach with the optimal parameters, as deﬁned in Sections
IV-C and IV-D. We also tested the speciﬁc vectors without
weighting to test the particular inﬂuence of this parameter. To
highlight the difﬁculty of the task, we compare our system
with a baseline which consists in a random draw, and with

3The best results were obtained with the following parameter values: vector
size: 700, sliding window size: 5, minimum frequency: 10, vectorization
method: skip grams, and a soft hierarchical max function for the model
learning.

Prec. Recall

F-score

Without pre-processing

Baseline
k-means
Without weighting
Speciﬁc Vectors

0.18
0.23
0.54
0.53

With pre-processing

k-means
Without weighting
Speciﬁc Vectors

0.30
0.55
0.54

0.16
0.20
0.50
0.54

0.21
0.51
0.54

0.17
0.22
0.52
0.53

0.25
0.53
0.54

Fig. 4. Results obtained by each system.

Figure 4 presents a summary of the results obtained with
each systems. We ﬁrst observe that baseline scores are very
low, but remain relatively close to the theoretical random (0.2)
given by the number of classes. Linguistic pre-treatments are
not very efﬁcient individually, but improve overall the results
of other experiments. The k-means algorithm obtains slightly
better results in terms of F-score, but remains weak. Speciﬁc
vectors get excellent results that outperform other systems with
an F-score of 0.54. The execution without weighting improve
slightly the recall.

C. Cluster Analysis

Initial objective of this work was the exploration of the
METICS collection, we apply the whole process with the
speciﬁc vectors approach to automatically categorize all the
conversations. We use the Latent Dirichlet Allocation [17] in
order to obtain the main topic of each cluster. Figure 5 presents
average weight of each thematic keywords according to each
clusters.

In ﬁgure 5, fear, shrink and trust are present designations
for each cluster with a largely signiﬁcant rank; yet, does
the writer still express fear when he writes, ”I’m afraid of
being sick”? Do these designations not participate in opening
and constructing spheres of meanings around these pivotal
words? Conversely, with a lower rank, but also signiﬁcant, the
designations of thing, difﬁcult, and problem are more vague,
but more reformulating to take up the elements involved in
writing what is wrong.

Fig. 5. Distribution of discursive routines by cluster.

[7] A. P. Dempster, N. M. Laird, and D. B. Rubin, “Maximum likelihood
from incomplete data via the em algorithm,” in Journal of the royal
society, series B, 1977, pp. 1–38.

[8] J. D. Banﬁeld and A. E. Raftery, “Model-based gaussian and non-

gaussian clustering,” in Biometrics, vol. 49, 1993, pp. 803–821.

[9] T. Kohonen, “Self-organized formation of topologically correct feature

maps,” Biological Cybernetics, pp. 59–69, Jan 1982.

[10] U. N. Raghavan, R. Albert, and S. Kumara, “Near linear time algorithm
to detect community structures in large-scale networks.” Physical review.
E, Statistical, nonlinear, and soft matter physics, p. 036106, 2007.

[11] J. P. Pestian, P. Matykiewicz, M. Linn-Gust, B. South, O. Uzuner,
J. Wiebe, K. B. Cohen, J. Hurdle, and C. Brew, “Sentiment analysis
of suicide notes: A shared task,” Biomedical Informatics Insights, pp.
3–16, 2012.

[12] A. Abboute, Y. Boudjeriou, G. Entringer, J. Az´e, S. Bringay, and
P. Poncelet, “Mining twitter for suicide prevention,” in Natural Language
Processing and Information Systems: 19th International Conference on
Applications of Natural Language to Information Systems, NLDB 2014,
Montpellier, France, June 18-20, 2014. Proceedings.
Springer, 2014,
pp. 250–253.

[13] R. Kessler, J.-M. Torres, and M. El-B`eze, “Classiﬁcation th´ematique de
courriel par des m´ethodes hybrides,” Journ´ee ATALA sur les nouvelles
formes de communication ´ecrite, 2004.

[14] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, “Distributed
representations of words and phrases and their compositionality,” in
Proceedings of NIPS’13. USA: Curran Associates
Inc., 2013,
pp. 3111–3119. [Online]. Available: http://dl.acm.org/citation.cfm?id=
2999792.2999959

[15] C. D. Manning and H. Sch¨utze, Foundations of Statistical Natural

Language Processing. Cambridge, MA, USA: MIT Press, 1999.

[16] C. Goutte and E. Gaussier, “ A Probabilistic Interpretation of Precision,
Recall and F-Score, with Implication for Evaluation,” ECIR 2005, pp.
345–359, 2005.

[17] M. Hoffman, F. R. Bach, and D. M. Blei, “Online learning for latent
dirichlet allocation,” in Advances in Neural Information Processing
Systems, J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel,
and A. Culotta, Eds.

23, 2010, pp. 856–864.

VI. CONCLUSION AND FUTURE WORK

In this article, we presented an unsupervised approach to
exploring a collection of stories about human distress. This
approach uses a word embedding model
to build vectors
containing only vocabulary from the linguistic context of the
model. We evaluated the quality of the approach on a col-
lection labeled with classical measures. The detailed analysis
showed very good results (average Fscore of 0.54) compared
to the other systems tested. This method of analysis has also
made it possible to highlight semantic universes and thematic
groupings. We ﬁrst intend to study in more detail the inﬂuence
of each of the parameters on the results obtained. We are also
planning to be able to assign several tags to each discussion,
which would allow thematic overlaps to be taken into account.
The analysis reinforces the cluster approach to highlight the
deﬁning features of this type of speech production and to
reveal its inner workings. This entry by the discursive routines
is only one example which will then make it possible to
approach other explorations with a particular focus on the
argumentative forms and on the forms of intensity.

REFERENCES

[1] D. Fassin, “Et la souffrance devint sociale,” in Critique. 680(1), 2004,

pp. 16–29.

2006, pp. 137–157.

[2] ——, “Souffrir par le social, gouverner par l’´ecoute,” in Politix. 73(1),

[3] MacQueen, J., “Some methods for classiﬁcation and analysis of multi-
variate observations,” in Proceedings of the Fifth Berkeley Symposium
on Mathematical Statistics and Probability, Vol. 1: Statistics. USA:
University of California Press, 1967, pp. 281–297.

[4] D. Arthur and S. Vassilvitskii, “K-means++: The advantages of careful
seeding,” Proceedings of the Eighteenth Annual ACM-SIAM Symposium
on Discrete Algorithms, pp. 1027–1035, 2007.

[5] L. Kaufman and P. Rousseeuw, Clustering by Means of Medoids.
Delft University of Technology :
the Faculty of
Technical Mathematics and Informatics, 1987. [Online]. Available:
https://books.google.fr/books?id=HK-4GwAACAAJ

reports of

[6] G. N. Lance and W. T. Williams, “A general theory of classiﬁcatory
sorting strategies1. hierarchical systems,” The Computer Journal 4, pp.
373–380, 1967.

