A word embedding approach to explore a collection
 of discussions of people in psychological distress

    1st Rémy Kessler         2nd Nicolas Béchet            3rd Gudrun Ledegen              4rd Frederic Pugnière-Saavedra
Université Bretagne Sud Université Bretagne Sud     Université Rennes II            Université Bretagne Sud
      CNRS 6074A              CNRS 6074A               PREFics, EA 4246                    PREFics, EA 4246
  56017 Vannes,France     56017 Vannes,France         5043 Rennes, France                56017 Vannes, France
remy.kessler@univ-ubs.fr nicolas.bechet@irisa.fr gudrun.ledegen@univ-rennes2.fr frederic.pugniere-saavedra@univ-ubs.fr




   Abstract—In order to better adapt to society, an association     work in Section II. Section III presents the resources used and
has developed a web chat application that allows anyone to          gives some statistics about the collection. An overview of the
express and share their concerns and anguishes. Several thousand    system and the strategy for identify the reason for coming
anonymous conversations have been gathered and form a new
corpus of stories about human distress and social violence. We      on the web chat is given in Section IV. Section V presents
present a method of corpus analysis combining unsupervised          the experimental protocol, an evaluation of our system and an
learning and word embedding in order to bring out the themes        interpretation of the final results on the collection of human
of this particular collection. We compare this approach with a      distress.
standard algorithm of the literature on a labeled corpus and
obtain very good results. An interpretation of the obtained
clusters collection confirms the interest of the method.                                II. R ELATED WORKS
   Keywords—word2vec, unsupervised learning, word embedding.
                                                                       The main characteristic of the approach presented in this
                                                                    paper is to only have to provide the labels of the classes to
                      I. I NTRODUCTION                              be predicted. This method does not need to have a tagged
   Since the nineties, social suffering has been a theme that has   data set to predict the different classes, so it is closer to an
received much attention from public and associative action.         unsupervised (clustering) or semi-supervised learning method
Among the consequences, there is an explosion of listening          than a supervised. The main idea of clustering is to group
places or socio-technical devices of communication whose            untagged data into a number of clusters, such that similar ex-
objectives consist in moderating the various forms of suffering     amples are grouped together and different ones are separated.
by the liberation of the speech for a therapeutic purpose [1]       In clustering, the number of classes and the distribution of
[2]. As part of the METICS project, a suicide prevention            instances between classes are unknown and the goal is to find
association developed an application of web chat to meet            meaningful clusters.
this need. The web chat is an area that allows anyone to               One kind of clustering methods is the partitioning-based
express and share with a volunteer listener their concerns and      one. The k-means algorithm [3] is one of the most popu-
anguishes. The main specificity of this device is its anonymous     lar partitioning-based algorithms because it provides a good
nature. Protected by a pseudonym, the writers are invited           compromise between the quality of the solution obtained and
to discuss with a volunteer the problematic aspects of their        its computational complexity [4]. K-means aims to find k
existence. Several thousand anonymous conversations have            centroids, one for each cluster, minimizing the sum of the
been gathered and form a corpus of unpublished stories about        distances of each instance of data from its respective centroid.
human distress. The purpose of the METICS project is to make        We can cite other partitioning-based algorithms such as k-
visible the ordinary forms of suffering usually removed from        medoids or PAM (Partition Around Medoids), which is an
common spaces and to grasp both its modes of enunciation and        evolution of k-means [5]. Hierarchical approaches produce
digital support. In this study, we want to automatically identify   clusters by recursively partitioning data backwards or upwards.
the reason for coming on the web chat for each participant.         For example, in a hierarchical ascending classification or
Indeed, even if the association provided us with the theme          CAH [6], each example from the initial dataset represents a
of all the conversations (work, loneliness, violence, racism,       cluster. Then, the clusters are merged, according to a similarity
addictions, family, etc.), the original reason has not been         measure, until the desired tree structure is obtained. The result
preserved. In what follows, we first review some of the related     of this clustering method is called a dendrogram. Density-
                                                                    based methods like the EM algorithm [7] assume that the data
                                                                    belonging to each cluster is derived from a specific probability
distribution [8]. The idea is to grow a cluster as the density in            Collection                          METICS                                   Le-Monde
the neighborhood of the cluster exceeds a predefined threshold.              Total number of documents             17 594                                   205 661
                                                                                        Without pre-processing
   Model-based classification methods like self-organizing
                                                                             Total number of words             12 276 973                                87 122 002
map - SOM [9] are focus on finding features to represent each                Total number of different words      158 361                                   419 579
cluster. The most used methods are decision trees and neural                 Average words/document                   698                                       424
networks. Approaches based on semi-supervised learning such                              With pre-processing
as label propagation algorithm [10] are similar to the method                Total number of words              4 529 793                                41 425 938
proposed in this paper because they consist in using a learning              Total number of different words      120 684                                   419 006
                                                                             Average words/document                   257                                       201
dataset consisting of a few labelled data points to build a
model for labelling a larger number of unlabelled data. Closer
to the theme of our collection, [11] and [12] use supervised                                  Fig. 1. Statistics of both collections.
approaches to automatically detect suicidal people in social
networks. They extract specific features like word distribution
                                                                                                      Word2vec          Specific
statistics or sentiments to train different machine-learning clas-                                     model            vectors
                                                                                                                        creation
sifiers and compare performance of machine-learning models                                                       ➁                  ➂
against the judgments of psychiatric trainees and mental health                                                                    Class 1     Class 2           Class n

professionals. More recently, CLEF challenge in 2018 consists                                                                    Model 1 Model 2                     Model n

of performing a task on early risk detection of depression on
texts written in Social Media1 . However, these papers and this                               pre-processing           Prediction
                                                                                                           ①                        ④
task involve tagged data sets, which is the main difference                   Conversations
                                                                              or documents
with our proposed approach (we do not have tagged data set).
                  III. R ESOURCES AND STATISTICS
   The association provided a collection of conversations be-                                                        Cluster 1     Cluster 2             Cluster n

tween volunteers and callers between 2005 and 2015, which
is called “METICS collection” henceforth.
   To reduce noise in the collection, we removed all the                                              Fig. 2. System overview
discussions containing fewer than 15 exchanges between a
caller and a person from the association, these exchanges are
generally unrepresentative (connection problem, request for               B. Normalization and pre-processing
information, etc.). We observe particular linguistic phenomena
                                                                             We first extract the textual content of each discussion. In
like emoticons2 , acronyms, mistakes (spelling, typography,
                                                                          step ¬, a text normalization is performed to improve the
glued words) and an explosive lexical creativity [13]. These
                                                                          quality of the process. We remove accents, special characters
phenomena have their origin in the mode of communication
                                                                          such as “-”,“/” or “()”. Different linguistic processes are used
(direct or semi-direct), the speed of the composition of the
                                                                          to reduce noise in the model: we remove numbers (numeric
message or in the technological constraints of input imposed
                                                                          and/or textual), special symbols and terms contained in a stop-
by the material (mobile terminal, tablet, etc.). In addition, we
                                                                          list adapted to our problem. A lemmatization process was
used a subset of the collection of the French newspaper, Le
                                                                          incorporated during the first experiments but it was inefficient
Monde to validate our method on a tagged corpus. We only
                                                                          considering the typographical variations described in Section
keep articles on television, politics, art, science or economics.
                                                                          III.
Figure 1 presents some descriptive statistics of these two
collections.                                                              C. word2vec model
                              IV. M ETHODOLOGY                               In the next step we build a word embedding model using
                                                                          word2vec [14]. We project each word of our corpus in a vector
A. System Overview
                                                                          space in order to obtain a semantic representation of these.
   Figure 2 presents an overview of the system, each step will            In this way, words appearing in similar contexts will have a
be detailed in the rest of the section. In the first step (mod-           relatively close vector representation. In addition to semantic
ule ¬), we apply different linguistic pre-processing to each              information, one advantage of such modeling is the production
discussion. The next module (­) creates a word embedding                  of vector representations of words, depending on the context
model with these discussions while the third module (®) uses              in which they are encountered. Some words close to a term t in
this model to create specific vectors. The last module (¯)                a model learned from a corpus c1 may be very different from
performs a prediction for each discussion before separating               those from a model learned from a corpus c2. For example,
the collection into clusters based on the predicted class.                we observe in figure 3 that the first eight words close to the
  1 http://early.irlab.org/                                               term “teen” vary according to the corpus used. This example
  2 Symbols used in written messages to express emotions, e.g. smile or   also shows that the use of a generic model like Le Monde in
sadness                                                                   French or Wikipedia is irrelevant in our case, since the corpus
          corpus           words                                                  the k-means algorithm [3], commonly used in the literature,
          METICS           teenager, young, 15years, kid,                         as mentioned in Section II. To feed the k-means algorithm, we
                           school, problem , spoiled, teen,                       transformed our initial collection into a bag of words matrix
          Le-Monde         sitcom, radio, compote, hearing                        [15] where each conversation is described by the frequency
                           boy, styx, scamp, rebel                                of its words. Each of the experiments was evaluated using the
                                                                                  classic measures of Precision, Recall and F-measure, averaged
Fig. 3. Eight words closest to the term “teenager” according to the type of
corpus in learning.
                                                                                  over all classes (with beta = 1 in order not to privilege
                                                                                  precision or recall [16]). Since the k-means algorithm does not
                                                                                  associate a tag with the final clusters, we have exhaustively
of the association is noisy and contains a number of apocopes,                    calculated the set of solutions to keep only the one yielding
abbreviations or acronyms. Different parameters were tested                       the highest F-score.
and the configuration with the best results was kept3 .
                                                                                  B. Results
D. Specific vectors creation and cluster predictions
   In this step, we build vectors containing terms that are
selected using the word2vec model described in step IV-C.                                                     Prec. Recall            F-score
For each theme in the collection, we build a specific linguistic                               Without pre-processing
model by performing a word embedding to reconstruct the                                  Baseline             0.18    0.16                 0.17
linguistic context of each theme. We observe, for example,                               k-means              0.23    0.20                 0.22
that the terms closest to the thematic “work” are: “unemploy-                            Without weighting 0.54       0.50                 0.52
ment”, “job”, “stress”. Similarly, for the “addiction” theme,                            Specific Vectors     0.53    0.54                 0.53
we observe the terms: “cannabis”, “alcoholism”, “drugs” and                                      With pre-processing
“heroin”. We used this context subsequently to construct a                               k-means              0.30    0.21                 0.25
vector, containing the distance distc (i) between each term i                            Without weighting 0.55       0.51                 0.53
and the theme c. Each of these models is independent, so                                 Specific Vectors     0.54    0.54                 0.54
the same term can appear in several models. In this way,                                        Fig. 4. Results obtained by each system.
we observed that the word “stress” is present in the vector
“suicide” and in that of “work”, however, the associated weight
                                                                                     Figure 4 presents a summary of the results obtained with
is different. We varied the size of these vectors between 20
                                                                                  each systems. We first observe that baseline scores are very
and 1000 and the best results were obtained with a size of 400.
                                                                                  low, but remain relatively close to the theoretical random (0.2)
In the last step ¯, the system computes an Sc score for each
                                                                                  given by the number of classes. Linguistic pre-treatments are
discussion and for each cluster according to each linguistic
                                                                                  not very efficient individually, but improve overall the results
model such as:
                             n
                                                                                  of other experiments. The k-means algorithm obtains slightly
                            X                                                     better results in terms of F-score, but remains weak. Specific
                  Sc (d) =      tf (i) · distc (i)           (1)
                                                                                  vectors get excellent results that outperform other systems with
                                 i=1
                                                                                  an F-score of 0.54. The execution without weighting improve
with i the considered term, tf (i) frequency of i in the                          slightly the recall.
collection, and distc (i) is the distance between the term i and
the thematic c. In the end, the class with the highest score is                   C. Cluster Analysis
chosen.
                                                                                     Initial objective of this work was the exploration of the
                  V. E XPERIMENTS AND RESULTS                                     METICS collection, we apply the whole process with the
A. Experimental protocol                                                          specific vectors approach to automatically categorize all the
  To evaluate the quality of the obtained clusters, we used a                     conversations. We use the Latent Dirichlet Allocation [17] in
subset of the texts of the Le-Monde newspaper, described in                       order to obtain the main topic of each cluster. Figure 5 presents
Section III, each article having a label according to the theme.                  average weight of each thematic keywords according to each
For these experiments, we configured the specific vectors (SV)                    clusters.
approach with the optimal parameters, as defined in Sections                         In figure 5, fear, shrink and trust are present designations
IV-C and IV-D. We also tested the specific vectors without                        for each cluster with a largely significant rank; yet, does
weighting to test the particular influence of this parameter. To                  the writer still express fear when he writes, ”I’m afraid of
highlight the difficulty of the task, we compare our system                       being sick”? Do these designations not participate in opening
with a baseline which consists in a random draw, and with                         and constructing spheres of meanings around these pivotal
                                                                                  words? Conversely, with a lower rank, but also significant, the
   3 The best results were obtained with the following parameter values: vector
                                                                                  designations of thing, difficult, and problem are more vague,
size: 700, sliding window size: 5, minimum frequency: 10, vectorization
method: skip grams, and a soft hierarchical max function for the model            but more reformulating to take up the elements involved in
learning.                                                                         writing what is wrong.
                                                    Fig. 5. Distribution of discursive routines by cluster.


            VI. C ONCLUSION AND FUTURE WORK                                       [7] A. P. Dempster, N. M. Laird, and D. B. Rubin, “Maximum likelihood
                                                                                      from incomplete data via the em algorithm,” in Journal of the royal
   In this article, we presented an unsupervised approach to                          society, series B, 1977, pp. 1–38.
exploring a collection of stories about human distress. This                      [8] J. D. Banfield and A. E. Raftery, “Model-based gaussian and non-
approach uses a word embedding model to build vectors                                 gaussian clustering,” in Biometrics, vol. 49, 1993, pp. 803–821.
                                                                                  [9] T. Kohonen, “Self-organized formation of topologically correct feature
containing only vocabulary from the linguistic context of the                         maps,” Biological Cybernetics, pp. 59–69, Jan 1982.
model. We evaluated the quality of the approach on a col-                        [10] U. N. Raghavan, R. Albert, and S. Kumara, “Near linear time algorithm
lection labeled with classical measures. The detailed analysis                        to detect community structures in large-scale networks.” Physical review.
                                                                                      E, Statistical, nonlinear, and soft matter physics, p. 036106, 2007.
showed very good results (average Fscore of 0.54) compared                       [11] J. P. Pestian, P. Matykiewicz, M. Linn-Gust, B. South, O. Uzuner,
to the other systems tested. This method of analysis has also                         J. Wiebe, K. B. Cohen, J. Hurdle, and C. Brew, “Sentiment analysis
made it possible to highlight semantic universes and thematic                         of suicide notes: A shared task,” Biomedical Informatics Insights, pp.
                                                                                      3–16, 2012.
groupings. We first intend to study in more detail the influence                 [12] A. Abboute, Y. Boudjeriou, G. Entringer, J. Azé, S. Bringay, and
of each of the parameters on the results obtained. We are also                        P. Poncelet, “Mining twitter for suicide prevention,” in Natural Language
planning to be able to assign several tags to each discussion,                        Processing and Information Systems: 19th International Conference on
                                                                                      Applications of Natural Language to Information Systems, NLDB 2014,
which would allow thematic overlaps to be taken into account.                         Montpellier, France, June 18-20, 2014. Proceedings. Springer, 2014,
The analysis reinforces the cluster approach to highlight the                         pp. 250–253.
defining features of this type of speech production and to                       [13] R. Kessler, J.-M. Torres, and M. El-Bèze, “Classification thématique de
                                                                                      courriel par des méthodes hybrides,” Journée ATALA sur les nouvelles
reveal its inner workings. This entry by the discursive routines                      formes de communication écrite, 2004.
is only one example which will then make it possible to                          [14] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, “Distributed
approach other explorations with a particular focus on the                            representations of words and phrases and their compositionality,” in
                                                                                      Proceedings of NIPS’13. USA: Curran Associates Inc., 2013,
argumentative forms and on the forms of intensity.                                    pp. 3111–3119. [Online]. Available: http://dl.acm.org/citation.cfm?id=
                                                                                      2999792.2999959
                              R EFERENCES                                        [15] C. D. Manning and H. Schütze, Foundations of Statistical Natural
 [1] D. Fassin, “Et la souffrance devint sociale,” in Critique. 680(1), 2004,         Language Processing. Cambridge, MA, USA: MIT Press, 1999.
     pp. 16–29.                                                                  [16] C. Goutte and E. Gaussier, “ A Probabilistic Interpretation of Precision,
 [2] ——, “Souffrir par le social, gouverner par l’écoute,” in Politix. 73(1),        Recall and F-Score, with Implication for Evaluation,” ECIR 2005, pp.
     2006, pp. 137–157.                                                               345–359, 2005.
 [3] MacQueen, J., “Some methods for classification and analysis of multi-       [17] M. Hoffman, F. R. Bach, and D. M. Blei, “Online learning for latent
     variate observations,” in Proceedings of the Fifth Berkeley Symposium            dirichlet allocation,” in Advances in Neural Information Processing
     on Mathematical Statistics and Probability, Vol. 1: Statistics. USA:             Systems, J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel,
     University of California Press, 1967, pp. 281–297.                               and A. Culotta, Eds. 23, 2010, pp. 856–864.
 [4] D. Arthur and S. Vassilvitskii, “K-means++: The advantages of careful
     seeding,” Proceedings of the Eighteenth Annual ACM-SIAM Symposium
     on Discrete Algorithms, pp. 1027–1035, 2007.
 [5] L. Kaufman and P. Rousseeuw, Clustering by Means of Medoids.
     Delft University of Technology : reports of the Faculty of
     Technical Mathematics and Informatics, 1987. [Online]. Available:
     https://books.google.fr/books?id=HK-4GwAACAAJ
 [6] G. N. Lance and W. T. Williams, “A general theory of classificatory
     sorting strategies1. hierarchical systems,” The Computer Journal 4, pp.
     373–380, 1967.
