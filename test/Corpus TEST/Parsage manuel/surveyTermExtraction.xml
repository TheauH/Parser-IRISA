<article>
    <preamble>surveyTermExtraction.pdf</preamble>
    <titre>AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS</titre>
    <auteurs>
        <auteur>M. Teresa Cabré Castellví (teresa.cabre@trad.upf.es)</auteur>
        <auteur>Rosa Estopà Bagot (rosa.estopa@trad.upf.es)</auteur>
        <auteur>Jordi Vivaldi Palatresi (jorge.vivaldi@info.upf.es)</auteur>
        <affiliation>Institut Universitari de Lingüística Aplicada. Universitat Pompeu Fabra
La Rambla, 30-32
Barcelona – Spain - 08002</affiliation>
    </auteurs>
    <abstract>Abstract
In this paper we account for the main characteristics and performance of a
number of recently developed term extraction systems. The analysed tools
represent the main strategies followed by researchers in this area. All systems
are analysed and compared against a set of technically relevant characteristics.</abstract>
    <introduction>1.
Introduction
In the late 80s there was an acute need, from different disciplines and goals, to
automatically extract terminological units from specialised texts. In the 90s large
computerised textual corpora have been constructed resulting in the first
programs for terminology extraction1 (henceforth TE) which have showed
encouraging results.
Throughout the current decade computational linguists, applied linguists,
translators, interpreters, scientific journalists and computer engineers have been
interested in automatically isolating terminology from texts. There are many
goals that have led these different professional groups to design software tools so
as to directly extract terminology from texts: building of glossaries, vocabularies
and terminological dictionaries; text indexing; automatic translation; building of
1 In order to give a broader view of TE we use both extractor and detector to refer to the same notion
knowledge databases; construction of hypertext systems; construction of expert
systems and corpus analysis.
From the appearance of TERMINO (the first broadly known term detector) in
1990 until today a number of projects to design different types of automatic
terminology detectors have been carried out to assist terminological work.
However, despite the large number of studies in progress, the automatisation of
the terminological extraction phase is still fraught with problems. The main
problems encountered by term extractors are: (1) identification of complex
terms, that is, determining where a terminological phrase begins and ends; (2)
recognition of complex terms, that is, deciding whether a discursive unit
constitutes a terminological phrase or a free unit; (3) identification of the
terminological nature of a lexical unit, that is, knowing whether in a specialised
text a lexical unit has a terminological nature or belongs to general language and
(4) appropriateness of a terminological unit to a given vocabulary (this has
scarcely been addressed from the point of view of automatization).
Systems for TE are based on three types of knowledge: (a) linguistic; (b)
statistical; (c) hybrid (statistical and linguistic). Hence, there are different
approaches to automatic term detection. All systems analyse a corpus of
specialised texts in electronic form and extract lists of word chunks (i.e.
candidate terms) that are to be confirmed by the terminologist. To make the
terminologist’s task easier the candidate term is provided with its context and,
when available, with any other further information (frequency, relationship
between terms, etc.)
Two relevant aspects regarding the nature of terms are termhood and
unithood2; TE systems may be designed based on only one of these two aspects.
Some practical experiments following each scheme for ranking a set terms
extracted from Japanese texts are presented in (Nakagawa & Mori, 1998). They
show that results in precision and recall are very close but the set of terms
extracted are a somewhat different. This is still a research issue.
Alongside term detection we find the task of automatic document indexing (i.e.
information retrieval, IR). This applied field of natural language processing
(NLP) techniques has an interesting common point with automatic term
detection, that is, word chunks that index a given document are often
terminological units. This same goal explains why many extraction systems are
rooted on IR as well as on the analysis of a specific IR system with no
application whatsoever to TE.
2 (Kageura &amp; Umino, 1996) refer to unithood as the degree of stability of syntagmatic combinations
(collocations) and termhood as the degree in that a linguistic unit is related to a domain-specific
concept.
The difference between these two approaches lies in the fact that a tool for TE
should extract all terminological units from a text, whereas IR focuses on the
extraction of only words or word sequences that better describe the contents of
the document regardless of their grammatical features.
The standard approach to IR consists in processing documents so as to extract
the so-called indexing terms. These terms are usually isolated words containing
enough semantic load to provide information about its goodness when describing
documents. Queries are processed in a similar fashion to extract query terms.
With regard to queries the relevance of documents is based exclusively on their
representing terms. This is the reason why their choice is crucial.
Often these indexing terms are single words although it is known that isolated
words are seldom relevant enough to decide the semantic value of a document
with regard to the query. This fact has given rise to the ever-growing appearance,
in the TREC3 assessments, of word and word-sequence indexing systems using
NLP techniques.
Statistically based systems function by detecting two or more lexical units whose
occurrence is higher than a given level. This is not a random situation, but it is
related to a particular usage of these lexical units. This principle, called Mutual
Information, also applies to other science domains such as telecommunications
and physics. Term detectors based on hybrid knowledge tend to use this idea
prior to a linguistic-based processing.
The problem with this kind of approach is that there are low-frequency terms
difficult to be managed by extraction systems. Here it is important to note that
these systems use basically numerical information and thus are prone to be
language-independent. The two most frequently used measures in the assessment
of these systems are found in IR: recall and precision. Recall is defined as the
relationship between the sum of retrieved terms and the sum of existing terms in
the document that is being explored. In contrast precision accounts for the
relationship between those extracted terms that are really terms and the aggregate
of candidate terms that are found. These measures can be interpreted as the
capacity of the detection system to extract all terms from a document (recall) and
the capacity to discriminate between those units detected by the system which
are terms and those which are not (precision). The fact that recall accounts for all
terms from a document implies that it is a figure much more difficult to estimate
and improve than precision.
In contrast with this traditional approach, other approaches attempt to solve the
problem by using linguistic knowledge, which may include two types of
information:
3 TREC (Text Retrieval Engineering Conference) refers to a series of conferences supported by NIST
and DARPA (U.S. agencies). Further information can be found at: http://trec.nist.gov/.
a) Term specific: it consists in the detection of the recurrent patterns from
complex terminological units such as noun-adjective and noun-preposition-noun.
This calls for the use of regular expressions and techniques of finite state
automata.
b) Language generic: it consists in the use of more complex systems of NLP that
start with the detection of more basic linguistic structures: noun phrase (NP),
prepositional phrase (PP), etc.
In both approaches each word is associated to a morphological category. In order
to do so different strategies are proposed: from coarse systems that do not make
use of any dictionary to complex systems that have an extremely detailed
morphological analysis and a final phase of disambiguation.
Systems that harness structural information resort to techniques of partial
analysis to detect potentially terminological phrasal structures. There are also
systems that benefit from their understanding of what is a non-term so they are at
some point in between those systems already mentioned. Other systems try to
reutilize current terminological databases to find terms, variants or new terms.
Systems based on linguistic knowledge tend to use noise and silence as a
measure of its efficiency. Noise attempts to assess the rate between discarded
candidates and accepted ones; silence attempts to assess those terms contained in
an analysed text that are not detected by the system. Noise is common problem
of those systems using this approach. Errors in the assignation of morphological
category are also shared by these systems.
The type of knowledge used leads to language-specific systems and therefore it
requires a prior linguistic analysis and probably a redesign of many parts of the
system. Knowledge in artificial intelligence has been traditionally obtained from
experts in each domain. This has yielded several difficulties so that some
scholars have focused on automatization and systematisation in knowledge
acquisition. This strategy seems to show the benefits of a terminological
approach. Thus some researchers (e.g. Condamines, 1995) have proposed the
construction of terminological knowledge databases so as to include linguistic
knowledge in traditional databases. Although this is a recent approach, there is
no database yet containing all the features that could be used in TE, i.e. there is
hardly any semantic information. Thus closed lists of words containing sparse
semantic information within a given specialised domain have been proposed.
In this paper we attempt to analyse the main systems of terminology extraction in
order to describe its current status and thus be able to enrich them. This paper is
divided up into two main parts: firstly, the largest part is devoted to describe
various systems of terminology extraction together with a short evaluation in
which weak and strong points have been outlined. Secondly, the terminology
extraction systems have been classified according to some parameters.</introduction>
   <results>3.5. Results
The table below summarises for each system the type of corpus used for the tests and the results attained:
Table 3: Results
         System
Test corpora
                  Name /Author
Domain
Language
Size.[Kw.]
precision
recall
 1
2 3 4 5 6 7 8 9
10 11 12
ANA
CLARIT11 Daille
FASTR
Heid
LEXTER Naulleau NEURAL NODALIDA-95
Termight TERMINO TERMS
Aviation engineering Acoustics
News Telecommunications Medicine (abstracts) Engineering Engineering Technical
Medicine Cosmology Technical text Computer science Medicine Statistics Semantics Chromatography
French English English French French German French French English English
English French English
120 ?
25
240 Mb 800 1.560 35 3.250
?
55
20
? ?
- 81.6 ? ? 86.7 74.9 ? ?
95 ?
? ?
? 70
95-98 98.5-100
?
?
2.3 77 6.3 86 14.9 96
Terms %
? ?
72 70-74</results>
    <conclusion>4.
Conclusions
We can reach some conclusions after having analysed and evaluated some of the
main systems of TE designed in the last decade:
a) The efficiency of the extraction presents a high degree of variation from one
to another. Broadly speaking, there is neither clear nor measurable explanation of
the final results. Besides, we have to bear in mind that these systems are tested
with small and highly specialised corpora. This lack of data makes it difficult to
evaluate and compare them. However, it does not prevent pinpointing those
solutions, which are considered valid to solve specific problems.
b) None of the systems is entirely satisfactory due to two main reasons. First, all
systems produce too much silence, especially statistically-based systems.
Second, all of them generate a great deal of noise, especially linguistically-based
systems.
c) Taking into account the noise generated, all systems propose large lists of
candidate terms, which at the end of the process have to be manually accepted or
rejected.
d) Most of the TE systems are related to only one language: French or English.
Usually the language specific data is embedded in the tool. This makes difficult
to use the system in a language other than the original.
e) As has been already pointed out, training corpora tend to be small (from 2.3 to
12 Kwords) and highly specialised with regard to the topic as well as the
specialisation degree. This allows for a quite precise patterns and lexicosemantic,
formal and morphosyntactic heuristics albeit this only applies to highly
specialised corpora.
f) All systems focus entirely on NPs and none of them deals with verbal phrases.
This is because there is a high rate of terminological NPs in specialised texts.
This rate can vary according to the topic and the specialisation degree. Despite
what has just been noted, it is noteworthy that all specialised languages have
their own verbs (or specific combinations of a verbal nature), no matter how low
the ratio is in comparison with nouns.
g) As a result, none of the systems refers to the distinction between nominal
collocations and nominal terminological units of a syntactic nature. Nor do they
refer to phraseology.
h) Many of the systems make use of a number of morphosyntactic patterns to
identify complex terms. However they account for most of the terminological
units they are still too few and also not very constraining. Thus, for English are
AN and NN, for French NA and N prep N. Some terms present structures other
than these ones and they are never detected. Those systems based only on these
types of linguistic techniques generate too much noise.
i) It is generally agreed that frequency is a good criterion to indicate that a
candidate term is actually a terminological unit. However, frequency is not on its
own a sufficient criterion, as it yields a great deal of noise.
j) Only a few recent systems use semantic information to recognise and delimit
terminological units although its use takes place at different levels.
k) None of the systems uses extensively the combinatory features of terms from
specialised languages in relation to a given domain. It is needed more studies
about the type of constraints that terminological units present with regard to
conceptual field and text type.
l) Only one of the analysed systems take profit of the possibilities given by the
alignment of specialised text.
i) Most of the authors consider the POS disambiguation as one of the most
important error sources. However, they do not provide exact figures about its
incidence degree.
To improve these systems of terminology extraction and lessen the noise and
silence that are generated, two type of studies should be encouraged. First, it is
required more linguistic oriented studies on the semantic relationships among
terms, the semantic relationships among constituents of a terminological unit,
semantico-lexical representation, constraints of terminological units within a
given specialised domain and in a given text type, all the grammatical categories
that are likely to become terms in specialised domains, the influence of the
syntactic function of terminological phrases on texts, the relationships between
terms and their arrangement in texts.
Second, we should focus on software systems that: combine in a more active
manner statistical and linguistic methods; improve statistical measures; combine
more than one strategy; are easily applicable to more than one language; improve
interfaces to facilitate the machine-user interaction. Also it should be very useful,
as suggested in Kageura et al. (1998), the development of a common test bench
for aiding the evaluation/comparison of extracting methods.
In sum, should we progress in the field of automatic terminology extraction,
statistical and linguistic methods have to actively be combined. It means that
they are not either-or approaches but complementary ones. The final goal is to
reduce the amount of silence and noise so that the process of terminological
extraction becomes as automatic and precise as possible. In the future, we
believe that any current terminology extractor, apart from accounting for the
morphological, syntactic and structural aspects of terminological units, has to
necessarily include semantic aspects if the efficiency of the system is to be
improved with regard to the existing ones.</conclusion>
    <biblio>References
Arppe, A. 1995. “Term extraction from unrestricted text”. Lingsoft Web Site:
http://www.lingsoft.com
Ahmad, K., Davies, A., Fulford, H. and Rogers, M. 1992. “What is a term? The
semiautomatic extraction of terms from text”. Translation Studies – an
interdiscipline. Amsterdam: John Benjamins.
Bourigault, D. 1994. LEXTER, un Logiciel d'EXtraction de TERminologie.
Application à l'acquisition des connaissances à partir de textes. PhD Thesis.
Paris: École des Hautes Études en Sciences Sociales.
Bourigault, D., Gonzalez-Mullier, I. and Gros, C. 1996. “LEXTER, a Natural
Language Processing Tool for Terminology Extraction”. Proceedings of the
7th EURALEX International Congress. Göteborg.
Brown, P. F., Cocke, F., Pietra, S., Felihek. F., Merces, R. and Rossin, P. (1988)
A statistical approach to language translation. Procedings of 12th International
Conference of Computational Linguistic (Coling-88). Budapest, Hungary.
Cabré, M.T. 1999. Terminology. Theory, methods and applications. Amsterdam:
John Benjamins.
Church, K. 1989. “Word association norms, mutual information and
lexicography”. Proceedings of the 27th annual meeting of the ACL. Vancouver,
76-83.
Condamines, A. 1995. “Terminology: new needs, new perspectives”.
Terminology, 2, 2: 219-238.
Dagan, I. and Church, K. 1994. “Termight: Identifying and translating technical
terminology”. Proceedings of the Fourth Conference on Applied Natural
Language Processing, 34-40.
Daille, B. 1994. Approche mixte pour l'extraction de terminologie: statistique
lexicale et filtres linguistiques. PhD dissertation. Paris: Université Paris VII.
Daille, B. and Jacquemin, C 1998. “Lexical database and information access: a
fruitfull association?”. First International Conference on LREC. Granada.
David, S. and Plante, P. 1991. “Le progiciel TERMINO: de la nécessité d'une
analyse morphosyntaxique pour le dépouillement terminologique des textes”.
Proceedings of the Montreal Colloquium Les industries de la langue:
perspectives des années 1990, 1: 71-88.
Enguehard, C. and Pantera, L. 1994. “Automatic Natural Acquisition of a
Terminology”. Journal of Quantitative Linguistics, 2, 1: 27-32.
Estopà, R. 1999. Extracció de terminologia: elements per a la construcció d’un
SEACUSE (Sistema d’extracció automàtica de candidats a unitats de
significació especialitzada). PhD thesis, Barcelona: Universitat Pompeu Fabra.
Estopà, R. and Vivaldi, J. 1998. “Systèmes de détection automatique de
(candidats à) termes: vers une proposition intégratrice”. Actes des 7èmes
Journées ERLA-GLAT, Brest, 385-410
Evans, D.A. and Zhai, C. 1996. “Noun-phrase Analysis in Unrestricted Text for
information retrieval”. Proceedings of ACL, Santa Cruz, University of
California, 17-24.
Frantzi, K. and Ananiadou, S. 1995. Statistical measures for terminological
extraction. Working paper of the Department of Computing of Manchester
Metropolitan University.
Frantzi, K. T. 1997. “Incorporating context information for extraction of terms”.
Proceedings of ACL/EACL, Madrid, 501-503.
Habert, B., Naulleau, E. and Nazarenko, A. 1996. “Symbolic word clustering for
medium-size corpora”. Proceedings of Coling’96: 490-495.
Heid, U., Jauss, S., Krüger, K. and Hohmann, A. 1996. “Term extraction with
standard tools for corpus exploration. Experience from German”. In: TKE ‘96:
Terminology and Knowledge Engineering,, 139-150. Berlin: Indeks Verlag.
Jacquemin, C. 1994. “Recycling Terms into a Partial Parser”. Proceedings of
ANLP’94, 113-118.
Jacquemin, C. 1999. “Syntagmatic and paradigmatic representations of term
variation”. Proceedings of ACL'99, University of Maryland, 341-348.
Jacquin, C. and Liscouet, M. 1996. “Terminology extraction from texts corpora:
application to document keeping via Internet”. In: TKE ‘96: Terminology and
Knowledge Engineering, 74-83. Berlin: Indeks Verlag.
Justeson, J. and Katz, S. 1995. “Technical terminology: some linguistic
properties and an algorithm for identification in text”. Natural Language
Engineering, 1, 1: 9-27.
Kageura, K. and Umino, B. 1996. “Methods of Automatic Term Recognition”.
Papers of the National Center for Science Information Systems, 1-22.
Kageura, K., Yoshioka, M., Koyama, T. and Nozue, T. 1998. “Towards a
common testbed for corpus-based computational terminology”. Proceedings of
Computerm ‘98, Montreal, 81-85.
Karlsson, F. 1990. “Constraint grammar as a framework for parsing running
text”. Proceedings of the 13th International conference on computational
linguistic, 3: 168-173.
Lauriston, A. 1994. “Automatic recognition of complex terms: Problems and the
TERMINO solution”. Terminology, 1, 1: 147-170.
Maynard, D. and Ananiadou, S. 1999. “Identifying contextual information for
multi-word term extraction”. In: TKE ‘99: Terminology and Knowledge
Engineering, 212-221. Vienna: TermNet.
Nakagawa, H. and Mori , T. 1998. “Nested collocation and Compound Noun for
Term Extraction”. Proceedings of Computerm ’98, Montreal, 64-70.
Naulleau, E 1998. Apprentissage et filtrage syntaxico-sémantique de syntagmes
nominaux pertinents pour la recherche documentaire. PhD thesis. Paris:
Université Paris 13.
Naulleau, E. 1999. “Profile-guided terminology extraction”. In: TKE‘99:
Terminology and Knowledge Engineering. 222-240. Vienna: TermNet.
Plante, P. and Dumas, L. 1998. “Le Dépoulliment terminologique assisté par
ordinateur”. Terminogramme, 46, 24-28.
Shieber, S.N. 1986. “An Introduction to Unification-Based Approaches to
grammar”. CSLI Lecture Notes of University Press, 4.
Smadja, F. 1991. Extracting collocations from text. An application : language
generation. Columbia: Columbia University. Department of Computer
Science. [Unpublished doctoral dissertation]
Voutilainen, A. 1993. “NPtool, a detector of English noun phrases”. Proceedings
of the Workshop on Very Large Corpora.
Zhai, C., Tong, X., Milic-Frayling, N. and Evans, D.A. 1996. “Evaluation of
syntactic phrase indexing CLARIT. NLP track report”. Proceedings of the
TREC-5. TREC Web Site: http://trec.nist.gov/pubs/trec5/t5_proceedings.html</biblio>
</article>